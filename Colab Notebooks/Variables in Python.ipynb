{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"15wkeak-0ojUShJnlDTH4vQ8LEDT6CuGy","authorship_tag":"ABX9TyPqbaL5lYEHJJkvBVJ43PQo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"QFNjWpMgceY7","executionInfo":{"status":"ok","timestamp":1750928089920,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"outputs":[],"source":["my_var = 10"]},{"cell_type":"code","source":["hex(id(my_var))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"grhEkrUHcjQO","executionInfo":{"status":"ok","timestamp":1750928112301,"user_tz":-330,"elapsed":8,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"4757833a-7367-431c-a644-655850e4af57"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0xa42788'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["greeting = \"Hello\""],"metadata":{"id":"HqqwwwaBcpew","executionInfo":{"status":"ok","timestamp":1750928134172,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["hex(id(greeting))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"2Fx_MqWycw5-","executionInfo":{"status":"ok","timestamp":1750928148705,"user_tz":-330,"elapsed":10,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"e30dcf15-db6e-45e4-ebd7-839a784df0de"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0x7d0fffa2c7f0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["hex(id(1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"slrmWIOzc0cU","executionInfo":{"status":"ok","timestamp":1750928190280,"user_tz":-330,"elapsed":48,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"66770fb8-3cec-4d2b-afcf-d8f3993fcb12"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0xa42668'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["id(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZUIdJDN5c4Ww","executionInfo":{"status":"ok","timestamp":1750928174644,"user_tz":-330,"elapsed":9,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"ec6b4d18-799b-4edf-d7ec-8bc0904d5267"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10757768"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["import ctypes\n","import gc"],"metadata":{"id":"HVgU72ivc6yI","executionInfo":{"status":"ok","timestamp":1750931890392,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def ref_count(address):\n","  return ctypes.addressof.c_long.from_address(address).value"],"metadata":{"id":"PTnL6FXMrF8C","executionInfo":{"status":"ok","timestamp":1750931944106,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import pyspark"],"metadata":{"id":"58-LK-6irTES","executionInfo":{"status":"ok","timestamp":1750932695339,"user_tz":-330,"elapsed":429,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["!pip install findspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fucmsv2quKXV","executionInfo":{"status":"ok","timestamp":1750932778548,"user_tz":-330,"elapsed":8583,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"50da99ed-dae9-4367-f090-ffe78a9b8e5c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting findspark\n","  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n","Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n","Installing collected packages: findspark\n","Successfully installed findspark-2.0.1\n"]}]},{"cell_type":"code","source":["import findspark\n","findspark.init()\n","\n","import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()"],"metadata":{"id":"RSJSzz-hucsZ","executionInfo":{"status":"ok","timestamp":1750932865525,"user_tz":-330,"elapsed":9438,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["help(pyspark)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wZ8B-xiHujfg","executionInfo":{"status":"ok","timestamp":1750932954430,"user_tz":-330,"elapsed":794,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"925a5029-ece2-495c-c273-a5a56874840e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on package pyspark:\n","\n","NAME\n","    pyspark - PySpark is the Python API for Spark.\n","\n","DESCRIPTION\n","    Public classes:\n","    \n","      - :class:`SparkContext`:\n","          Main entry point for Spark functionality.\n","      - :class:`RDD`:\n","          A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n","      - :class:`Broadcast`:\n","          A broadcast variable that gets reused across tasks.\n","      - :class:`Accumulator`:\n","          An \"add-only\" shared variable that tasks can only add values to.\n","      - :class:`SparkConf`:\n","          For configuring Spark.\n","      - :class:`SparkFiles`:\n","          Access files shipped with jobs.\n","      - :class:`StorageLevel`:\n","          Finer-grained cache persistence levels.\n","      - :class:`TaskContext`:\n","          Information about the current running task, available on the workers and experimental.\n","      - :class:`RDDBarrier`:\n","          Wraps an RDD under a barrier stage for barrier execution.\n","      - :class:`BarrierTaskContext`:\n","          A :class:`TaskContext` that provides extra info and tooling for barrier execution.\n","      - :class:`BarrierTaskInfo`:\n","          Information about a barrier task.\n","      - :class:`InheritableThread`:\n","          A inheritable thread to use in Spark when the pinned thread mode is on.\n","\n","PACKAGE CONTENTS\n","    _globals\n","    accumulators\n","    broadcast\n","    cloudpickle (package)\n","    conf\n","    context\n","    daemon\n","    errors (package)\n","    files\n","    find_spark_home\n","    install\n","    instrumentation_utils\n","    java_gateway\n","    join\n","    ml (package)\n","    mllib (package)\n","    pandas (package)\n","    profiler\n","    rdd\n","    rddsampler\n","    resource (package)\n","    resultiterable\n","    serializers\n","    shell\n","    shuffle\n","    sql (package)\n","    statcounter\n","    status\n","    storagelevel\n","    streaming (package)\n","    taskcontext\n","    testing (package)\n","    traceback_utils\n","    util\n","    version\n","    worker\n","    worker_util\n","\n","CLASSES\n","    builtins.object\n","        pyspark.conf.SparkConf\n","        pyspark.context.SparkContext\n","        pyspark.files.SparkFiles\n","        pyspark.profiler.Profiler\n","            pyspark.profiler.BasicProfiler\n","        pyspark.status.StatusTracker\n","        pyspark.storagelevel.StorageLevel\n","        pyspark.taskcontext.BarrierTaskInfo\n","        pyspark.taskcontext.TaskContext\n","            pyspark.taskcontext.BarrierTaskContext\n","    builtins.tuple(builtins.object)\n","        pyspark.status.SparkJobInfo\n","        pyspark.status.SparkStageInfo\n","    pyspark.serializers.FramedSerializer(pyspark.serializers.Serializer)\n","        pyspark.serializers.CloudPickleSerializer\n","        pyspark.serializers.MarshalSerializer\n","    threading.Thread(builtins.object)\n","        pyspark.util.InheritableThread\n","    typing.Generic(builtins.object)\n","        pyspark.accumulators.Accumulator\n","        pyspark.accumulators.AccumulatorParam\n","        pyspark.broadcast.Broadcast\n","        pyspark.rdd.RDD\n","        pyspark.rdd.RDDBarrier\n","    \n","    class Accumulator(typing.Generic)\n","     |  Accumulator(aid: int, value: ~T, accum_param: 'AccumulatorParam[T]')\n","     |  \n","     |  A shared variable that can be accumulated, i.e., has a commutative and associative \"add\"\n","     |  operation. Worker tasks on a Spark cluster can add values to an Accumulator with the `+=`\n","     |  operator, but only the driver program is allowed to access its value, using `value`.\n","     |  Updates from the workers get propagated automatically to the driver program.\n","     |  \n","     |  While :class:`SparkContext` supports accumulators for primitive data types like :class:`int` and\n","     |  :class:`float`, users can also define accumulators for custom types by providing a custom\n","     |  :py:class:`AccumulatorParam` object. Refer to its doctest for an example.\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> a = sc.accumulator(1)\n","     |  >>> a.value\n","     |  1\n","     |  >>> a.value = 2\n","     |  >>> a.value\n","     |  2\n","     |  >>> a += 5\n","     |  >>> a.value\n","     |  7\n","     |  >>> sc.accumulator(1.0).value\n","     |  1.0\n","     |  >>> sc.accumulator(1j).value\n","     |  1j\n","     |  >>> rdd = sc.parallelize([1,2,3])\n","     |  >>> def f(x):\n","     |  ...     global a\n","     |  ...     a += x\n","     |  ...\n","     |  >>> rdd.foreach(f)\n","     |  >>> a.value\n","     |  13\n","     |  >>> b = sc.accumulator(0)\n","     |  >>> def g(x):\n","     |  ...     b.add(x)\n","     |  ...\n","     |  >>> rdd.foreach(g)\n","     |  >>> b.value\n","     |  6\n","     |  \n","     |  >>> rdd.map(lambda x: a.value).collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n","     |  Traceback (most recent call last):\n","     |      ...\n","     |  Py4JJavaError: ...\n","     |  \n","     |  >>> def h(x):\n","     |  ...     global a\n","     |  ...     a.value = 7\n","     |  ...\n","     |  >>> rdd.foreach(h) # doctest: +IGNORE_EXCEPTION_DETAIL\n","     |  Traceback (most recent call last):\n","     |      ...\n","     |  Py4JJavaError: ...\n","     |  \n","     |  >>> sc.accumulator([1.0, 2.0, 3.0]) # doctest: +IGNORE_EXCEPTION_DETAIL\n","     |  Traceback (most recent call last):\n","     |      ...\n","     |  TypeError: ...\n","     |  \n","     |  Method resolution order:\n","     |      Accumulator\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __iadd__(self, term: ~T) -> 'Accumulator[T]'\n","     |      The += operator; adds a term to this accumulator's value\n","     |  \n","     |  __init__(self, aid: int, value: ~T, accum_param: 'AccumulatorParam[T]')\n","     |      Create a new Accumulator with a given initial value and AccumulatorParam object\n","     |  \n","     |  __reduce__(self) -> Tuple[Callable[[int, ~T, ForwardRef('AccumulatorParam[T]')], ForwardRef('Accumulator[T]')], Tuple[int, ~T, ForwardRef('AccumulatorParam[T]')]]\n","     |      Custom serialization; saves the zero value from our AccumulatorParam\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  __str__(self) -> str\n","     |      Return str(self).\n","     |  \n","     |  add(self, term: ~T) -> None\n","     |      Adds a term to this accumulator's value\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","     |  \n","     |  value\n","     |      Get the accumulator's value; only usable in driver program\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __annotations__ = {}\n","     |  \n","     |  __orig_bases__ = (typing.Generic[~T],)\n","     |  \n","     |  __parameters__ = (~T,)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params)\n","     |      Parameterizes a generic class.\n","     |      \n","     |      At least, parameterizing a generic class is the *main* thing this method\n","     |      does. For example, for some generic class `Foo`, this is called when we\n","     |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n","     |      \n","     |      However, note that this method is also called when defining generic\n","     |      classes in the first place with `class Foo(Generic[T]): ...`.\n","     |  \n","     |  __init_subclass__(*args, **kwargs)\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","    \n","    class AccumulatorParam(typing.Generic)\n","     |  Helper object that defines how to accumulate values of a given type.\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> from pyspark.accumulators import AccumulatorParam\n","     |  >>> class VectorAccumulatorParam(AccumulatorParam):\n","     |  ...     def zero(self, value):\n","     |  ...         return [0.0] * len(value)\n","     |  ...     def addInPlace(self, val1, val2):\n","     |  ...         for i in range(len(val1)):\n","     |  ...              val1[i] += val2[i]\n","     |  ...         return val1\n","     |  >>> va = sc.accumulator([1.0, 2.0, 3.0], VectorAccumulatorParam())\n","     |  >>> va.value\n","     |  [1.0, 2.0, 3.0]\n","     |  >>> def g(x):\n","     |  ...     global va\n","     |  ...     va += [x] * 3\n","     |  ...\n","     |  >>> rdd = sc.parallelize([1,2,3])\n","     |  >>> rdd.foreach(g)\n","     |  >>> va.value\n","     |  [7.0, 8.0, 9.0]\n","     |  \n","     |  Method resolution order:\n","     |      AccumulatorParam\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  addInPlace(self, value1: ~T, value2: ~T) -> ~T\n","     |      Add two values of the accumulator's data type, returning a new value;\n","     |      for efficiency, can also update `value1` in place and return it.\n","     |  \n","     |  zero(self, value: ~T) -> ~T\n","     |      Provide a \"zero value\" for the type, compatible in dimensions with the\n","     |      provided `value` (e.g., a zero vector)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __annotations__ = {}\n","     |  \n","     |  __orig_bases__ = (typing.Generic[~T],)\n","     |  \n","     |  __parameters__ = (~T,)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params)\n","     |      Parameterizes a generic class.\n","     |      \n","     |      At least, parameterizing a generic class is the *main* thing this method\n","     |      does. For example, for some generic class `Foo`, this is called when we\n","     |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n","     |      \n","     |      However, note that this method is also called when defining generic\n","     |      classes in the first place with `class Foo(Generic[T]): ...`.\n","     |  \n","     |  __init_subclass__(*args, **kwargs)\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","    \n","    class BarrierTaskContext(TaskContext)\n","     |  BarrierTaskContext() -> 'TaskContext'\n","     |  \n","     |  A :class:`TaskContext` with extra contextual info and tooling for tasks in a barrier stage.\n","     |  Use :func:`BarrierTaskContext.get` to obtain the barrier context for a running barrier task.\n","     |  \n","     |  .. versionadded:: 2.4.0\n","     |  \n","     |  Notes\n","     |  -----\n","     |  This API is experimental\n","     |  \n","     |  Examples\n","     |  --------\n","     |  Set a barrier, and execute it with RDD.\n","     |  \n","     |  >>> from pyspark import BarrierTaskContext\n","     |  >>> def block_and_do_something(itr):\n","     |  ...     taskcontext = BarrierTaskContext.get()\n","     |  ...     # Do something.\n","     |  ...\n","     |  ...     # Wait until all tasks finished.\n","     |  ...     taskcontext.barrier()\n","     |  ...\n","     |  ...     return itr\n","     |  ...\n","     |  >>> rdd = spark.sparkContext.parallelize([1])\n","     |  >>> rdd.barrier().mapPartitions(block_and_do_something).collect()\n","     |  [1]\n","     |  \n","     |  Method resolution order:\n","     |      BarrierTaskContext\n","     |      TaskContext\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  allGather(self, message: str = '') -> List[str]\n","     |      This function blocks until all tasks in the same stage have reached this routine.\n","     |      Each task passes in a message and returns with a list of all the messages passed in\n","     |      by each of those tasks.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This API is experimental\n","     |      \n","     |      In a barrier stage, each task much have the same number of `barrier()`\n","     |      calls, in all possible code branches. Otherwise, you may get the job hanging\n","     |      or a `SparkException` after timeout.\n","     |  \n","     |  barrier(self) -> None\n","     |      Sets a global barrier and waits until all tasks in this stage hit this barrier.\n","     |      Similar to `MPI_Barrier` function in MPI, this function blocks until all tasks\n","     |      in the same stage have reached this routine.\n","     |      \n","     |      .. versionadded:: 2.4.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This API is experimental\n","     |      \n","     |      In a barrier stage, each task much have the same number of `barrier()`\n","     |      calls, in all possible code branches. Otherwise, you may get the job hanging\n","     |      or a `SparkException` after timeout.\n","     |  \n","     |  getTaskInfos(self) -> List[ForwardRef('BarrierTaskInfo')]\n","     |      Returns :class:`BarrierTaskInfo` for all tasks in this barrier stage,\n","     |      ordered by partition ID.\n","     |      \n","     |      .. versionadded:: 2.4.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This API is experimental\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> from pyspark import BarrierTaskContext\n","     |      >>> rdd = spark.sparkContext.parallelize([1])\n","     |      >>> barrier_info = rdd.barrier().mapPartitions(\n","     |      ...     lambda _: [BarrierTaskContext.get().getTaskInfos()]).collect()[0][0]\n","     |      >>> barrier_info.address\n","     |      '...:...'\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods defined here:\n","     |  \n","     |  get() -> 'BarrierTaskContext'\n","     |      Return the currently active :class:`BarrierTaskContext`.\n","     |      This can be called inside of user functions to access contextual information about\n","     |      running tasks.\n","     |      \n","     |      Notes\n","     |      -----\n","     |      Must be called on the worker, not the driver. Returns ``None`` if not initialized.\n","     |      An Exception will raise if it is not in a barrier stage.\n","     |      \n","     |      This API is experimental\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __annotations__ = {'_port': typing.ClassVar[typing.Union[str, int, Non...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from TaskContext:\n","     |  \n","     |  attemptNumber(self) -> int\n","     |      How many times this task has been attempted.  The first task attempt will be assigned\n","     |      attemptNumber = 0, and subsequent attempts will have increasing attempt numbers.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          current attempt number.\n","     |  \n","     |  cpus(self) -> int\n","     |      CPUs allocated to the task.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          the number of CPUs.\n","     |  \n","     |  getLocalProperty(self, key: str) -> Optional[str]\n","     |      Get a local property set upstream in the driver, or None if it is missing.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      key : str\n","     |          the key of the local property to get.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          the value of the local property.\n","     |  \n","     |  partitionId(self) -> int\n","     |      The ID of the RDD partition that is computed by this task.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          current partition id.\n","     |  \n","     |  resources(self) -> Dict[str, pyspark.resource.information.ResourceInformation]\n","     |      Resources allocated to the task. The key is the resource name and the value is information\n","     |      about the resource.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          a dictionary of a string resource name, and :class:`ResourceInformation`.\n","     |  \n","     |  stageId(self) -> int\n","     |      The ID of the stage that this task belong to.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          current stage id.\n","     |  \n","     |  taskAttemptId(self) -> int\n","     |      An ID that is unique to this task attempt (within the same :class:`SparkContext`,\n","     |      no two task attempts will share the same attempt ID).  This is roughly equivalent\n","     |      to Hadoop's `TaskAttemptID`.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          current task attempt id.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods inherited from TaskContext:\n","     |  \n","     |  __new__(cls: Type[ForwardRef('TaskContext')]) -> 'TaskContext'\n","     |      Even if users construct :class:`TaskContext` instead of using get, give them the singleton.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from TaskContext:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","    \n","    class BarrierTaskInfo(builtins.object)\n","     |  BarrierTaskInfo(address: str) -> None\n","     |  \n","     |  Carries all task infos of a barrier task.\n","     |  \n","     |  .. versionadded:: 2.4.0\n","     |  \n","     |  Attributes\n","     |  ----------\n","     |  address : str\n","     |      The IPv4 address (host:port) of the executor that the barrier task is running on\n","     |  \n","     |  Notes\n","     |  -----\n","     |  This API is experimental\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, address: str) -> None\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","    \n","    class BasicProfiler(Profiler)\n","     |  BasicProfiler(ctx: 'SparkContext') -> None\n","     |  \n","     |  BasicProfiler is the default profiler, which is implemented based on\n","     |  cProfile and Accumulator\n","     |  \n","     |  Method resolution order:\n","     |      BasicProfiler\n","     |      Profiler\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, ctx: 'SparkContext') -> None\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  dump(self, id: int, path: str) -> None\n","     |      Dump the profile into path, id is the RDD id\n","     |  \n","     |  profile(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any\n","     |      Runs and profiles the method to_profile passed in. A profile object is returned.\n","     |  \n","     |  show(self, id: int) -> None\n","     |      Print the profile stats to stdout, id is the RDD id\n","     |  \n","     |  stats(self) -> pstats.Stats\n","     |      Return the collected profiling stats\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from Profiler:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","    \n","    class Broadcast(typing.Generic)\n","     |  Broadcast(sc: Optional[ForwardRef('SparkContext')] = None, value: Optional[~T] = None, pickle_registry: Optional[ForwardRef('BroadcastPickleRegistry')] = None, path: Optional[str] = None, sock_file: Optional[BinaryIO] = None)\n","     |  \n","     |  A broadcast variable created with :meth:`SparkContext.broadcast`.\n","     |  Access its value through :attr:`value`.\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> b = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n","     |  >>> b.value\n","     |  [1, 2, 3, 4, 5]\n","     |  >>> spark.sparkContext.parallelize([0, 0]).flatMap(lambda x: b.value).collect()\n","     |  [1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n","     |  >>> b.unpersist()\n","     |  \n","     |  >>> large_broadcast = spark.sparkContext.broadcast(range(10000))\n","     |  \n","     |  Method resolution order:\n","     |      Broadcast\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, sc: Optional[ForwardRef('SparkContext')] = None, value: Optional[~T] = None, pickle_registry: Optional[ForwardRef('BroadcastPickleRegistry')] = None, path: Optional[str] = None, sock_file: Optional[BinaryIO] = None)\n","     |      Should not be called directly by users -- use :meth:`SparkContext.broadcast`\n","     |      instead.\n","     |  \n","     |  __reduce__(self) -> Tuple[Callable[[int], ForwardRef('Broadcast[T]')], Tuple[int]]\n","     |      Helper for pickle.\n","     |  \n","     |  destroy(self, blocking: bool = False) -> None\n","     |      Destroy all data and metadata related to this broadcast variable.\n","     |      Use this with caution; once a broadcast variable has been destroyed,\n","     |      it cannot be used again.\n","     |      \n","     |      .. versionchanged:: 3.0.0\n","     |         Added optional argument `blocking` to specify whether to block until all\n","     |         blocks are deleted.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      blocking : bool, optional, default False\n","     |          Whether to block until unpersisting has completed.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> b = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n","     |      \n","     |      Destroy all data and metadata related to this broadcast variable\n","     |      \n","     |      >>> b.destroy()\n","     |  \n","     |  dump(self, value: ~T, f: <class 'BinaryIO'>) -> None\n","     |      Write a pickled representation of value to the open file or socket.\n","     |      The protocol pickle is HIGHEST_PROTOCOL.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      value : T\n","     |          Value to write.\n","     |      \n","     |      f : :class:`BinaryIO`\n","     |          File or socket where the pickled value will be stored.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      \n","     |      >>> b = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n","     |      \n","     |      Write a pickled representation of `b` to the open temp file.\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"test.txt\")\n","     |      ...     with open(path, \"wb\") as f:\n","     |      ...         b.dump(b.value, f)\n","     |  \n","     |  load(self, file: <class 'BinaryIO'>) -> ~T\n","     |      Read a pickled representation of value from the open file or socket.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      file : :class:`BinaryIO`\n","     |          File or socket where the pickled value will be read.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      T\n","     |          The object hierarchy specified therein reconstituted\n","     |          from the pickled representation of an object.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      \n","     |      >>> b = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n","     |      >>> c = spark.sparkContext.broadcast(1)\n","     |      \n","     |      Read the pickled representation of value from the open temp file.\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"test.txt\")\n","     |      ...     with open(path, \"wb\") as f:\n","     |      ...         b.dump(b.value, f)\n","     |      ...     with open(path, \"rb\") as f:\n","     |      ...         c.load(f)\n","     |      [1, 2, 3, 4, 5]\n","     |  \n","     |  load_from_path(self, path: str) -> ~T\n","     |      Read the pickled representation of an object from the open file and\n","     |      return the reconstituted object hierarchy specified therein.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          File path where reads the pickled value.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      T\n","     |          The object hierarchy specified therein reconstituted\n","     |          from the pickled representation of an object.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      \n","     |      >>> b = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n","     |      >>> c = spark.sparkContext.broadcast(1)\n","     |      \n","     |      Read the pickled representation of value from temp file.\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"test.txt\")\n","     |      ...     with open(path, \"wb\") as f:\n","     |      ...         b.dump(b.value, f)\n","     |      ...     c.load_from_path(path)\n","     |      [1, 2, 3, 4, 5]\n","     |  \n","     |  unpersist(self, blocking: bool = False) -> None\n","     |      Delete cached copies of this broadcast on the executors. If the\n","     |      broadcast is used after this is called, it will need to be\n","     |      re-sent to each executor.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      blocking : bool, optional, default False\n","     |          Whether to block until unpersisting has completed.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> b = spark.sparkContext.broadcast([1, 2, 3, 4, 5])\n","     |      \n","     |      Delete cached copies of this broadcast on the executors\n","     |      \n","     |      >>> b.unpersist()\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties defined here:\n","     |  \n","     |  value\n","     |      Return the broadcasted value\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __annotations__ = {}\n","     |  \n","     |  __orig_bases__ = (typing.Generic[~T],)\n","     |  \n","     |  __parameters__ = (~T,)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params)\n","     |      Parameterizes a generic class.\n","     |      \n","     |      At least, parameterizing a generic class is the *main* thing this method\n","     |      does. For example, for some generic class `Foo`, this is called when we\n","     |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n","     |      \n","     |      However, note that this method is also called when defining generic\n","     |      classes in the first place with `class Foo(Generic[T]): ...`.\n","     |  \n","     |  __init_subclass__(*args, **kwargs)\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","    \n","    CPickleSerializer = class CloudPickleSerializer(FramedSerializer)\n","     |  Method resolution order:\n","     |      CloudPickleSerializer\n","     |      FramedSerializer\n","     |      Serializer\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  dumps(self, obj)\n","     |      Serialize an object into a byte array.\n","     |      When batching is used, this will be called with an array of objects.\n","     |  \n","     |  loads(self, obj, encoding='bytes')\n","     |      Deserialize an object from a byte array.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from FramedSerializer:\n","     |  \n","     |  dump_stream(self, iterator, stream)\n","     |      Serialize an iterator of objects to the output stream.\n","     |  \n","     |  load_stream(self, stream)\n","     |      Return an iterator of deserialized objects from the input stream.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from Serializer:\n","     |  \n","     |  __eq__(self, other)\n","     |      Return self==value.\n","     |  \n","     |  __hash__(self)\n","     |      Return hash(self).\n","     |  \n","     |  __ne__(self, other)\n","     |      Return self!=value.\n","     |  \n","     |  __repr__(self)\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from Serializer:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","    \n","    class InheritableThread(threading.Thread)\n","     |  InheritableThread(target: Callable, *args: Any, **kwargs: Any)\n","     |  \n","     |  Thread that is recommended to be used in PySpark instead of :class:`threading.Thread`\n","     |  when the pinned thread mode is enabled. The usage of this class is exactly same as\n","     |  :class:`threading.Thread` but correctly inherits the inheritable properties specific\n","     |  to JVM thread such as ``InheritableThreadLocal``.\n","     |  \n","     |  Also, note that pinned thread mode does not close the connection from Python\n","     |  to JVM when the thread is finished in the Python side. With this class, Python\n","     |  garbage-collects the Python thread instance and also closes the connection\n","     |  which finishes JVM thread correctly.\n","     |  \n","     |  When the pinned thread mode is off, this works as :class:`threading.Thread`.\n","     |  \n","     |  .. versionadded:: 3.1.0\n","     |  \n","     |  Notes\n","     |  -----\n","     |  This API is experimental.\n","     |  \n","     |  Method resolution order:\n","     |      InheritableThread\n","     |      threading.Thread\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, target: Callable, *args: Any, **kwargs: Any)\n","     |      This constructor should always be called with keyword arguments. Arguments are:\n","     |      \n","     |      *group* should be None; reserved for future extension when a ThreadGroup\n","     |      class is implemented.\n","     |      \n","     |      *target* is the callable object to be invoked by the run()\n","     |      method. Defaults to None, meaning nothing is called.\n","     |      \n","     |      *name* is the thread name. By default, a unique name is constructed of\n","     |      the form \"Thread-N\" where N is a small decimal number.\n","     |      \n","     |      *args* is a list or tuple of arguments for the target invocation. Defaults to ().\n","     |      \n","     |      *kwargs* is a dictionary of keyword arguments for the target\n","     |      invocation. Defaults to {}.\n","     |      \n","     |      If a subclass overrides the constructor, it must make sure to invoke\n","     |      the base class constructor (Thread.__init__()) before doing anything\n","     |      else to the thread.\n","     |  \n","     |  start(self) -> None\n","     |      Start the thread's activity.\n","     |      \n","     |      It must be called at most once per thread object. It arranges for the\n","     |      object's run() method to be invoked in a separate thread of control.\n","     |      \n","     |      This method will raise a RuntimeError if called more than once on the\n","     |      same thread object.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __annotations__ = {'_props': <class 'py4j.java_gateway.JavaObject'>}\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from threading.Thread:\n","     |  \n","     |  __repr__(self)\n","     |      Return repr(self).\n","     |  \n","     |  getName(self)\n","     |      Return a string used for identification purposes only.\n","     |      \n","     |      This method is deprecated, use the name attribute instead.\n","     |  \n","     |  isDaemon(self)\n","     |      Return whether this thread is a daemon.\n","     |      \n","     |      This method is deprecated, use the daemon attribute instead.\n","     |  \n","     |  is_alive(self)\n","     |      Return whether the thread is alive.\n","     |      \n","     |      This method returns True just before the run() method starts until just\n","     |      after the run() method terminates. See also the module function\n","     |      enumerate().\n","     |  \n","     |  join(self, timeout=None)\n","     |      Wait until the thread terminates.\n","     |      \n","     |      This blocks the calling thread until the thread whose join() method is\n","     |      called terminates -- either normally or through an unhandled exception\n","     |      or until the optional timeout occurs.\n","     |      \n","     |      When the timeout argument is present and not None, it should be a\n","     |      floating point number specifying a timeout for the operation in seconds\n","     |      (or fractions thereof). As join() always returns None, you must call\n","     |      is_alive() after join() to decide whether a timeout happened -- if the\n","     |      thread is still alive, the join() call timed out.\n","     |      \n","     |      When the timeout argument is not present or None, the operation will\n","     |      block until the thread terminates.\n","     |      \n","     |      A thread can be join()ed many times.\n","     |      \n","     |      join() raises a RuntimeError if an attempt is made to join the current\n","     |      thread as that would cause a deadlock. It is also an error to join() a\n","     |      thread before it has been started and attempts to do so raises the same\n","     |      exception.\n","     |  \n","     |  run(self)\n","     |      Method representing the thread's activity.\n","     |      \n","     |      You may override this method in a subclass. The standard run() method\n","     |      invokes the callable object passed to the object's constructor as the\n","     |      target argument, if any, with sequential and keyword arguments taken\n","     |      from the args and kwargs arguments, respectively.\n","     |  \n","     |  setDaemon(self, daemonic)\n","     |      Set whether this thread is a daemon.\n","     |      \n","     |      This method is deprecated, use the .daemon property instead.\n","     |  \n","     |  setName(self, name)\n","     |      Set the name string for this thread.\n","     |      \n","     |      This method is deprecated, use the name attribute instead.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties inherited from threading.Thread:\n","     |  \n","     |  ident\n","     |      Thread identifier of this thread or None if it has not been started.\n","     |      \n","     |      This is a nonzero integer. See the get_ident() function. Thread\n","     |      identifiers may be recycled when a thread exits and another thread is\n","     |      created. The identifier is available even after the thread has exited.\n","     |  \n","     |  native_id\n","     |      Native integral thread ID of this thread, or None if it has not been started.\n","     |      \n","     |      This is a non-negative integer. See the get_native_id() function.\n","     |      This represents the Thread ID as reported by the kernel.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from threading.Thread:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","     |  \n","     |  daemon\n","     |      A boolean value indicating whether this thread is a daemon thread.\n","     |      \n","     |      This must be set before start() is called, otherwise RuntimeError is\n","     |      raised. Its initial value is inherited from the creating thread; the\n","     |      main thread is not a daemon thread and therefore all threads created in\n","     |      the main thread default to daemon = False.\n","     |      \n","     |      The entire Python program exits when only daemon threads are left.\n","     |  \n","     |  name\n","     |      A string used for identification purposes only.\n","     |      \n","     |      It has no semantics. Multiple threads may be given the same name. The\n","     |      initial name is set by the constructor.\n","    \n","    class MarshalSerializer(FramedSerializer)\n","     |  Serializes objects using Python's Marshal serializer:\n","     |  \n","     |      http://docs.python.org/2/library/marshal.html\n","     |  \n","     |  This serializer is faster than CloudPickleSerializer but supports fewer datatypes.\n","     |  \n","     |  Method resolution order:\n","     |      MarshalSerializer\n","     |      FramedSerializer\n","     |      Serializer\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  dumps(self, obj)\n","     |      Serialize an object into a byte array.\n","     |      When batching is used, this will be called with an array of objects.\n","     |  \n","     |  loads(self, obj)\n","     |      Deserialize an object from a byte array.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from FramedSerializer:\n","     |  \n","     |  dump_stream(self, iterator, stream)\n","     |      Serialize an iterator of objects to the output stream.\n","     |  \n","     |  load_stream(self, stream)\n","     |      Return an iterator of deserialized objects from the input stream.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from Serializer:\n","     |  \n","     |  __eq__(self, other)\n","     |      Return self==value.\n","     |  \n","     |  __hash__(self)\n","     |      Return hash(self).\n","     |  \n","     |  __ne__(self, other)\n","     |      Return self!=value.\n","     |  \n","     |  __repr__(self)\n","     |      Return repr(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors inherited from Serializer:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","    \n","    class Profiler(builtins.object)\n","     |  Profiler(ctx: 'SparkContext') -> None\n","     |  \n","     |  PySpark supports custom profilers, this is to allow for different profilers to\n","     |  be used as well as outputting to different formats than what is provided in the\n","     |  BasicProfiler.\n","     |  \n","     |  A custom profiler has to define or inherit the following methods:\n","     |      profile - will produce a system profile of some sort.\n","     |      stats - return the collected stats.\n","     |      dump - dumps the profiles to a path\n","     |      add - adds a profile to the existing accumulated profile\n","     |  \n","     |  The profiler class is chosen when creating a SparkContext\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> from pyspark import SparkConf, SparkContext\n","     |  >>> from pyspark import BasicProfiler\n","     |  >>> class MyCustomProfiler(BasicProfiler):\n","     |  ...     def show(self, id):\n","     |  ...         print(\"My custom profiles for RDD:%s\" % id)\n","     |  ...\n","     |  >>> conf = SparkConf().set(\"spark.python.profile\", \"true\")\n","     |  >>> sc = SparkContext('local', 'test', conf=conf, profiler_cls=MyCustomProfiler)\n","     |  >>> sc.parallelize(range(1000)).map(lambda x: 2 * x).take(10)\n","     |  [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n","     |  >>> sc.parallelize(range(1000)).count()\n","     |  1000\n","     |  >>> sc.show_profiles()\n","     |  My custom profiles for RDD:1\n","     |  My custom profiles for RDD:3\n","     |  >>> sc.stop()\n","     |  \n","     |  Notes\n","     |  -----\n","     |  This API is a developer API.\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, ctx: 'SparkContext') -> None\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  dump(self, id: int, path: str) -> None\n","     |      Dump the profile into path\n","     |  \n","     |  profile(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -> Any\n","     |      Do profiling on the function `func`\n","     |  \n","     |  show(self, id: int) -> None\n","     |      Print the profile stats to stdout\n","     |  \n","     |  stats(self) -> Union[pstats.Stats, Dict]\n","     |      Return the collected profiling stats\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","    \n","    class RDD(typing.Generic)\n","     |  RDD(jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n","     |  \n","     |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n","     |  Represents an immutable, partitioned collection of elements that can be\n","     |  operated on in parallel.\n","     |  \n","     |  Method resolution order:\n","     |      RDD\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __add__(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n","     |      Return the union of this RDD and another one.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n","     |      >>> (rdd + rdd).collect()\n","     |      [1, 1, 2, 3, 1, 1, 2, 3]\n","     |  \n","     |  __getnewargs__(self) -> NoReturn\n","     |  \n","     |  __init__(self, jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  aggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U]) -> ~U\n","     |      Aggregate the elements of each partition, and then the results for all\n","     |      the partitions, using a given combine functions and a neutral \"zero\n","     |      value.\"\n","     |      \n","     |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n","     |      as its result value to avoid object allocation; however, it should not\n","     |      modify ``t2``.\n","     |      \n","     |      The first function (seqOp) can return a different result type, U, than\n","     |      the type of this RDD. Thus, we need one operation for merging a T into\n","     |      an U and one operation for merging two U\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      zeroValue : U\n","     |          the initial value for the accumulated result of each partition\n","     |      seqOp : function\n","     |          a function used to accumulate results within a partition\n","     |      combOp : function\n","     |          an associative function used to combine results from different partitions\n","     |      \n","     |      Returns\n","     |      -------\n","     |      U\n","     |          the aggregated result\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.reduce`\n","     |      :meth:`RDD.fold`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n","     |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n","     |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n","     |      (10, 4)\n","     |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n","     |      (0, 0)\n","     |  \n","     |  aggregateByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~U, seqFunc: Callable[[~U, ~V], ~U], combFunc: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7d0fe60c3a60>) -> 'RDD[Tuple[K, U]]'\n","     |      Aggregate the values of each key, using given combine functions and a neutral\n","     |      \"zero value\". This function can return a different result type, U, than the type\n","     |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n","     |      a U and one operation for merging two U's, The former operation is used for merging\n","     |      values within a partition, and the latter is used for merging values between\n","     |      partitions. To avoid memory allocation, both of these functions are\n","     |      allowed to modify and return their first argument instead of creating a new U.\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      zeroValue : U\n","     |          the initial value for the accumulated result of each partition\n","     |      seqFunc : function\n","     |          a function to merge a V into a U\n","     |      combFunc : function\n","     |          a function to combine two U's into a single one\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      partitionFunc : function, optional, default `portable_hash`\n","     |          function to compute the partition index\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing the keys and the aggregated result for each key\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.reduceByKey`\n","     |      :meth:`RDD.combineByKey`\n","     |      :meth:`RDD.foldByKey`\n","     |      :meth:`RDD.groupByKey`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n","     |      >>> seqFunc = (lambda x, y: (x[0] + y, x[1] + 1))\n","     |      >>> combFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n","     |      >>> sorted(rdd.aggregateByKey((0, 0), seqFunc, combFunc).collect())\n","     |      [('a', (3, 2)), ('b', (1, 1))]\n","     |  \n","     |  barrier(self: 'RDD[T]') -> 'RDDBarrier[T]'\n","     |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n","     |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n","     |      entire stage and relaunch all tasks for this stage.\n","     |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n","     |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n","     |      \n","     |      .. versionadded:: 2.4.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDDBarrier`\n","     |          instance that provides actions within a barrier stage.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :class:`pyspark.BarrierTaskContext`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      For additional information see\n","     |      \n","     |      - `SPIP: Barrier Execution Mode <http://jira.apache.org/jira/browse/SPARK-24374>`_\n","     |      - `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n","     |      \n","     |      This API is experimental\n","     |  \n","     |  cache(self: 'RDD[T]') -> 'RDD[T]'\n","     |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          The same :class:`RDD` with storage level set to `MEMORY_ONLY`\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.persist`\n","     |      :meth:`RDD.unpersist`\n","     |      :meth:`RDD.getStorageLevel`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.range(5)\n","     |      >>> rdd2 = rdd.cache()\n","     |      >>> rdd2 is rdd\n","     |      True\n","     |      >>> str(rdd.getStorageLevel())\n","     |      'Memory Serialized 1x Replicated'\n","     |      >>> _ = rdd.unpersist()\n","     |  \n","     |  cartesian(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n","     |      Return the Cartesian product of this RDD and another one, that is, the\n","     |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n","     |      ``b`` is in `other`.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      other : :class:`RDD`\n","     |          another :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          the Cartesian product of this :class:`RDD` and another one\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`pyspark.sql.DataFrame.crossJoin`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1, 2])\n","     |      >>> sorted(rdd.cartesian(rdd).collect())\n","     |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n","     |  \n","     |  checkpoint(self) -> None\n","     |      Mark this RDD for checkpointing. It will be saved to a file inside the\n","     |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n","     |      all references to its parent RDDs will be removed. This function must\n","     |      be called before any job has been executed on this RDD. It is strongly\n","     |      recommended that this RDD is persisted in memory, otherwise saving it\n","     |      on a file will require recomputation.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.isCheckpointed`\n","     |      :meth:`RDD.getCheckpointFile`\n","     |      :meth:`RDD.localCheckpoint`\n","     |      :meth:`SparkContext.setCheckpointDir`\n","     |      :meth:`SparkContext.getCheckpointDir`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.range(5)\n","     |      >>> rdd.is_checkpointed\n","     |      False\n","     |      >>> rdd.getCheckpointFile() == None\n","     |      True\n","     |      \n","     |      >>> rdd.checkpoint()\n","     |      >>> rdd.is_checkpointed\n","     |      True\n","     |      >>> rdd.getCheckpointFile() == None\n","     |      True\n","     |      \n","     |      >>> rdd.count()\n","     |      5\n","     |      >>> rdd.is_checkpointed\n","     |      True\n","     |      >>> rdd.getCheckpointFile() == None\n","     |      False\n","     |  \n","     |  cleanShuffleDependencies(self, blocking: bool = False) -> None\n","     |      Removes an RDD's shuffles and it's non-persisted ancestors.\n","     |      \n","     |      When running without a shuffle service, cleaning up shuffle files enables downscaling.\n","     |      If you use the RDD after this call, you should checkpoint and materialize it first.\n","     |      \n","     |      .. versionadded:: 3.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      blocking : bool, optional, default False\n","     |         whether to block on shuffle cleanup tasks\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This API is a developer API.\n","     |  \n","     |  coalesce(self: 'RDD[T]', numPartitions: int, shuffle: bool = False) -> 'RDD[T]'\n","     |      Return a new RDD that is reduced into `numPartitions` partitions.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      shuffle : bool, optional, default False\n","     |          whether to add a shuffle step\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` that is reduced into `numPartitions` partitions\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.repartition`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n","     |      [[1], [2, 3], [4, 5]]\n","     |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n","     |      [[1, 2, 3, 4, 5]]\n","     |  \n","     |  cogroup(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[ResultIterable[V], ResultIterable[U]]]]'\n","     |      For each key k in `self` or `other`, return a resulting RDD that\n","     |      contains a tuple with the list of values for that key in `self` as\n","     |      well as `other`.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      other : :class:`RDD`\n","     |          another :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing the keys and cogrouped values\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.groupWith`\n","     |      :meth:`RDD.join`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n","     |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n","     |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(rdd1.cogroup(rdd2).collect()))]\n","     |      [('a', ([1], [2])), ('b', ([4], []))]\n","     |  \n","     |  collect(self: 'RDD[T]') -> List[~T]\n","     |      Return a list that contains all the elements in this RDD.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      list\n","     |          a list containing all the elements\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This method should only be used if the resulting array is expected\n","     |      to be small, as all the data is loaded into the driver's memory.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.toLocalIterator`\n","     |      :meth:`pyspark.sql.DataFrame.collect`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.range(5).collect()\n","     |      [0, 1, 2, 3, 4]\n","     |      >>> sc.parallelize([\"x\", \"y\", \"z\"]).collect()\n","     |      ['x', 'y', 'z']\n","     |  \n","     |  collectAsMap(self: 'RDD[Tuple[K, V]]') -> Dict[~K, ~V]\n","     |      Return the key-value pairs in this RDD to the master as a dictionary.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`dict`\n","     |          a dictionary of (key, value) pairs\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.countByValue`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This method should only be used if the resulting data is expected\n","     |      to be small, as all the data is loaded into the driver's memory.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n","     |      >>> m[1]\n","     |      2\n","     |      >>> m[3]\n","     |      4\n","     |  \n","     |  collectWithJobGroup(self: 'RDD[T]', groupId: str, description: str, interruptOnCancel: bool = False) -> 'List[T]'\n","     |      When collect rdd, use this method to specify job group.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |      \n","     |      .. deprecated:: 3.1.0\n","     |          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      groupId : str\n","     |          The group ID to assign.\n","     |      description : str\n","     |          The description to set for the job group.\n","     |      interruptOnCancel : bool, optional, default False\n","     |          whether to interrupt jobs on job cancellation.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      list\n","     |          a list containing all the elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.collect`\n","     |      :meth:`SparkContext.setJobGroup`\n","     |  \n","     |  combineByKey(self: 'RDD[Tuple[K, V]]', createCombiner: Callable[[~V], ~U], mergeValue: Callable[[~U, ~V], ~U], mergeCombiners: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7d0fe60c3a60>) -> 'RDD[Tuple[K, U]]'\n","     |      Generic function to combine the elements for each key using a custom\n","     |      set of aggregation functions.\n","     |      \n","     |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n","     |      type\" C.\n","     |      \n","     |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n","     |      modify and return their first argument instead of creating a new C.\n","     |      \n","     |      In addition, users can control the partitioning of the output RDD.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      createCombiner : function\n","     |          a function to turns a V into a C\n","     |      mergeValue : function\n","     |          a function to merge a V into a C\n","     |      mergeCombiners : function\n","     |          a function to combine two C's into a single one\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      partitionFunc : function, optional, default `portable_hash`\n","     |          function to compute the partition index\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing the keys and the aggregated result for each key\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.reduceByKey`\n","     |      :meth:`RDD.aggregateByKey`\n","     |      :meth:`RDD.foldByKey`\n","     |      :meth:`RDD.groupByKey`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      V and C can be different -- for example, one might group an RDD of type\n","     |          (Int, Int) into an RDD of type (Int, List[Int]).\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n","     |      >>> def to_list(a):\n","     |      ...     return [a]\n","     |      ...\n","     |      >>> def append(a, b):\n","     |      ...     a.append(b)\n","     |      ...     return a\n","     |      ...\n","     |      >>> def extend(a, b):\n","     |      ...     a.extend(b)\n","     |      ...     return a\n","     |      ...\n","     |      >>> sorted(rdd.combineByKey(to_list, append, extend).collect())\n","     |      [('a', [1, 2]), ('b', [1])]\n","     |  \n","     |  count(self) -> int\n","     |      Return the number of elements in this RDD.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          the number of elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.countApprox`\n","     |      :meth:`pyspark.sql.DataFrame.count`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([2, 3, 4]).count()\n","     |      3\n","     |  \n","     |  countApprox(self, timeout: int, confidence: float = 0.95) -> int\n","     |      Approximate version of count() that returns a potentially incomplete\n","     |      result within a timeout, even if not all tasks have finished.\n","     |      \n","     |      .. versionadded:: 1.2.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      timeout : int\n","     |          maximum time to wait for the job, in milliseconds\n","     |      confidence : float\n","     |          the desired statistical confidence in the result\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          a potentially incomplete result, with error bounds\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.count`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize(range(1000), 10)\n","     |      >>> rdd.countApprox(1000, 1.0)\n","     |      1000\n","     |  \n","     |  countApproxDistinct(self: 'RDD[T]', relativeSD: float = 0.05) -> int\n","     |      Return approximate number of distinct elements in the RDD.\n","     |      \n","     |      .. versionadded:: 1.2.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      relativeSD : float, optional\n","     |          Relative accuracy. Smaller values create\n","     |          counters that require more space.\n","     |          It must be greater than 0.000017.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          approximate number of distinct elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.distinct`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      The algorithm used is based on streamlib's implementation of\n","     |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n","     |      of The Art Cardinality Estimation Algorithm\", available here\n","     |      <https://doi.org/10.1145/2452376.2452456>`_.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n","     |      >>> 900 < n < 1100\n","     |      True\n","     |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n","     |      >>> 16 < n < 24\n","     |      True\n","     |  \n","     |  countByKey(self: 'RDD[Tuple[K, V]]') -> Dict[~K, int]\n","     |      Count the number of elements for each key, and return the result to the\n","     |      master as a dictionary.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          a dictionary of (key, count) pairs\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.collectAsMap`\n","     |      :meth:`RDD.countByValue`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n","     |      >>> sorted(rdd.countByKey().items())\n","     |      [('a', 2), ('b', 1)]\n","     |  \n","     |  countByValue(self: 'RDD[K]') -> Dict[~K, int]\n","     |      Return the count of each unique value in this RDD as a dictionary of\n","     |      (value, count) pairs.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          a dictionary of (value, count) pairs\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.collectAsMap`\n","     |      :meth:`RDD.countByKey`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n","     |      [(1, 2), (2, 3)]\n","     |  \n","     |  distinct(self: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n","     |      Return a new RDD containing the distinct elements in this RDD.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD` containing the distinct elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.countApproxDistinct`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n","     |      [1, 2, 3]\n","     |  \n","     |  filter(self: 'RDD[T]', f: Callable[[~T], bool]) -> 'RDD[T]'\n","     |      Return a new RDD containing only the elements that satisfy a predicate.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |          a function to run on each element of the RDD\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD` by applying a function to each element\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.map`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n","     |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n","     |      [2, 4]\n","     |  \n","     |  first(self: 'RDD[T]') -> ~T\n","     |      Return the first element in this RDD.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      T\n","     |          the first element\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.take`\n","     |      :meth:`pyspark.sql.DataFrame.first`\n","     |      :meth:`pyspark.sql.DataFrame.head`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([2, 3, 4]).first()\n","     |      2\n","     |      >>> sc.parallelize([]).first()\n","     |      Traceback (most recent call last):\n","     |          ...\n","     |      ValueError: RDD is empty\n","     |  \n","     |  flatMap(self: 'RDD[T]', f: Callable[[~T], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n","     |      Return a new RDD by first applying a function to all elements of this\n","     |      RDD, and then flattening the results.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |          a function to turn a T into a sequence of U\n","     |      preservesPartitioning : bool, optional, default False\n","     |          indicates whether the input function preserves the partitioner,\n","     |          which should be False unless this is a pair RDD and the input\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD` by applying a function to all elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.map`\n","     |      :meth:`RDD.mapPartitions`\n","     |      :meth:`RDD.mapPartitionsWithIndex`\n","     |      :meth:`RDD.mapPartitionsWithSplit`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([2, 3, 4])\n","     |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n","     |      [1, 1, 1, 2, 2, 3]\n","     |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n","     |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n","     |  \n","     |  flatMapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], Iterable[~U]]) -> 'RDD[Tuple[K, U]]'\n","     |      Pass each value in the key-value pair RDD through a flatMap function\n","     |      without changing the keys; this also retains the original RDD's\n","     |      partitioning.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |         a function to turn a V into a sequence of U\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing the keys and the flat-mapped value\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.flatMap`\n","     |      :meth:`RDD.mapValues`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n","     |      >>> def f(x): return x\n","     |      ...\n","     |      >>> rdd.flatMapValues(f).collect()\n","     |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n","     |  \n","     |  fold(self: 'RDD[T]', zeroValue: ~T, op: Callable[[~T, ~T], ~T]) -> ~T\n","     |      Aggregate the elements of each partition, and then the results for all\n","     |      the partitions, using a given associative function and a neutral \"zero value.\"\n","     |      \n","     |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n","     |      as its result value to avoid object allocation; however, it should not\n","     |      modify ``t2``.\n","     |      \n","     |      This behaves somewhat differently from fold operations implemented\n","     |      for non-distributed collections in functional languages like Scala.\n","     |      This fold operation may be applied to partitions individually, and then\n","     |      fold those results into the final result, rather than apply the fold\n","     |      to each element sequentially in some defined ordering. For functions\n","     |      that are not commutative, the result may differ from that of a fold\n","     |      applied to a non-distributed collection.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      zeroValue : T\n","     |          the initial value for the accumulated result of each partition\n","     |      op : function\n","     |          a function used to both accumulate results within a partition and combine\n","     |          results from different partitions\n","     |      \n","     |      Returns\n","     |      -------\n","     |      T\n","     |          the aggregated result\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.reduce`\n","     |      :meth:`RDD.aggregate`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> from operator import add\n","     |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n","     |      15\n","     |  \n","     |  foldByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~V, func: Callable[[~V, ~V], ~V], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7d0fe60c3a60>) -> 'RDD[Tuple[K, V]]'\n","     |      Merge the values for each key using an associative function \"func\"\n","     |      and a neutral \"zeroValue\" which may be added to the result an\n","     |      arbitrary number of times, and must not change the result\n","     |      (e.g., 0 for addition, or 1 for multiplication.).\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      zeroValue : V\n","     |          the initial value for the accumulated result of each partition\n","     |      func : function\n","     |          a function to combine two V's into a single one\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      partitionFunc : function, optional, default `portable_hash`\n","     |          function to compute the partition index\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing the keys and the aggregated result for each key\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.reduceByKey`\n","     |      :meth:`RDD.combineByKey`\n","     |      :meth:`RDD.aggregateByKey`\n","     |      :meth:`RDD.groupByKey`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n","     |      >>> from operator import add\n","     |      >>> sorted(rdd.foldByKey(0, add).collect())\n","     |      [('a', 2), ('b', 1)]\n","     |  \n","     |  foreach(self: 'RDD[T]', f: Callable[[~T], NoneType]) -> None\n","     |      Applies a function to all elements of this RDD.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |          a function applied to each element\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.foreachPartition`\n","     |      :meth:`pyspark.sql.DataFrame.foreach`\n","     |      :meth:`pyspark.sql.DataFrame.foreachPartition`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> def f(x): print(x)\n","     |      ...\n","     |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n","     |  \n","     |  foreachPartition(self: 'RDD[T]', f: Callable[[Iterable[~T]], NoneType]) -> None\n","     |      Applies a function to each partition of this RDD.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |          a function applied to each partition\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.foreach`\n","     |      :meth:`pyspark.sql.DataFrame.foreach`\n","     |      :meth:`pyspark.sql.DataFrame.foreachPartition`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> def f(iterator):\n","     |      ...     for x in iterator:\n","     |      ...          print(x)\n","     |      ...\n","     |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n","     |  \n","     |  fullOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], Optional[U]]]]'\n","     |      Perform a right outer join of `self` and `other`.\n","     |      \n","     |      For each element (k, v) in `self`, the resulting RDD will either\n","     |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n","     |      (k, (v, None)) if no elements in `other` have key k.\n","     |      \n","     |      Similarly, for each element (k, w) in `other`, the resulting RDD will\n","     |      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n","     |      (k, (None, w)) if no elements in `self` have key k.\n","     |      \n","     |      Hash-partitions the resulting RDD into the given number of partitions.\n","     |      \n","     |      .. versionadded:: 1.2.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      other : :class:`RDD`\n","     |          another :class:`RDD`\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing all pairs of elements with matching keys\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.join`\n","     |      :meth:`RDD.leftOuterJoin`\n","     |      :meth:`RDD.fullOuterJoin`\n","     |      :meth:`pyspark.sql.DataFrame.join`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n","     |      >>> rdd2 = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n","     |      >>> sorted(rdd1.fullOuterJoin(rdd2).collect())\n","     |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n","     |  \n","     |  getCheckpointFile(self) -> Optional[str]\n","     |      Gets the name of the file to which this RDD was checkpointed\n","     |      \n","     |      Not defined if RDD is checkpointed locally.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      str\n","     |          the name of the file to which this :class:`RDD` was checkpointed\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.checkpoint`\n","     |      :meth:`SparkContext.setCheckpointDir`\n","     |      :meth:`SparkContext.getCheckpointDir`\n","     |  \n","     |  getNumPartitions(self) -> int\n","     |      Returns the number of partitions in RDD\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          number of partitions\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n","     |      >>> rdd.getNumPartitions()\n","     |      2\n","     |  \n","     |  getResourceProfile(self) -> Optional[pyspark.resource.profile.ResourceProfile]\n","     |      Get the :class:`pyspark.resource.ResourceProfile` specified with this RDD or None\n","     |      if it wasn't specified.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      class:`pyspark.resource.ResourceProfile`\n","     |          The user specified profile or None if none were specified\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.withResources`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This API is experimental\n","     |  \n","     |  getStorageLevel(self) -> pyspark.storagelevel.StorageLevel\n","     |      Get the RDD's current storage level.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`StorageLevel`\n","     |          current :class:`StorageLevel`\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.name`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1,2])\n","     |      >>> rdd.getStorageLevel()\n","     |      StorageLevel(False, False, False, False, 1)\n","     |      >>> print(rdd.getStorageLevel())\n","     |      Serialized 1x Replicated\n","     |  \n","     |  glom(self: 'RDD[T]') -> 'RDD[List[T]]'\n","     |      Return an RDD created by coalescing all elements within each partition\n","     |      into a list.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD` coalescing all elements within each partition into a list\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n","     |      >>> sorted(rdd.glom().collect())\n","     |      [[1, 2], [3, 4]]\n","     |  \n","     |  groupBy(self: 'RDD[T]', f: Callable[[~T], ~K], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7d0fe60c3a60>) -> 'RDD[Tuple[K, Iterable[T]]]'\n","     |      Return an RDD of grouped items.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |          a function to compute the key\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      partitionFunc : function, optional, default `portable_hash`\n","     |          a function to compute the partition index\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD` of grouped items\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.groupByKey`\n","     |      :meth:`pyspark.sql.DataFrame.groupBy`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n","     |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n","     |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n","     |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n","     |  \n","     |  groupByKey(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7d0fe60c3a60>) -> 'RDD[Tuple[K, Iterable[V]]]'\n","     |      Group the values for each key in the RDD into a single sequence.\n","     |      Hash-partitions the resulting RDD with numPartitions partitions.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      partitionFunc : function, optional, default `portable_hash`\n","     |          function to compute the partition index\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing the keys and the grouped result for each key\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.reduceByKey`\n","     |      :meth:`RDD.combineByKey`\n","     |      :meth:`RDD.aggregateByKey`\n","     |      :meth:`RDD.foldByKey`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      If you are grouping in order to perform an aggregation (such as a\n","     |      sum or average) over each key, using reduceByKey or aggregateByKey will\n","     |      provide much better performance.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n","     |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n","     |      [('a', 2), ('b', 1)]\n","     |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n","     |      [('a', [1, 1]), ('b', [1])]\n","     |  \n","     |  groupWith(self: 'RDD[Tuple[Any, Any]]', other: 'RDD[Tuple[Any, Any]]', *others: 'RDD[Tuple[Any, Any]]') -> 'RDD[Tuple[Any, Tuple[ResultIterable[Any], ...]]]'\n","     |      Alias for cogroup but with support for multiple RDDs.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      other : :class:`RDD`\n","     |          another :class:`RDD`\n","     |      others : :class:`RDD`\n","     |          other :class:`RDD`\\s\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing the keys and cogrouped values\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.cogroup`\n","     |      :meth:`RDD.join`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd1 = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n","     |      >>> rdd2 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n","     |      >>> rdd3 = sc.parallelize([(\"a\", 2)])\n","     |      >>> rdd4 = sc.parallelize([(\"b\", 42)])\n","     |      >>> [(x, tuple(map(list, y))) for x, y in\n","     |      ...     sorted(list(rdd1.groupWith(rdd2, rdd3, rdd4).collect()))]\n","     |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n","     |  \n","     |  histogram(self: 'RDD[S]', buckets: Union[int, List[ForwardRef('S')], Tuple[ForwardRef('S'), ...]]) -> Tuple[Sequence[ForwardRef('S')], List[int]]\n","     |      Compute a histogram using the provided buckets. The buckets\n","     |      are all open to the right except for the last which is closed.\n","     |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n","     |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n","     |      and 50 we would have a histogram of 1,0,1.\n","     |      \n","     |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n","     |      this can be switched from an O(log n) insertion to O(1) per\n","     |      element (where n is the number of buckets).\n","     |      \n","     |      Buckets must be sorted, not contain any duplicates, and have\n","     |      at least two elements.\n","     |      \n","     |      If `buckets` is a number, it will generate buckets which are\n","     |      evenly spaced between the minimum and maximum of the RDD. For\n","     |      example, if the min value is 0 and the max is 100, given `buckets`\n","     |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n","     |      be at least 1. An exception is raised if the RDD contains infinity.\n","     |      If the elements in the RDD do not vary (max == min), a single bucket\n","     |      will be used.\n","     |      \n","     |      .. versionadded:: 1.2.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      buckets : int, or list, or tuple\n","     |          if `buckets` is a number, it computes a histogram of the data using\n","     |          `buckets` number of buckets evenly, otherwise, `buckets` is the provided\n","     |          buckets to bin the data.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      tuple\n","     |          a tuple of buckets and histogram\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.stats`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize(range(51))\n","     |      >>> rdd.histogram(2)\n","     |      ([0, 25, 50], [25, 26])\n","     |      >>> rdd.histogram([0, 5, 25, 50])\n","     |      ([0, 5, 25, 50], [5, 20, 26])\n","     |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n","     |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n","     |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n","     |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n","     |      (('a', 'b', 'c'), [2, 2])\n","     |  \n","     |  id(self) -> int\n","     |      A unique ID for this RDD (within its SparkContext).\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          The unique ID for this :class:`RDD`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.range(5)\n","     |      >>> rdd.id()  # doctest: +SKIP\n","     |      3\n","     |  \n","     |  intersection(self: 'RDD[T]', other: 'RDD[T]') -> 'RDD[T]'\n","     |      Return the intersection of this RDD and another one. The output will\n","     |      not contain any duplicate elements, even if the input RDDs did.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      other : :class:`RDD`\n","     |          another :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          the intersection of this :class:`RDD` and another one\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`pyspark.sql.DataFrame.intersect`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This method performs a shuffle internally.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n","     |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n","     |      >>> rdd1.intersection(rdd2).collect()\n","     |      [1, 2, 3]\n","     |  \n","     |  isCheckpointed(self) -> bool\n","     |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      bool\n","     |          whether this :class:`RDD` is checkpointed and materialized, either reliably or locally\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.checkpoint`\n","     |      :meth:`RDD.getCheckpointFile`\n","     |      :meth:`SparkContext.setCheckpointDir`\n","     |      :meth:`SparkContext.getCheckpointDir`\n","     |  \n","     |  isEmpty(self) -> bool\n","     |      Returns true if and only if the RDD contains no elements at all.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      bool\n","     |          whether the :class:`RDD` is empty\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.first`\n","     |      :meth:`pyspark.sql.DataFrame.isEmpty`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      An RDD may be empty even when it has at least 1 partition.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([]).isEmpty()\n","     |      True\n","     |      >>> sc.parallelize([1]).isEmpty()\n","     |      False\n","     |  \n","     |  isLocallyCheckpointed(self) -> bool\n","     |      Return whether this RDD is marked for local checkpointing.\n","     |      \n","     |      Exposed for testing.\n","     |      \n","     |      .. versionadded:: 2.2.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      bool\n","     |          whether this :class:`RDD` is marked for local checkpointing\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.localCheckpoint`\n","     |  \n","     |  join(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[V, U]]]'\n","     |      Return an RDD containing all pairs of elements with matching keys in\n","     |      `self` and `other`.\n","     |      \n","     |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n","     |      (k, v1) is in `self` and (k, v2) is in `other`.\n","     |      \n","     |      Performs a hash join across the cluster.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      other : :class:`RDD`\n","     |          another :class:`RDD`\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing all pairs of elements with matching keys\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.leftOuterJoin`\n","     |      :meth:`RDD.rightOuterJoin`\n","     |      :meth:`RDD.fullOuterJoin`\n","     |      :meth:`RDD.cogroup`\n","     |      :meth:`RDD.groupWith`\n","     |      :meth:`pyspark.sql.DataFrame.join`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n","     |      >>> rdd2 = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n","     |      >>> sorted(rdd1.join(rdd2).collect())\n","     |      [('a', (1, 2)), ('a', (1, 3))]\n","     |  \n","     |  keyBy(self: 'RDD[T]', f: Callable[[~T], ~K]) -> 'RDD[Tuple[K, T]]'\n","     |      Creates tuples of the elements in this RDD by applying `f`.\n","     |      \n","     |      .. versionadded:: 0.9.1\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |          a function to compute the key\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` with the elements from this that are not in `other`\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.map`\n","     |      :meth:`RDD.keys`\n","     |      :meth:`RDD.values`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd1 = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n","     |      >>> rdd2 = sc.parallelize(zip(range(0,5), range(0,5)))\n","     |      >>> [(x, list(map(list, y))) for x, y in sorted(rdd1.cogroup(rdd2).collect())]\n","     |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n","     |  \n","     |  keys(self: 'RDD[Tuple[K, V]]') -> 'RDD[K]'\n","     |      Return an RDD with the keys of each tuple.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` only containing the keys\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.values`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([(1, 2), (3, 4)]).keys()\n","     |      >>> rdd.collect()\n","     |      [1, 3]\n","     |  \n","     |  leftOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[V, Optional[U]]]]'\n","     |      Perform a left outer join of `self` and `other`.\n","     |      \n","     |      For each element (k, v) in `self`, the resulting RDD will either\n","     |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n","     |      (k, (v, None)) if no elements in `other` have key k.\n","     |      \n","     |      Hash-partitions the resulting RDD into the given number of partitions.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      other : :class:`RDD`\n","     |          another :class:`RDD`\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing all pairs of elements with matching keys\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.join`\n","     |      :meth:`RDD.rightOuterJoin`\n","     |      :meth:`RDD.fullOuterJoin`\n","     |      :meth:`pyspark.sql.DataFrame.join`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n","     |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n","     |      >>> sorted(rdd1.leftOuterJoin(rdd2).collect())\n","     |      [('a', (1, 2)), ('b', (4, None))]\n","     |  \n","     |  localCheckpoint(self) -> None\n","     |      Mark this RDD for local checkpointing using Spark's existing caching layer.\n","     |      \n","     |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n","     |      step of replicating the materialized data in a reliable distributed file system. This is\n","     |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n","     |      \n","     |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n","     |      data is written to ephemeral local storage in the executors instead of to a reliable,\n","     |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n","     |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n","     |      \n","     |      This is NOT safe to use with dynamic allocation, which removes executors along\n","     |      with their cached blocks. If you must use both features, you are advised to set\n","     |      `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.\n","     |      \n","     |      The checkpoint directory set through :meth:`SparkContext.setCheckpointDir` is not used.\n","     |      \n","     |      .. versionadded:: 2.2.0\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.checkpoint`\n","     |      :meth:`RDD.isLocallyCheckpointed`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.range(5)\n","     |      >>> rdd.isLocallyCheckpointed()\n","     |      False\n","     |      \n","     |      >>> rdd.localCheckpoint()\n","     |      >>> rdd.isLocallyCheckpointed()\n","     |      True\n","     |  \n","     |  lookup(self: 'RDD[Tuple[K, V]]', key: ~K) -> List[~V]\n","     |      Return the list of values in the RDD for key `key`. This operation\n","     |      is done efficiently if the RDD has a known partitioner by only\n","     |      searching the partition that the key maps to.\n","     |      \n","     |      .. versionadded:: 1.2.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      key : K\n","     |          the key to look up\n","     |      \n","     |      Returns\n","     |      -------\n","     |      list\n","     |          the list of values in the :class:`RDD` for key `key`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> l = range(1000)\n","     |      >>> rdd = sc.parallelize(zip(l, l), 10)\n","     |      >>> rdd.lookup(42)  # slow\n","     |      [42]\n","     |      >>> sorted = rdd.sortByKey()\n","     |      >>> sorted.lookup(42)  # fast\n","     |      [42]\n","     |      >>> sorted.lookup(1024)\n","     |      []\n","     |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n","     |      >>> list(rdd2.lookup(('a', 'b'))[0])\n","     |      ['c']\n","     |  \n","     |  map(self: 'RDD[T]', f: Callable[[~T], ~U], preservesPartitioning: bool = False) -> 'RDD[U]'\n","     |      Return a new RDD by applying a function to each element of this RDD.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |          a function to run on each element of the RDD\n","     |      preservesPartitioning : bool, optional, default False\n","     |          indicates whether the input function preserves the partitioner,\n","     |          which should be False unless this is a pair RDD and the input\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD` by applying a function to all elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.flatMap`\n","     |      :meth:`RDD.mapPartitions`\n","     |      :meth:`RDD.mapPartitionsWithIndex`\n","     |      :meth:`RDD.mapPartitionsWithSplit`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n","     |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n","     |      [('a', 1), ('b', 1), ('c', 1)]\n","     |  \n","     |  mapPartitions(self: 'RDD[T]', f: Callable[[Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n","     |      Return a new RDD by applying a function to each partition of this RDD.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |          a function to run on each partition of the RDD\n","     |      preservesPartitioning : bool, optional, default False\n","     |          indicates whether the input function preserves the partitioner,\n","     |          which should be False unless this is a pair RDD and the input\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD` by applying a function to each partition\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.map`\n","     |      :meth:`RDD.flatMap`\n","     |      :meth:`RDD.mapPartitionsWithIndex`\n","     |      :meth:`RDD.mapPartitionsWithSplit`\n","     |      :meth:`RDDBarrier.mapPartitions`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n","     |      >>> def f(iterator): yield sum(iterator)\n","     |      ...\n","     |      >>> rdd.mapPartitions(f).collect()\n","     |      [3, 7]\n","     |  \n","     |  mapPartitionsWithIndex(self: 'RDD[T]', f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n","     |      Return a new RDD by applying a function to each partition of this RDD,\n","     |      while tracking the index of the original partition.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |          a function to run on each partition of the RDD\n","     |      preservesPartitioning : bool, optional, default False\n","     |          indicates whether the input function preserves the partitioner,\n","     |          which should be False unless this is a pair RDD and the input\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD` by applying a function to each partition\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.map`\n","     |      :meth:`RDD.flatMap`\n","     |      :meth:`RDD.mapPartitions`\n","     |      :meth:`RDD.mapPartitionsWithSplit`\n","     |      :meth:`RDDBarrier.mapPartitionsWithIndex`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n","     |      >>> def f(splitIndex, iterator): yield splitIndex\n","     |      ...\n","     |      >>> rdd.mapPartitionsWithIndex(f).sum()\n","     |      6\n","     |  \n","     |  mapPartitionsWithSplit(self: 'RDD[T]', f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n","     |      Return a new RDD by applying a function to each partition of this RDD,\n","     |      while tracking the index of the original partition.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      .. deprecated:: 0.9.0\n","     |          use meth:`RDD.mapPartitionsWithIndex` instead.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |          a function to run on each partition of the RDD\n","     |      preservesPartitioning : bool, optional, default False\n","     |          indicates whether the input function preserves the partitioner,\n","     |          which should be False unless this is a pair RDD and the input\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD` by applying a function to each partition\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.map`\n","     |      :meth:`RDD.flatMap`\n","     |      :meth:`RDD.mapPartitions`\n","     |      :meth:`RDD.mapPartitionsWithIndex`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n","     |      >>> def f(splitIndex, iterator): yield splitIndex\n","     |      ...\n","     |      >>> rdd.mapPartitionsWithSplit(f).sum()\n","     |      6\n","     |  \n","     |  mapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], ~U]) -> 'RDD[Tuple[K, U]]'\n","     |      Pass each value in the key-value pair RDD through a map function\n","     |      without changing the keys; this also retains the original RDD's\n","     |      partitioning.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |         a function to turn a V into a U\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing the keys and the mapped value\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.map`\n","     |      :meth:`RDD.flatMapValues`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n","     |      >>> def f(x): return len(x)\n","     |      ...\n","     |      >>> rdd.mapValues(f).collect()\n","     |      [('a', 3), ('b', 1)]\n","     |  \n","     |  max(self: 'RDD[T]', key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> ~T\n","     |      Find the maximum item in this RDD.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      key : function, optional\n","     |          A function used to generate key for comparing\n","     |      \n","     |      Returns\n","     |      -------\n","     |      T\n","     |          the maximum item\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.min`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n","     |      >>> rdd.max()\n","     |      43.0\n","     |      >>> rdd.max(key=str)\n","     |      5.0\n","     |  \n","     |  mean(self: 'RDD[NumberOrArray]') -> float\n","     |      Compute the mean of this RDD's elements.\n","     |      \n","     |      .. versionadded:: 0.9.1\n","     |      \n","     |      Returns\n","     |      -------\n","     |      float\n","     |          the mean of all elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.stats`\n","     |      :meth:`RDD.sum`\n","     |      :meth:`RDD.meanApprox`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([1, 2, 3]).mean()\n","     |      2.0\n","     |  \n","     |  meanApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n","     |      Approximate operation to return the mean within a timeout\n","     |      or meet the confidence.\n","     |      \n","     |      .. versionadded:: 1.2.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      timeout : int\n","     |          maximum time to wait for the job, in milliseconds\n","     |      confidence : float\n","     |          the desired statistical confidence in the result\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`BoundedFloat`\n","     |          a potentially incomplete result, with error bounds\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.mean`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize(range(1000), 10)\n","     |      >>> r = sum(range(1000)) / 1000.0\n","     |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n","     |      True\n","     |  \n","     |  min(self: 'RDD[T]', key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> ~T\n","     |      Find the minimum item in this RDD.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      key : function, optional\n","     |          A function used to generate key for comparing\n","     |      \n","     |      Returns\n","     |      -------\n","     |      T\n","     |          the minimum item\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.max`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n","     |      >>> rdd.min()\n","     |      2.0\n","     |      >>> rdd.min(key=str)\n","     |      10.0\n","     |  \n","     |  name(self) -> Optional[str]\n","     |      Return the name of this RDD.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      str\n","     |          :class:`RDD` name\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.setName`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.range(5)\n","     |      >>> rdd.name() == None\n","     |      True\n","     |  \n","     |  partitionBy(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int], partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7d0fe60c3a60>) -> 'RDD[Tuple[K, V]]'\n","     |      Return a copy of the RDD partitioned using the specified partitioner.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      partitionFunc : function, optional, default `portable_hash`\n","     |          function to compute the partition index\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` partitioned using the specified partitioner\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.repartition`\n","     |      :meth:`RDD.repartitionAndSortWithinPartitions`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n","     |      >>> sets = pairs.partitionBy(2).glom().collect()\n","     |      >>> len(set(sets[0]).intersection(set(sets[1])))\n","     |      0\n","     |  \n","     |  persist(self: 'RDD[T]', storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(False, True, False, False, 1)) -> 'RDD[T]'\n","     |      Set this RDD's storage level to persist its values across operations\n","     |      after the first time it is computed. This can only be used to assign\n","     |      a new storage level if the RDD does not have a storage level set yet.\n","     |      If no storage level is specified defaults to (`MEMORY_ONLY`).\n","     |      \n","     |      .. versionadded:: 0.9.1\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      storageLevel : :class:`StorageLevel`, default `MEMORY_ONLY`\n","     |          the target storage level\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          The same :class:`RDD` with storage level set to `storageLevel`.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.cache`\n","     |      :meth:`RDD.unpersist`\n","     |      :meth:`RDD.getStorageLevel`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n","     |      >>> rdd.persist().is_cached\n","     |      True\n","     |      >>> str(rdd.getStorageLevel())\n","     |      'Memory Serialized 1x Replicated'\n","     |      >>> _ = rdd.unpersist()\n","     |      >>> rdd.is_cached\n","     |      False\n","     |      \n","     |      >>> from pyspark import StorageLevel\n","     |      >>> rdd2 = sc.range(5)\n","     |      >>> _ = rdd2.persist(StorageLevel.MEMORY_AND_DISK)\n","     |      >>> rdd2.is_cached\n","     |      True\n","     |      >>> str(rdd2.getStorageLevel())\n","     |      'Disk Memory Serialized 1x Replicated'\n","     |      \n","     |      Can not override existing storage level\n","     |      \n","     |      >>> _ = rdd2.persist(StorageLevel.MEMORY_ONLY_2)\n","     |      Traceback (most recent call last):\n","     |          ...\n","     |      py4j.protocol.Py4JJavaError: ...\n","     |      \n","     |      Assign another storage level after `unpersist`\n","     |      \n","     |      >>> _ = rdd2.unpersist()\n","     |      >>> rdd2.is_cached\n","     |      False\n","     |      >>> _ = rdd2.persist(StorageLevel.MEMORY_ONLY_2)\n","     |      >>> str(rdd2.getStorageLevel())\n","     |      'Memory Serialized 2x Replicated'\n","     |      >>> rdd2.is_cached\n","     |      True\n","     |      >>> _ = rdd2.unpersist()\n","     |  \n","     |  pipe(self, command: str, env: Optional[Dict[str, str]] = None, checkCode: bool = False) -> 'RDD[str]'\n","     |      Return an RDD created by piping elements to a forked external process.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      command : str\n","     |          command to run.\n","     |      env : dict, optional\n","     |          environment variables to set.\n","     |      checkCode : bool, optional\n","     |          whether to check the return value of the shell command.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD` of strings\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n","     |      ['1', '2', '', '3']\n","     |  \n","     |  randomSplit(self: 'RDD[T]', weights: Sequence[Union[int, float]], seed: Optional[int] = None) -> 'List[RDD[T]]'\n","     |      Randomly splits this RDD with the provided weights.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      weights : list\n","     |          weights for splits, will be normalized if they don't sum to 1\n","     |      seed : int, optional\n","     |          random seed\n","     |      \n","     |      Returns\n","     |      -------\n","     |      list\n","     |          split :class:`RDD`\\s in a list\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`pyspark.sql.DataFrame.randomSplit`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize(range(500), 1)\n","     |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n","     |      >>> len(rdd1.collect() + rdd2.collect())\n","     |      500\n","     |      >>> 150 < rdd1.count() < 250\n","     |      True\n","     |      >>> 250 < rdd2.count() < 350\n","     |      True\n","     |  \n","     |  reduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T]) -> ~T\n","     |      Reduces the elements of this RDD using the specified commutative and\n","     |      associative binary operator. Currently reduces partitions locally.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |          the reduce function\n","     |      \n","     |      Returns\n","     |      -------\n","     |      T\n","     |          the aggregated result\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.treeReduce`\n","     |      :meth:`RDD.aggregate`\n","     |      :meth:`RDD.treeAggregate`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> from operator import add\n","     |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n","     |      15\n","     |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n","     |      10\n","     |      >>> sc.parallelize([]).reduce(add)\n","     |      Traceback (most recent call last):\n","     |          ...\n","     |      ValueError: Can not reduce() empty RDD\n","     |  \n","     |  reduceByKey(self: 'RDD[Tuple[K, V]]', func: Callable[[~V, ~V], ~V], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7d0fe60c3a60>) -> 'RDD[Tuple[K, V]]'\n","     |      Merge the values for each key using an associative and commutative reduce function.\n","     |      \n","     |      This will also perform the merging locally on each mapper before\n","     |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n","     |      \n","     |      Output will be partitioned with `numPartitions` partitions, or\n","     |      the default parallelism level if `numPartitions` is not specified.\n","     |      Default partitioner is hash-partition.\n","     |      \n","     |      .. versionadded:: 1.6.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      func : function\n","     |          the reduce function\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      partitionFunc : function, optional, default `portable_hash`\n","     |          function to compute the partition index\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing the keys and the aggregated result for each key\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.reduceByKeyLocally`\n","     |      :meth:`RDD.combineByKey`\n","     |      :meth:`RDD.aggregateByKey`\n","     |      :meth:`RDD.foldByKey`\n","     |      :meth:`RDD.groupByKey`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> from operator import add\n","     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n","     |      >>> sorted(rdd.reduceByKey(add).collect())\n","     |      [('a', 2), ('b', 1)]\n","     |  \n","     |  reduceByKeyLocally(self: 'RDD[Tuple[K, V]]', func: Callable[[~V, ~V], ~V]) -> Dict[~K, ~V]\n","     |      Merge the values for each key using an associative and commutative reduce function, but\n","     |      return the results immediately to the master as a dictionary.\n","     |      \n","     |      This will also perform the merging locally on each mapper before\n","     |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      func : function\n","     |          the reduce function\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          a dict containing the keys and the aggregated result for each key\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.reduceByKey`\n","     |      :meth:`RDD.aggregateByKey`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> from operator import add\n","     |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n","     |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n","     |      [('a', 2), ('b', 1)]\n","     |  \n","     |  repartition(self: 'RDD[T]', numPartitions: int) -> 'RDD[T]'\n","     |       Return a new RDD that has exactly numPartitions partitions.\n","     |      \n","     |       Can increase or decrease the level of parallelism in this RDD.\n","     |       Internally, this uses a shuffle to redistribute data.\n","     |       If you are decreasing the number of partitions in this RDD, consider\n","     |       using `coalesce`, which can avoid performing a shuffle.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` with exactly numPartitions partitions\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.coalesce`\n","     |      :meth:`RDD.partitionBy`\n","     |      :meth:`RDD.repartitionAndSortWithinPartitions`\n","     |      \n","     |      Examples\n","     |      --------\n","     |       >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n","     |       >>> sorted(rdd.glom().collect())\n","     |       [[1], [2, 3], [4, 5], [6, 7]]\n","     |       >>> len(rdd.repartition(2).glom().collect())\n","     |       2\n","     |       >>> len(rdd.repartition(10).glom().collect())\n","     |       10\n","     |  \n","     |  repartitionAndSortWithinPartitions(self: 'RDD[Tuple[Any, Any]]', numPartitions: Optional[int] = None, partitionFunc: Callable[[Any], int] = <function portable_hash at 0x7d0fe60c3a60>, ascending: bool = True, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7d0fe5f03060>) -> 'RDD[Tuple[Any, Any]]'\n","     |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n","     |      sort records by their keys.\n","     |      \n","     |      .. versionadded:: 1.2.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      partitionFunc : function, optional, default `portable_hash`\n","     |          a function to compute the partition index\n","     |      ascending : bool, optional, default True\n","     |          sort the keys in ascending or descending order\n","     |      keyfunc : function, optional, default identity mapping\n","     |          a function to compute the key\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD`\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.repartition`\n","     |      :meth:`RDD.partitionBy`\n","     |      :meth:`RDD.sortBy`\n","     |      :meth:`RDD.sortByKey`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n","     |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n","     |      >>> rdd2.glom().collect()\n","     |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n","     |  \n","     |  rightOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], U]]]'\n","     |      Perform a right outer join of `self` and `other`.\n","     |      \n","     |      For each element (k, w) in `other`, the resulting RDD will either\n","     |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n","     |      if no elements in `self` have key k.\n","     |      \n","     |      Hash-partitions the resulting RDD into the given number of partitions.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      other : :class:`RDD`\n","     |          another :class:`RDD`\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing all pairs of elements with matching keys\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.join`\n","     |      :meth:`RDD.leftOuterJoin`\n","     |      :meth:`RDD.fullOuterJoin`\n","     |      :meth:`pyspark.sql.DataFrame.join`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n","     |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n","     |      >>> sorted(rdd2.rightOuterJoin(rdd1).collect())\n","     |      [('a', (2, 1)), ('b', (None, 4))]\n","     |  \n","     |  sample(self: 'RDD[T]', withReplacement: bool, fraction: float, seed: Optional[int] = None) -> 'RDD[T]'\n","     |      Return a sampled subset of this RDD.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      withReplacement : bool\n","     |          can elements be sampled multiple times (replaced when sampled out)\n","     |      fraction : float\n","     |          expected size of the sample as a fraction of this RDD's size\n","     |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n","     |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n","     |      seed : int, optional\n","     |          seed for the random number generator\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD` containing a sampled subset of elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.takeSample`\n","     |      :meth:`RDD.sampleByKey`\n","     |      :meth:`pyspark.sql.DataFrame.sample`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This is not guaranteed to provide exactly the fraction specified of the total\n","     |      count of the given :class:`DataFrame`.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize(range(100), 4)\n","     |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n","     |      True\n","     |  \n","     |  sampleByKey(self: 'RDD[Tuple[K, V]]', withReplacement: bool, fractions: Dict[~K, Union[float, int]], seed: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n","     |      Return a subset of this RDD sampled by key (via stratified sampling).\n","     |      Create a sample of this RDD using variable sampling rates for\n","     |      different keys as specified by fractions, a key to sampling rate map.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      withReplacement : bool\n","     |          whether to sample with or without replacement\n","     |      fractions : dict\n","     |          map of specific keys to sampling rates\n","     |      seed : int, optional\n","     |          seed for the random number generator\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing the stratified sampling result\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.sample`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n","     |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n","     |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n","     |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n","     |      True\n","     |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n","     |      True\n","     |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n","     |      True\n","     |  \n","     |  sampleStdev(self: 'RDD[NumberOrArray]') -> float\n","     |      Compute the sample standard deviation of this RDD's elements (which\n","     |      corrects for bias in estimating the standard deviation by dividing by\n","     |      N-1 instead of N).\n","     |      \n","     |      .. versionadded:: 0.9.1\n","     |      \n","     |      Returns\n","     |      -------\n","     |      float\n","     |          the sample standard deviation of all elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.stats`\n","     |      :meth:`RDD.stdev`\n","     |      :meth:`RDD.variance`\n","     |      :meth:`RDD.sampleVariance`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n","     |      1.0\n","     |  \n","     |  sampleVariance(self: 'RDD[NumberOrArray]') -> float\n","     |      Compute the sample variance of this RDD's elements (which corrects\n","     |      for bias in estimating the variance by dividing by N-1 instead of N).\n","     |      \n","     |      .. versionadded:: 0.9.1\n","     |      \n","     |      Returns\n","     |      -------\n","     |      float\n","     |          the sample variance of all elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.stats`\n","     |      :meth:`RDD.variance`\n","     |      :meth:`RDD.stdev`\n","     |      :meth:`RDD.sampleStdev`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n","     |      1.0\n","     |  \n","     |  saveAsHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n","     |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n","     |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n","     |      converted for output using either user specified converters or, by default,\n","     |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      conf : dict\n","     |          Hadoop job configuration\n","     |      keyConverter : str, optional\n","     |          fully qualified classname of key converter (None by default)\n","     |      valueConverter : str, optional\n","     |          fully qualified classname of value converter (None by default)\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.hadoopRDD`\n","     |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n","     |      :meth:`RDD.saveAsHadoopFile`\n","     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n","     |      :meth:`RDD.saveAsSequenceFile`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      \n","     |      Set the related classes\n","     |      \n","     |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n","     |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n","     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n","     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"old_hadoop_file\")\n","     |      ...\n","     |      ...     # Create the conf for writing\n","     |      ...     write_conf = {\n","     |      ...         \"mapred.output.format.class\": output_format_class,\n","     |      ...         \"mapreduce.job.output.key.class\": key_class,\n","     |      ...         \"mapreduce.job.output.value.class\": value_class,\n","     |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n","     |      ...     }\n","     |      ...\n","     |      ...     # Write a temporary Hadoop file\n","     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n","     |      ...     rdd.saveAsHadoopDataset(conf=write_conf)\n","     |      ...\n","     |      ...     # Create the conf for reading\n","     |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n","     |      ...\n","     |      ...     # Load this Hadoop file as an RDD\n","     |      ...     loaded = sc.hadoopRDD(input_format_class, key_class, value_class, conf=read_conf)\n","     |      ...     sorted(loaded.collect())\n","     |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n","     |  \n","     |  saveAsHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, compressionCodecClass: Optional[str] = None) -> None\n","     |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n","     |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n","     |      will be inferred if not specified. Keys and values are converted for output using either\n","     |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n","     |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n","     |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          path to Hadoop file\n","     |      outputFormatClass : str\n","     |          fully qualified classname of Hadoop OutputFormat\n","     |          (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n","     |      keyClass : str, optional\n","     |          fully qualified classname of key Writable class\n","     |          (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n","     |      valueClass : str, optional\n","     |          fully qualified classname of value Writable class\n","     |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n","     |      keyConverter : str, optional\n","     |          fully qualified classname of key converter (None by default)\n","     |      valueConverter : str, optional\n","     |          fully qualified classname of value converter (None by default)\n","     |      conf : dict, optional\n","     |          (None by default)\n","     |      compressionCodecClass : str\n","     |          fully qualified classname of the compression codec class\n","     |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.hadoopFile`\n","     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n","     |      :meth:`RDD.saveAsHadoopDataset`\n","     |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n","     |      :meth:`RDD.saveAsSequenceFile`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      \n","     |      Set the related classes\n","     |      \n","     |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n","     |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n","     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n","     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"old_hadoop_file\")\n","     |      ...\n","     |      ...     # Write a temporary Hadoop file\n","     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n","     |      ...     rdd.saveAsHadoopFile(path, output_format_class, key_class, value_class)\n","     |      ...\n","     |      ...     # Load this Hadoop file as an RDD\n","     |      ...     loaded = sc.hadoopFile(path, input_format_class, key_class, value_class)\n","     |      ...     sorted(loaded.collect())\n","     |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n","     |  \n","     |  saveAsNewAPIHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n","     |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n","     |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n","     |      converted for output using either user specified converters or, by default,\n","     |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      conf : dict\n","     |          Hadoop job configuration\n","     |      keyConverter : str, optional\n","     |          fully qualified classname of key converter (None by default)\n","     |      valueConverter : str, optional\n","     |          fully qualified classname of value converter (None by default)\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.newAPIHadoopRDD`\n","     |      :meth:`RDD.saveAsHadoopDataset`\n","     |      :meth:`RDD.saveAsHadoopFile`\n","     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n","     |      :meth:`RDD.saveAsSequenceFile`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      \n","     |      Set the related classes\n","     |      \n","     |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n","     |      >>> input_format_class = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n","     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n","     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"new_hadoop_file\")\n","     |      ...\n","     |      ...     # Create the conf for writing\n","     |      ...     write_conf = {\n","     |      ...         \"mapreduce.job.outputformat.class\": (output_format_class),\n","     |      ...         \"mapreduce.job.output.key.class\": key_class,\n","     |      ...         \"mapreduce.job.output.value.class\": value_class,\n","     |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n","     |      ...     }\n","     |      ...\n","     |      ...     # Write a temporary Hadoop file\n","     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n","     |      ...     rdd.saveAsNewAPIHadoopDataset(conf=write_conf)\n","     |      ...\n","     |      ...     # Create the conf for reading\n","     |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n","     |      ...\n","     |      ...     # Load this Hadoop file as an RDD\n","     |      ...     loaded = sc.newAPIHadoopRDD(input_format_class,\n","     |      ...         key_class, value_class, conf=read_conf)\n","     |      ...     sorted(loaded.collect())\n","     |      [(1, ''), (1, 'a'), (3, 'x')]\n","     |  \n","     |  saveAsNewAPIHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None) -> None\n","     |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n","     |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n","     |      will be inferred if not specified. Keys and values are converted for output using either\n","     |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n","     |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n","     |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          path to Hadoop file\n","     |      outputFormatClass : str\n","     |          fully qualified classname of Hadoop OutputFormat\n","     |          (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n","     |      keyClass : str, optional\n","     |          fully qualified classname of key Writable class\n","     |           (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n","     |      valueClass : str, optional\n","     |          fully qualified classname of value Writable class\n","     |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n","     |      keyConverter : str, optional\n","     |          fully qualified classname of key converter (None by default)\n","     |      valueConverter : str, optional\n","     |          fully qualified classname of value converter (None by default)\n","     |      conf : dict, optional\n","     |          Hadoop job configuration (None by default)\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.newAPIHadoopFile`\n","     |      :meth:`RDD.saveAsHadoopDataset`\n","     |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n","     |      :meth:`RDD.saveAsHadoopFile`\n","     |      :meth:`RDD.saveAsSequenceFile`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      \n","     |      Set the class of output format\n","     |      \n","     |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"hadoop_file\")\n","     |      ...\n","     |      ...     # Write a temporary Hadoop file\n","     |      ...     rdd = sc.parallelize([(1, {3.0: \"bb\"}), (2, {1.0: \"aa\"}), (3, {2.0: \"dd\"})])\n","     |      ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class)\n","     |      ...\n","     |      ...     # Load this Hadoop file as an RDD\n","     |      ...     sorted(sc.sequenceFile(path).collect())\n","     |      [(1, {3.0: 'bb'}), (2, {1.0: 'aa'}), (3, {2.0: 'dd'})]\n","     |  \n","     |  saveAsPickleFile(self, path: str, batchSize: int = 10) -> None\n","     |      Save this RDD as a SequenceFile of serialized objects. The serializer\n","     |      used is :class:`pyspark.serializers.CPickleSerializer`, default batch size\n","     |      is 10.\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          path to pickled file\n","     |      batchSize : int, optional, default 10\n","     |          the number of Python objects represented as a single Java object.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.pickleFile`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"pickle_file\")\n","     |      ...\n","     |      ...     # Write a temporary pickled file\n","     |      ...     sc.parallelize(range(10)).saveAsPickleFile(path, 3)\n","     |      ...\n","     |      ...     # Load picked file as an RDD\n","     |      ...     sorted(sc.pickleFile(path, 3).collect())\n","     |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","     |  \n","     |  saveAsSequenceFile(self: 'RDD[Tuple[K, V]]', path: str, compressionCodecClass: Optional[str] = None) -> None\n","     |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n","     |      system, using the \"org.apache.hadoop.io.Writable\" types that we convert from the\n","     |      RDD's key and value types. The mechanism is as follows:\n","     |      \n","     |          1. Pickle is used to convert pickled Python RDD into RDD of Java objects.\n","     |          2. Keys and values of this Java RDD are converted to Writables and written out.\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          path to sequence file\n","     |      compressionCodecClass : str, optional\n","     |          fully qualified classname of the compression codec class\n","     |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.sequenceFile`\n","     |      :meth:`RDD.saveAsHadoopFile`\n","     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n","     |      :meth:`RDD.saveAsHadoopDataset`\n","     |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n","     |      :meth:`RDD.saveAsSequenceFile`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      \n","     |      Set the related classes\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"sequence_file\")\n","     |      ...\n","     |      ...     # Write a temporary sequence file\n","     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n","     |      ...     rdd.saveAsSequenceFile(path)\n","     |      ...\n","     |      ...     # Load this sequence file as an RDD\n","     |      ...     loaded = sc.sequenceFile(path)\n","     |      ...     sorted(loaded.collect())\n","     |      [(1, ''), (1, 'a'), (3, 'x')]\n","     |  \n","     |  saveAsTextFile(self, path: str, compressionCodecClass: Optional[str] = None) -> None\n","     |      Save this RDD as a text file, using string representations of elements.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          path to text file\n","     |      compressionCodecClass : str, optional\n","     |          fully qualified classname of the compression codec class\n","     |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.textFile`\n","     |      :meth:`SparkContext.wholeTextFiles`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      >>> from fileinput import input\n","     |      >>> from glob import glob\n","     |      >>> with tempfile.TemporaryDirectory() as d1:\n","     |      ...     path1 = os.path.join(d1, \"text_file1\")\n","     |      ...\n","     |      ...     # Write a temporary text file\n","     |      ...     sc.parallelize(range(10)).saveAsTextFile(path1)\n","     |      ...\n","     |      ...     # Load text file as an RDD\n","     |      ...     ''.join(sorted(input(glob(path1 + \"/part-0000*\"))))\n","     |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n","     |      \n","     |      Empty lines are tolerated when saving to text files.\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d2:\n","     |      ...     path2 = os.path.join(d2, \"text2_file2\")\n","     |      ...\n","     |      ...     # Write another temporary text file\n","     |      ...     sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(path2)\n","     |      ...\n","     |      ...     # Load text file as an RDD\n","     |      ...     ''.join(sorted(input(glob(path2 + \"/part-0000*\"))))\n","     |      '\\n\\n\\nbar\\nfoo\\n'\n","     |      \n","     |      Using compressionCodecClass\n","     |      \n","     |      >>> from fileinput import input, hook_compressed\n","     |      >>> with tempfile.TemporaryDirectory() as d3:\n","     |      ...     path3 = os.path.join(d3, \"text3\")\n","     |      ...     codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n","     |      ...\n","     |      ...     # Write another temporary text file with specified codec\n","     |      ...     sc.parallelize(['foo', 'bar']).saveAsTextFile(path3, codec)\n","     |      ...\n","     |      ...     # Load text file as an RDD\n","     |      ...     result = sorted(input(glob(path3 + \"/part*.gz\"), openhook=hook_compressed))\n","     |      ...     ''.join([r.decode('utf-8') if isinstance(r, bytes) else r for r in result])\n","     |      'bar\\nfoo\\n'\n","     |  \n","     |  setName(self: 'RDD[T]', name: str) -> 'RDD[T]'\n","     |      Assign a name to this RDD.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      name : str\n","     |          new name\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          the same :class:`RDD` with name updated\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.name`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1, 2])\n","     |      >>> rdd.setName('I am an RDD').name()\n","     |      'I am an RDD'\n","     |  \n","     |  sortBy(self: 'RDD[T]', keyfunc: Callable[[~T], ForwardRef('S')], ascending: bool = True, numPartitions: Optional[int] = None) -> 'RDD[T]'\n","     |      Sorts this RDD by the given keyfunc\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      keyfunc : function\n","     |          a function to compute the key\n","     |      ascending : bool, optional, default True\n","     |          sort the keys in ascending or descending order\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD`\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.sortByKey`\n","     |      :meth:`pyspark.sql.DataFrame.sort`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n","     |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n","     |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n","     |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n","     |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n","     |  \n","     |  sortByKey(self: 'RDD[Tuple[K, V]]', ascending: Optional[bool] = True, numPartitions: Optional[int] = None, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7d0fe5f03420>) -> 'RDD[Tuple[K, V]]'\n","     |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n","     |      \n","     |      .. versionadded:: 0.9.1\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      ascending : bool, optional, default True\n","     |          sort the keys in ascending or descending order\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      keyfunc : function, optional, default identity mapping\n","     |          a function to compute the key\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD`\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.sortBy`\n","     |      :meth:`pyspark.sql.DataFrame.sort`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n","     |      >>> sc.parallelize(tmp).sortByKey().first()\n","     |      ('1', 3)\n","     |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n","     |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n","     |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n","     |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n","     |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n","     |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n","     |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n","     |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n","     |  \n","     |  stats(self: 'RDD[NumberOrArray]') -> pyspark.statcounter.StatCounter\n","     |      Return a :class:`StatCounter` object that captures the mean, variance\n","     |      and count of the RDD's elements in one operation.\n","     |      \n","     |      .. versionadded:: 0.9.1\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`StatCounter`\n","     |          a :class:`StatCounter` capturing the mean, variance and count of all elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.stdev`\n","     |      :meth:`RDD.sampleStdev`\n","     |      :meth:`RDD.variance`\n","     |      :meth:`RDD.sampleVariance`\n","     |      :meth:`RDD.histogram`\n","     |      :meth:`pyspark.sql.DataFrame.stat`\n","     |  \n","     |  stdev(self: 'RDD[NumberOrArray]') -> float\n","     |      Compute the standard deviation of this RDD's elements.\n","     |      \n","     |      .. versionadded:: 0.9.1\n","     |      \n","     |      Returns\n","     |      -------\n","     |      float\n","     |          the standard deviation of all elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.stats`\n","     |      :meth:`RDD.sampleStdev`\n","     |      :meth:`RDD.variance`\n","     |      :meth:`RDD.sampleVariance`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([1, 2, 3]).stdev()\n","     |      0.816...\n","     |  \n","     |  subtract(self: 'RDD[T]', other: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n","     |      Return each value in `self` that is not contained in `other`.\n","     |      \n","     |      .. versionadded:: 0.9.1\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      other : :class:`RDD`\n","     |          another :class:`RDD`\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` with the elements from this that are not in `other`\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.subtractByKey`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n","     |      >>> rdd2 = sc.parallelize([(\"a\", 3), (\"c\", None)])\n","     |      >>> sorted(rdd1.subtract(rdd2).collect())\n","     |      [('a', 1), ('b', 4), ('b', 5)]\n","     |  \n","     |  subtractByKey(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, Any]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n","     |      Return each (key, value) pair in `self` that has no pair with matching\n","     |      key in `other`.\n","     |      \n","     |      .. versionadded:: 0.9.1\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      other : :class:`RDD`\n","     |          another :class:`RDD`\n","     |      numPartitions : int, optional\n","     |          the number of partitions in new :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` with the pairs from this whose keys are not in `other`\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.subtract`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n","     |      >>> rdd2 = sc.parallelize([(\"a\", 3), (\"c\", None)])\n","     |      >>> sorted(rdd1.subtractByKey(rdd2).collect())\n","     |      [('b', 4), ('b', 5)]\n","     |  \n","     |  sum(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n","     |      Add up the elements in this RDD.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      float, int, or complex\n","     |          the sum of all elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.mean`\n","     |      :meth:`RDD.sumApprox`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n","     |      6.0\n","     |  \n","     |  sumApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n","     |      Approximate operation to return the sum within a timeout\n","     |      or meet the confidence.\n","     |      \n","     |      .. versionadded:: 1.2.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      timeout : int\n","     |          maximum time to wait for the job, in milliseconds\n","     |      confidence : float\n","     |          the desired statistical confidence in the result\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`BoundedFloat`\n","     |          a potentially incomplete result, with error bounds\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.sum`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize(range(1000), 10)\n","     |      >>> r = sum(range(1000))\n","     |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n","     |      True\n","     |  \n","     |  take(self: 'RDD[T]', num: int) -> List[~T]\n","     |      Take the first num elements of the RDD.\n","     |      \n","     |      It works by first scanning one partition, and use the results from\n","     |      that partition to estimate the number of additional partitions needed\n","     |      to satisfy the limit.\n","     |      \n","     |      Translated from the Scala implementation in RDD#take().\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      num : int\n","     |          first number of elements\n","     |      \n","     |      Returns\n","     |      -------\n","     |      list\n","     |          the first `num` elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.first`\n","     |      :meth:`pyspark.sql.DataFrame.take`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This method should only be used if the resulting array is expected\n","     |      to be small, as all the data is loaded into the driver's memory.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n","     |      [2, 3]\n","     |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n","     |      [2, 3, 4, 5, 6]\n","     |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n","     |      [91, 92, 93]\n","     |  \n","     |  takeOrdered(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n","     |      Get the N elements from an RDD ordered in ascending order or as\n","     |      specified by the optional key function.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      num : int\n","     |          top N\n","     |      key : function, optional\n","     |          a function used to generate key for comparing\n","     |      \n","     |      Returns\n","     |      -------\n","     |      list\n","     |          the top N elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.top`\n","     |      :meth:`RDD.max`\n","     |      :meth:`RDD.min`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This method should only be used if the resulting array is expected\n","     |      to be small, as all the data is loaded into the driver's memory.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n","     |      [1, 2, 3, 4, 5, 6]\n","     |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n","     |      [10, 9, 7, 6, 5, 4]\n","     |      >>> sc.emptyRDD().takeOrdered(3)\n","     |      []\n","     |  \n","     |  takeSample(self: 'RDD[T]', withReplacement: bool, num: int, seed: Optional[int] = None) -> List[~T]\n","     |      Return a fixed-size sampled subset of this RDD.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      withReplacement : list\n","     |          whether sampling is done with replacement\n","     |      num : int\n","     |          size of the returned sample\n","     |      seed : int, optional\n","     |          random seed\n","     |      \n","     |      Returns\n","     |      -------\n","     |      list\n","     |          a fixed-size sampled subset of this :class:`RDD` in an array\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.sample`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This method should only be used if the resulting array is expected\n","     |      to be small, as all the data is loaded into the driver's memory.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import sys\n","     |      >>> rdd = sc.parallelize(range(0, 10))\n","     |      >>> len(rdd.takeSample(True, 20, 1))\n","     |      20\n","     |      >>> len(rdd.takeSample(False, 5, 2))\n","     |      5\n","     |      >>> len(rdd.takeSample(False, 15, 3))\n","     |      10\n","     |      >>> sc.range(0, 10).takeSample(False, sys.maxsize)\n","     |      Traceback (most recent call last):\n","     |          ...\n","     |      ValueError: Sample size cannot be greater than ...\n","     |  \n","     |  toDF(self, schema=None, sampleRatio=None) from pyspark.sql.session._monkey_patch_RDD.<locals>\n","     |      Converts current :class:`RDD` into a :class:`DataFrame`\n","     |      \n","     |      This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n","     |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n","     |          column names, default is None.  The data type string format equals to\n","     |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n","     |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n","     |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n","     |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n","     |      sampleRatio : float, optional\n","     |          the sample ratio of rows used for inferring\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`DataFrame`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = spark.range(1).rdd.map(lambda x: tuple(x))\n","     |      >>> rdd.collect()\n","     |      [(0,)]\n","     |      >>> rdd.toDF().show()\n","     |      +---+\n","     |      | _1|\n","     |      +---+\n","     |      |  0|\n","     |      +---+\n","     |  \n","     |  toDebugString(self) -> Optional[bytes]\n","     |      A description of this RDD and its recursive dependencies for debugging.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      bytes\n","     |          debugging information of this :class:`RDD`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.range(5)\n","     |      >>> rdd.toDebugString()\n","     |      b'...PythonRDD...ParallelCollectionRDD...'\n","     |  \n","     |  toLocalIterator(self: 'RDD[T]', prefetchPartitions: bool = False) -> Iterator[~T]\n","     |      Return an iterator that contains all of the elements in this RDD.\n","     |      The iterator will consume as much memory as the largest partition in this RDD.\n","     |      With prefetch it may consume up to the memory of the 2 largest partitions.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      prefetchPartitions : bool, optional\n","     |          If Spark should pre-fetch the next partition\n","     |          before it is needed.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`collections.abc.Iterator`\n","     |          an iterator that contains all of the elements in this :class:`RDD`\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.collect`\n","     |      :meth:`pyspark.sql.DataFrame.toLocalIterator`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize(range(10))\n","     |      >>> [x for x in rdd.toLocalIterator()]\n","     |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","     |  \n","     |  top(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n","     |      Get the top N elements from an RDD.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      num : int\n","     |          top N\n","     |      key : function, optional\n","     |          a function used to generate key for comparing\n","     |      \n","     |      Returns\n","     |      -------\n","     |      list\n","     |          the top N elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.takeOrdered`\n","     |      :meth:`RDD.max`\n","     |      :meth:`RDD.min`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This method should only be used if the resulting array is expected\n","     |      to be small, as all the data is loaded into the driver's memory.\n","     |      \n","     |      It returns the list sorted in descending order.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n","     |      [12]\n","     |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n","     |      [6, 5]\n","     |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n","     |      [4, 3, 2]\n","     |  \n","     |  treeAggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U], depth: int = 2) -> ~U\n","     |      Aggregates the elements of this RDD in a multi-level tree\n","     |      pattern.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      zeroValue : U\n","     |          the initial value for the accumulated result of each partition\n","     |      seqOp : function\n","     |          a function used to accumulate results within a partition\n","     |      combOp : function\n","     |          an associative function used to combine results from different partitions\n","     |      depth : int, optional, default 2\n","     |          suggested depth of the tree\n","     |      \n","     |      Returns\n","     |      -------\n","     |      U\n","     |          the aggregated result\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.aggregate`\n","     |      :meth:`RDD.treeReduce`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> add = lambda x, y: x + y\n","     |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n","     |      >>> rdd.treeAggregate(0, add, add)\n","     |      -5\n","     |      >>> rdd.treeAggregate(0, add, add, 1)\n","     |      -5\n","     |      >>> rdd.treeAggregate(0, add, add, 2)\n","     |      -5\n","     |      >>> rdd.treeAggregate(0, add, add, 5)\n","     |      -5\n","     |      >>> rdd.treeAggregate(0, add, add, 10)\n","     |      -5\n","     |  \n","     |  treeReduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T], depth: int = 2) -> ~T\n","     |      Reduces the elements of this RDD in a multi-level tree pattern.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |          the reduce function\n","     |      depth : int, optional, default 2\n","     |          suggested depth of the tree (default: 2)\n","     |      \n","     |      Returns\n","     |      -------\n","     |      T\n","     |          the aggregated result\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.reduce`\n","     |      :meth:`RDD.aggregate`\n","     |      :meth:`RDD.treeAggregate`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> add = lambda x, y: x + y\n","     |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n","     |      >>> rdd.treeReduce(add)\n","     |      -5\n","     |      >>> rdd.treeReduce(add, 1)\n","     |      -5\n","     |      >>> rdd.treeReduce(add, 2)\n","     |      -5\n","     |      >>> rdd.treeReduce(add, 5)\n","     |      -5\n","     |      >>> rdd.treeReduce(add, 10)\n","     |      -5\n","     |  \n","     |  union(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n","     |      Return the union of this RDD and another one.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      other : :class:`RDD`\n","     |          another :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          the union of this :class:`RDD` and another one\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.union`\n","     |      :meth:`pyspark.sql.DataFrame.union`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n","     |      >>> rdd.union(rdd).collect()\n","     |      [1, 1, 2, 3, 1, 1, 2, 3]\n","     |  \n","     |  unpersist(self: 'RDD[T]', blocking: bool = False) -> 'RDD[T]'\n","     |      Mark the RDD as non-persistent, and remove all blocks for it from\n","     |      memory and disk.\n","     |      \n","     |      .. versionadded:: 0.9.1\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      blocking : bool, optional, default False\n","     |          whether to block until all blocks are deleted\n","     |      \n","     |          .. versionadded:: 3.0.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          The same :class:`RDD`\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.cache`\n","     |      :meth:`RDD.persist`\n","     |      :meth:`RDD.getStorageLevel`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.range(5)\n","     |      >>> rdd.is_cached\n","     |      False\n","     |      >>> _ = rdd.unpersist()\n","     |      >>> rdd.is_cached\n","     |      False\n","     |      >>> _ = rdd.cache()\n","     |      >>> rdd.is_cached\n","     |      True\n","     |      >>> _ = rdd.unpersist()\n","     |      >>> rdd.is_cached\n","     |      False\n","     |      >>> _ = rdd.unpersist()\n","     |  \n","     |  values(self: 'RDD[Tuple[K, V]]') -> 'RDD[V]'\n","     |      Return an RDD with the values of each tuple.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` only containing the values\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.keys`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([(1, 2), (3, 4)]).values()\n","     |      >>> rdd.collect()\n","     |      [2, 4]\n","     |  \n","     |  variance(self: 'RDD[NumberOrArray]') -> float\n","     |      Compute the variance of this RDD's elements.\n","     |      \n","     |      .. versionadded:: 0.9.1\n","     |      \n","     |      Returns\n","     |      -------\n","     |      float\n","     |          the variance of all elements\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.stats`\n","     |      :meth:`RDD.sampleVariance`\n","     |      :meth:`RDD.stdev`\n","     |      :meth:`RDD.sampleStdev`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([1, 2, 3]).variance()\n","     |      0.666...\n","     |  \n","     |  withResources(self: 'RDD[T]', profile: pyspark.resource.profile.ResourceProfile) -> 'RDD[T]'\n","     |      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n","     |      This is only supported on certain cluster managers and currently requires dynamic\n","     |      allocation to be enabled. It will result in new executors with the resources specified\n","     |      being acquired to calculate the RDD.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      profile : :class:`pyspark.resource.ResourceProfile`\n","     |          a resource profile\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          the same :class:`RDD` with user specified profile\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.getResourceProfile`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This API is experimental\n","     |  \n","     |  zip(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n","     |      Zips this RDD with another one, returning key-value pairs with the\n","     |      first element in each RDD second element in each RDD, etc. Assumes\n","     |      that the two RDDs have the same number of partitions and the same\n","     |      number of elements in each partition (e.g. one was made through\n","     |      a map on the other).\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      other : :class:`RDD`\n","     |          another :class:`RDD`\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing the zipped key-value pairs\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.zipWithIndex`\n","     |      :meth:`RDD.zipWithUniqueId`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd1 = sc.parallelize(range(0,5))\n","     |      >>> rdd2 = sc.parallelize(range(1000, 1005))\n","     |      >>> rdd1.zip(rdd2).collect()\n","     |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n","     |  \n","     |  zipWithIndex(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n","     |      Zips this RDD with its element indices.\n","     |      \n","     |      The ordering is first based on the partition index and then the\n","     |      ordering of items within each partition. So the first item in\n","     |      the first partition gets index 0, and the last item in the last\n","     |      partition receives the largest index.\n","     |      \n","     |      This method needs to trigger a spark job when this RDD contains\n","     |      more than one partitions.\n","     |      \n","     |      .. versionadded:: 1.2.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing the zipped key-index pairs\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.zip`\n","     |      :meth:`RDD.zipWithUniqueId`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n","     |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n","     |  \n","     |  zipWithUniqueId(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n","     |      Zips this RDD with generated unique Long ids.\n","     |      \n","     |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n","     |      n is the number of partitions. So there may exist gaps, but this\n","     |      method won't trigger a spark job, which is different from\n","     |      :meth:`zipWithIndex`.\n","     |      \n","     |      .. versionadded:: 1.2.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a :class:`RDD` containing the zipped key-UniqueId pairs\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.zip`\n","     |      :meth:`RDD.zipWithIndex`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n","     |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties defined here:\n","     |  \n","     |  context\n","     |      The :class:`SparkContext` that this RDD was created on.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`SparkContext`\n","     |          The :class:`SparkContext` that this RDD was created on\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.range(5)\n","     |      >>> rdd.context\n","     |      <SparkContext ...>\n","     |      >>> rdd.context is sc\n","     |      True\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __annotations__ = {}\n","     |  \n","     |  __orig_bases__ = (typing.Generic[+T_co],)\n","     |  \n","     |  __parameters__ = (+T_co,)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params)\n","     |      Parameterizes a generic class.\n","     |      \n","     |      At least, parameterizing a generic class is the *main* thing this method\n","     |      does. For example, for some generic class `Foo`, this is called when we\n","     |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n","     |      \n","     |      However, note that this method is also called when defining generic\n","     |      classes in the first place with `class Foo(Generic[T]): ...`.\n","     |  \n","     |  __init_subclass__(*args, **kwargs)\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","    \n","    class RDDBarrier(typing.Generic)\n","     |  RDDBarrier(rdd: pyspark.rdd.RDD[~T])\n","     |  \n","     |  Wraps an RDD in a barrier stage, which forces Spark to launch tasks of this stage together.\n","     |  :class:`RDDBarrier` instances are created by :meth:`RDD.barrier`.\n","     |  \n","     |  .. versionadded:: 2.4.0\n","     |  \n","     |  Notes\n","     |  -----\n","     |  This API is experimental\n","     |  \n","     |  Method resolution order:\n","     |      RDDBarrier\n","     |      typing.Generic\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, rdd: pyspark.rdd.RDD[~T])\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  mapPartitions(self, f: Callable[[Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> pyspark.rdd.RDD[~U]\n","     |      Returns a new RDD by applying a function to each partition of the wrapped RDD,\n","     |      where tasks are launched together in a barrier stage.\n","     |      The interface is the same as :meth:`RDD.mapPartitions`.\n","     |      Please see the API doc there.\n","     |      \n","     |      .. versionadded:: 2.4.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |         a function to run on each partition of the RDD\n","     |      preservesPartitioning : bool, optional, default False\n","     |          indicates whether the input function preserves the partitioner,\n","     |          which should be False unless this is a pair RDD and the input\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD` by applying a function to each partition\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.mapPartitions`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This API is experimental\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n","     |      >>> def f(iterator): yield sum(iterator)\n","     |      ...\n","     |      >>> barrier = rdd.barrier()\n","     |      >>> barrier\n","     |      <pyspark.rdd.RDDBarrier ...>\n","     |      >>> barrier.mapPartitions(f).collect()\n","     |      [3, 7]\n","     |  \n","     |  mapPartitionsWithIndex(self, f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> pyspark.rdd.RDD[~U]\n","     |      Returns a new RDD by applying a function to each partition of the wrapped RDD, while\n","     |      tracking the index of the original partition. And all tasks are launched together\n","     |      in a barrier stage.\n","     |      The interface is the same as :meth:`RDD.mapPartitionsWithIndex`.\n","     |      Please see the API doc there.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      f : function\n","     |         a function to run on each partition of the RDD\n","     |      preservesPartitioning : bool, optional, default False\n","     |          indicates whether the input function preserves the partitioner,\n","     |          which should be False unless this is a pair RDD and the input\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          a new :class:`RDD` by applying a function to each partition\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.mapPartitionsWithIndex`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      This API is experimental\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n","     |      >>> def f(splitIndex, iterator): yield splitIndex\n","     |      ...\n","     |      >>> barrier = rdd.barrier()\n","     |      >>> barrier\n","     |      <pyspark.rdd.RDDBarrier ...>\n","     |      >>> barrier.mapPartitionsWithIndex(f).sum()\n","     |      6\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __annotations__ = {}\n","     |  \n","     |  __orig_bases__ = (typing.Generic[~T],)\n","     |  \n","     |  __parameters__ = (~T,)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from typing.Generic:\n","     |  \n","     |  __class_getitem__(params)\n","     |      Parameterizes a generic class.\n","     |      \n","     |      At least, parameterizing a generic class is the *main* thing this method\n","     |      does. For example, for some generic class `Foo`, this is called when we\n","     |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n","     |      \n","     |      However, note that this method is also called when defining generic\n","     |      classes in the first place with `class Foo(Generic[T]): ...`.\n","     |  \n","     |  __init_subclass__(*args, **kwargs)\n","     |      This method is called when a class is subclassed.\n","     |      \n","     |      The default implementation does nothing. It may be\n","     |      overridden to extend subclasses.\n","    \n","    class SparkConf(builtins.object)\n","     |  SparkConf(loadDefaults: bool = True, _jvm: Optional[py4j.java_gateway.JVMView] = None, _jconf: Optional[py4j.java_gateway.JavaObject] = None)\n","     |  \n","     |  Configuration for a Spark application. Used to set various Spark\n","     |  parameters as key-value pairs.\n","     |  \n","     |  Most of the time, you would create a SparkConf object with\n","     |  ``SparkConf()``, which will load values from `spark.*` Java system\n","     |  properties as well. In this case, any parameters you set directly on\n","     |  the :class:`SparkConf` object take priority over system properties.\n","     |  \n","     |  For unit tests, you can also call ``SparkConf(false)`` to skip\n","     |  loading external settings and get the same configuration no matter\n","     |  what the system properties are.\n","     |  \n","     |  All setter methods in this class support chaining. For example,\n","     |  you can write ``conf.setMaster(\"local\").setAppName(\"My app\")``.\n","     |  \n","     |  Parameters\n","     |  ----------\n","     |  loadDefaults : bool\n","     |      whether to load values from Java system properties (True by default)\n","     |  _jvm : class:`py4j.java_gateway.JVMView`\n","     |      internal parameter used to pass a handle to the\n","     |      Java VM; does not need to be set by users\n","     |  _jconf : class:`py4j.java_gateway.JavaObject`\n","     |      Optionally pass in an existing SparkConf handle\n","     |      to use its parameters\n","     |  \n","     |  Notes\n","     |  -----\n","     |  Once a SparkConf object is passed to Spark, it is cloned\n","     |  and can no longer be modified by the user.\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> from pyspark.conf import SparkConf\n","     |  >>> from pyspark.context import SparkContext\n","     |  >>> conf = SparkConf()\n","     |  >>> conf.setMaster(\"local\").setAppName(\"My app\")\n","     |  <pyspark.conf.SparkConf object at ...>\n","     |  >>> conf.get(\"spark.master\")\n","     |  'local'\n","     |  >>> conf.get(\"spark.app.name\")\n","     |  'My app'\n","     |  >>> sc = SparkContext(conf=conf)\n","     |  >>> sc.master\n","     |  'local'\n","     |  >>> sc.appName\n","     |  'My app'\n","     |  >>> sc.sparkHome is None\n","     |  True\n","     |  \n","     |  >>> conf = SparkConf(loadDefaults=False)\n","     |  >>> conf.setSparkHome(\"/path\")\n","     |  <pyspark.conf.SparkConf object at ...>\n","     |  >>> conf.get(\"spark.home\")\n","     |  '/path'\n","     |  >>> conf.setExecutorEnv(\"VAR1\", \"value1\")\n","     |  <pyspark.conf.SparkConf object at ...>\n","     |  >>> conf.setExecutorEnv(pairs = [(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n","     |  <pyspark.conf.SparkConf object at ...>\n","     |  >>> conf.get(\"spark.executorEnv.VAR1\")\n","     |  'value1'\n","     |  >>> print(conf.toDebugString())\n","     |  spark.executorEnv.VAR1=value1\n","     |  spark.executorEnv.VAR3=value3\n","     |  spark.executorEnv.VAR4=value4\n","     |  spark.home=/path\n","     |  >>> for p in sorted(conf.getAll(), key=lambda p: p[0]):\n","     |  ...     print(p)\n","     |  ('spark.executorEnv.VAR1', 'value1')\n","     |  ('spark.executorEnv.VAR3', 'value3')\n","     |  ('spark.executorEnv.VAR4', 'value4')\n","     |  ('spark.home', '/path')\n","     |  >>> conf._jconf.setExecutorEnv(\"VAR5\", \"value5\")\n","     |  JavaObject id...\n","     |  >>> print(conf.toDebugString())\n","     |  spark.executorEnv.VAR1=value1\n","     |  spark.executorEnv.VAR3=value3\n","     |  spark.executorEnv.VAR4=value4\n","     |  spark.executorEnv.VAR5=value5\n","     |  spark.home=/path\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, loadDefaults: bool = True, _jvm: Optional[py4j.java_gateway.JVMView] = None, _jconf: Optional[py4j.java_gateway.JavaObject] = None)\n","     |      Create a new Spark configuration.\n","     |  \n","     |  contains(self, key: str) -> bool\n","     |      Does this configuration contain a given key?\n","     |  \n","     |  get(self, key: str, defaultValue: Optional[str] = None) -> Optional[str]\n","     |      Get the configured value for some key, or return a default otherwise.\n","     |  \n","     |  getAll(self) -> List[Tuple[str, str]]\n","     |      Get all values as a list of key-value pairs.\n","     |  \n","     |  set(self, key: str, value: str) -> 'SparkConf'\n","     |      Set a configuration property.\n","     |  \n","     |  setAll(self, pairs: List[Tuple[str, str]]) -> 'SparkConf'\n","     |      Set multiple parameters, passed as a list of key-value pairs.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      pairs : iterable of tuples\n","     |          list of key-value pairs to set\n","     |  \n","     |  setAppName(self, value: str) -> 'SparkConf'\n","     |      Set application name.\n","     |  \n","     |  setExecutorEnv(self, key: Optional[str] = None, value: Optional[str] = None, pairs: Optional[List[Tuple[str, str]]] = None) -> 'SparkConf'\n","     |      Set an environment variable to be passed to executors.\n","     |  \n","     |  setIfMissing(self, key: str, value: str) -> 'SparkConf'\n","     |      Set a configuration property, if not already set.\n","     |  \n","     |  setMaster(self, value: str) -> 'SparkConf'\n","     |      Set master URL to connect to.\n","     |  \n","     |  setSparkHome(self, value: str) -> 'SparkConf'\n","     |      Set path where Spark is installed on worker nodes.\n","     |  \n","     |  toDebugString(self) -> str\n","     |      Returns a printable version of the configuration, as a list of\n","     |      key=value pairs, one per line.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __annotations__ = {'_conf': typing.Optional[typing.Dict[str, str]], '_...\n","    \n","    class SparkContext(builtins.object)\n","     |  SparkContext(master: Optional[str] = None, appName: Optional[str] = None, sparkHome: Optional[str] = None, pyFiles: Optional[List[str]] = None, environment: Optional[Dict[str, Any]] = None, batchSize: int = 0, serializer: 'Serializer' = CloudPickleSerializer(), conf: Optional[pyspark.conf.SparkConf] = None, gateway: Optional[py4j.java_gateway.JavaGateway] = None, jsc: Optional[py4j.java_gateway.JavaObject] = None, profiler_cls: Type[pyspark.profiler.BasicProfiler] = <class 'pyspark.profiler.BasicProfiler'>, udf_profiler_cls: Type[pyspark.profiler.UDFBasicProfiler] = <class 'pyspark.profiler.UDFBasicProfiler'>, memory_profiler_cls: Type[pyspark.profiler.MemoryProfiler] = <class 'pyspark.profiler.MemoryProfiler'>)\n","     |  \n","     |  Main entry point for Spark functionality. A SparkContext represents the\n","     |  connection to a Spark cluster, and can be used to create :class:`RDD` and\n","     |  broadcast variables on that cluster.\n","     |  \n","     |  When you create a new SparkContext, at least the master and app name should\n","     |  be set, either through the named parameters here or through `conf`.\n","     |  \n","     |  Parameters\n","     |  ----------\n","     |  master : str, optional\n","     |      Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n","     |  appName : str, optional\n","     |      A name for your job, to display on the cluster web UI.\n","     |  sparkHome : str, optional\n","     |      Location where Spark is installed on cluster nodes.\n","     |  pyFiles : list, optional\n","     |      Collection of .zip or .py files to send to the cluster\n","     |      and add to PYTHONPATH.  These can be paths on the local file\n","     |      system or HDFS, HTTP, HTTPS, or FTP URLs.\n","     |  environment : dict, optional\n","     |      A dictionary of environment variables to set on\n","     |      worker nodes.\n","     |  batchSize : int, optional, default 0\n","     |      The number of Python objects represented as a single\n","     |      Java object. Set 1 to disable batching, 0 to automatically choose\n","     |      the batch size based on object sizes, or -1 to use an unlimited\n","     |      batch size\n","     |  serializer : :class:`Serializer`, optional, default :class:`CPickleSerializer`\n","     |      The serializer for RDDs.\n","     |  conf : :class:`SparkConf`, optional\n","     |      An object setting Spark properties.\n","     |  gateway : class:`py4j.java_gateway.JavaGateway`,  optional\n","     |      Use an existing gateway and JVM, otherwise a new JVM\n","     |      will be instantiated. This is only used internally.\n","     |  jsc : class:`py4j.java_gateway.JavaObject`, optional\n","     |      The JavaSparkContext instance. This is only used internally.\n","     |  profiler_cls : type, optional, default :class:`BasicProfiler`\n","     |      A class of custom Profiler used to do profiling\n","     |  udf_profiler_cls : type, optional, default :class:`UDFBasicProfiler`\n","     |      A class of custom Profiler used to do udf profiling\n","     |  \n","     |  Notes\n","     |  -----\n","     |  Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n","     |  the active :class:`SparkContext` before creating a new one.\n","     |  \n","     |  :class:`SparkContext` instance is not supported to share across multiple\n","     |  processes out of the box, and PySpark does not guarantee multi-processing execution.\n","     |  Use threads instead for concurrent processing purpose.\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> from pyspark.context import SparkContext\n","     |  >>> sc = SparkContext('local', 'test')\n","     |  >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n","     |  Traceback (most recent call last):\n","     |      ...\n","     |  ValueError: ...\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __enter__(self) -> 'SparkContext'\n","     |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.\n","     |  \n","     |  __exit__(self, type: Optional[Type[BaseException]], value: Optional[BaseException], trace: Optional[traceback]) -> None\n","     |      Enable 'with SparkContext(...) as sc: app' syntax.\n","     |      \n","     |      Specifically stop the context on exit of the with block.\n","     |  \n","     |  __getnewargs__(self) -> NoReturn\n","     |  \n","     |  __init__(self, master: Optional[str] = None, appName: Optional[str] = None, sparkHome: Optional[str] = None, pyFiles: Optional[List[str]] = None, environment: Optional[Dict[str, Any]] = None, batchSize: int = 0, serializer: 'Serializer' = CloudPickleSerializer(), conf: Optional[pyspark.conf.SparkConf] = None, gateway: Optional[py4j.java_gateway.JavaGateway] = None, jsc: Optional[py4j.java_gateway.JavaObject] = None, profiler_cls: Type[pyspark.profiler.BasicProfiler] = <class 'pyspark.profiler.BasicProfiler'>, udf_profiler_cls: Type[pyspark.profiler.UDFBasicProfiler] = <class 'pyspark.profiler.UDFBasicProfiler'>, memory_profiler_cls: Type[pyspark.profiler.MemoryProfiler] = <class 'pyspark.profiler.MemoryProfiler'>)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  accumulator(self, value: ~T, accum_param: Optional[ForwardRef('AccumulatorParam[T]')] = None) -> 'Accumulator[T]'\n","     |      Create an :class:`Accumulator` with the given initial value, using a given\n","     |      :class:`AccumulatorParam` helper object to define how to add values of the\n","     |      data type if provided. Default AccumulatorParams are used for integers\n","     |      and floating-point numbers if you do not provide one. For other types,\n","     |      a custom AccumulatorParam can be used.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      value : T\n","     |          initialized value\n","     |      accum_param : :class:`pyspark.AccumulatorParam`, optional\n","     |          helper object to define how to add values\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`Accumulator`\n","     |          `Accumulator` object, a shared variable that can be accumulated\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> acc = sc.accumulator(9)\n","     |      >>> acc.value\n","     |      9\n","     |      >>> acc += 1\n","     |      >>> acc.value\n","     |      10\n","     |      \n","     |      Accumulator object can be accumulated in RDD operations:\n","     |      \n","     |      >>> rdd = sc.range(5)\n","     |      >>> def f(x):\n","     |      ...     global acc\n","     |      ...     acc += 1\n","     |      ...\n","     |      >>> rdd.foreach(f)\n","     |      >>> acc.value\n","     |      15\n","     |  \n","     |  addArchive(self, path: str) -> None\n","     |      Add an archive to be downloaded with this Spark job on every node.\n","     |      The `path` passed can be either a local file, a file in HDFS\n","     |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n","     |      FTP URI.\n","     |      \n","     |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n","     |      filename to find its download/unpacked location. The given path should\n","     |      be one of .zip, .tar, .tar.gz, .tgz and .jar.\n","     |      \n","     |      .. versionadded:: 3.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          can be either a local file, a file in HDFS (or other Hadoop-supported\n","     |          filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n","     |          use :meth:`SparkFiles.get` to find its download location.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.listArchives`\n","     |      :meth:`SparkFiles.get`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      A path can be added only once. Subsequent additions of the same path are ignored.\n","     |      This API is experimental.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      Creates a zipped file that contains a text file written '100'.\n","     |      \n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      >>> import zipfile\n","     |      >>> from pyspark import SparkFiles\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"test.txt\")\n","     |      ...     with open(path, \"w\") as f:\n","     |      ...         _ = f.write(\"100\")\n","     |      ...\n","     |      ...     zip_path1 = os.path.join(d, \"test1.zip\")\n","     |      ...     with zipfile.ZipFile(zip_path1, \"w\", zipfile.ZIP_DEFLATED) as z:\n","     |      ...         z.write(path, os.path.basename(path))\n","     |      ...\n","     |      ...     zip_path2 = os.path.join(d, \"test2.zip\")\n","     |      ...     with zipfile.ZipFile(zip_path2, \"w\", zipfile.ZIP_DEFLATED) as z:\n","     |      ...         z.write(path, os.path.basename(path))\n","     |      ...\n","     |      ...     sc.addArchive(zip_path1)\n","     |      ...     arch_list1 = sorted(sc.listArchives)\n","     |      ...\n","     |      ...     sc.addArchive(zip_path2)\n","     |      ...     arch_list2 = sorted(sc.listArchives)\n","     |      ...\n","     |      ...     # add zip_path2 twice, this addition will be ignored\n","     |      ...     sc.addArchive(zip_path2)\n","     |      ...     arch_list3 = sorted(sc.listArchives)\n","     |      ...\n","     |      ...     def func(iterator):\n","     |      ...         with open(\"%s/test.txt\" % SparkFiles.get(\"test1.zip\")) as f:\n","     |      ...             mul = int(f.readline())\n","     |      ...             return [x * mul for x in iterator]\n","     |      ...\n","     |      ...     collected = sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n","     |      \n","     |      >>> arch_list1\n","     |      ['file:/.../test1.zip']\n","     |      >>> arch_list2\n","     |      ['file:/.../test1.zip', 'file:/.../test2.zip']\n","     |      >>> arch_list3\n","     |      ['file:/.../test1.zip', 'file:/.../test2.zip']\n","     |      >>> collected\n","     |      [100, 200, 300, 400]\n","     |  \n","     |  addFile(self, path: str, recursive: bool = False) -> None\n","     |      Add a file to be downloaded with this Spark job on every node.\n","     |      The `path` passed can be either a local file, a file in HDFS\n","     |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n","     |      FTP URI.\n","     |      \n","     |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n","     |      filename to find its download location.\n","     |      \n","     |      A directory can be given if the recursive option is set to True.\n","     |      Currently directories are only supported for Hadoop-supported filesystems.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          can be either a local file, a file in HDFS (or other Hadoop-supported\n","     |          filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n","     |          use :meth:`SparkFiles.get` to find its download location.\n","     |      recursive : bool, default False\n","     |          whether to recursively add files in the input directory\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.listFiles`\n","     |      :meth:`SparkContext.addPyFile`\n","     |      :meth:`SparkFiles.get`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      A path can be added only once. Subsequent additions of the same path are ignored.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      >>> from pyspark import SparkFiles\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path1 = os.path.join(d, \"test1.txt\")\n","     |      ...     with open(path1, \"w\") as f:\n","     |      ...         _ = f.write(\"100\")\n","     |      ...\n","     |      ...     path2 = os.path.join(d, \"test2.txt\")\n","     |      ...     with open(path2, \"w\") as f:\n","     |      ...         _ = f.write(\"200\")\n","     |      ...\n","     |      ...     sc.addFile(path1)\n","     |      ...     file_list1 = sorted(sc.listFiles)\n","     |      ...\n","     |      ...     sc.addFile(path2)\n","     |      ...     file_list2 = sorted(sc.listFiles)\n","     |      ...\n","     |      ...     # add path2 twice, this addition will be ignored\n","     |      ...     sc.addFile(path2)\n","     |      ...     file_list3 = sorted(sc.listFiles)\n","     |      ...\n","     |      ...     def func(iterator):\n","     |      ...         with open(SparkFiles.get(\"test1.txt\")) as f:\n","     |      ...             mul = int(f.readline())\n","     |      ...             return [x * mul for x in iterator]\n","     |      ...\n","     |      ...     collected = sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n","     |      \n","     |      >>> file_list1\n","     |      ['file:/.../test1.txt']\n","     |      >>> file_list2\n","     |      ['file:/.../test1.txt', 'file:/.../test2.txt']\n","     |      >>> file_list3\n","     |      ['file:/.../test1.txt', 'file:/.../test2.txt']\n","     |      >>> collected\n","     |      [100, 200, 300, 400]\n","     |  \n","     |  addJobTag(self, tag: str) -> None\n","     |      Add a tag to be assigned to all the jobs started by this thread.\n","     |      \n","     |      .. versionadded:: 3.5.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      tag : str\n","     |          The tag to be added. Cannot contain ',' (comma) character.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.removeJobTag`\n","     |      :meth:`SparkContext.getJobTags`\n","     |      :meth:`SparkContext.clearJobTags`\n","     |      :meth:`SparkContext.cancelJobsWithTag`\n","     |      :meth:`SparkContext.setInterruptOnCancel`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import threading\n","     |      >>> from time import sleep\n","     |      >>> from pyspark import InheritableThread\n","     |      >>> sc.setInterruptOnCancel(interruptOnCancel=True)\n","     |      >>> result = \"Not Set\"\n","     |      >>> lock = threading.Lock()\n","     |      >>> def map_func(x):\n","     |      ...     sleep(100)\n","     |      ...     raise RuntimeError(\"Task should have been cancelled\")\n","     |      ...\n","     |      >>> def start_job(x):\n","     |      ...     global result\n","     |      ...     try:\n","     |      ...         sc.addJobTag(\"job_to_cancel\")\n","     |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n","     |      ...     except Exception as e:\n","     |      ...         result = \"Cancelled\"\n","     |      ...     lock.release()\n","     |      ...\n","     |      >>> def stop_job():\n","     |      ...     sleep(5)\n","     |      ...     sc.cancelJobsWithTag(\"job_to_cancel\")\n","     |      ...\n","     |      >>> suppress = lock.acquire()\n","     |      >>> suppress = InheritableThread(target=start_job, args=(10,)).start()\n","     |      >>> suppress = InheritableThread(target=stop_job).start()\n","     |      >>> suppress = lock.acquire()\n","     |      >>> print(result)\n","     |      Cancelled\n","     |      >>> sc.clearJobTags()\n","     |  \n","     |  addPyFile(self, path: str) -> None\n","     |      Add a .py or .zip dependency for all tasks to be executed on this\n","     |      SparkContext in the future.  The `path` passed can be either a local\n","     |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n","     |      HTTP, HTTPS or FTP URI.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          can be either a .py file or .zip dependency.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.addFile`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      A path can be added only once. Subsequent additions of the same path are ignored.\n","     |  \n","     |  binaryFiles(self, path: str, minPartitions: Optional[int] = None) -> pyspark.rdd.RDD[typing.Tuple[str, bytes]]\n","     |      Read a directory of binary files from HDFS, a local file system\n","     |      (available on all nodes), or any Hadoop-supported file system URI\n","     |      as a byte array. Each file is read as a single record and returned\n","     |      in a key-value pair, where the key is the path of each file, the\n","     |      value is the content of each file.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          directory to the input data files, the path can be comma separated\n","     |          paths as a list of inputs\n","     |      minPartitions : int, optional\n","     |          suggested minimum number of partitions for the resulting RDD\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          RDD representing path-content pairs from the file(s).\n","     |      \n","     |      Notes\n","     |      -----\n","     |      Small files are preferred, large file is also allowable, but may cause bad performance.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.binaryRecords`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     # Write a temporary binary file\n","     |      ...     with open(os.path.join(d, \"1.bin\"), \"wb\") as f1:\n","     |      ...         _ = f1.write(b\"binary data I\")\n","     |      ...\n","     |      ...     # Write another temporary binary file\n","     |      ...     with open(os.path.join(d, \"2.bin\"), \"wb\") as f2:\n","     |      ...         _ = f2.write(b\"binary data II\")\n","     |      ...\n","     |      ...     collected = sorted(sc.binaryFiles(d).collect())\n","     |      \n","     |      >>> collected\n","     |      [('.../1.bin', b'binary data I'), ('.../2.bin', b'binary data II')]\n","     |  \n","     |  binaryRecords(self, path: str, recordLength: int) -> pyspark.rdd.RDD[bytes]\n","     |      Load data from a flat binary file, assuming each record is a set of numbers\n","     |      with the specified numerical format (see ByteBuffer), and the number of\n","     |      bytes per record is constant.\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          Directory to the input data files\n","     |      recordLength : int\n","     |          The length at which to split the records\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          RDD of data with values, represented as byte arrays\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.binaryFiles`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     # Write a temporary file\n","     |      ...     with open(os.path.join(d, \"1.bin\"), \"w\") as f:\n","     |      ...         for i in range(3):\n","     |      ...             _ = f.write(\"%04d\" % i)\n","     |      ...\n","     |      ...     # Write another file\n","     |      ...     with open(os.path.join(d, \"2.bin\"), \"w\") as f:\n","     |      ...         for i in [-1, -2, -10]:\n","     |      ...             _ = f.write(\"%04d\" % i)\n","     |      ...\n","     |      ...     collected = sorted(sc.binaryRecords(d, 4).collect())\n","     |      \n","     |      >>> collected\n","     |      [b'-001', b'-002', b'-010', b'0000', b'0001', b'0002']\n","     |  \n","     |  broadcast(self, value: ~T) -> 'Broadcast[T]'\n","     |      Broadcast a read-only variable to the cluster, returning a :class:`Broadcast`\n","     |      object for reading it in distributed functions. The variable will\n","     |      be sent to each cluster only once.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      value : T\n","     |          value to broadcast to the Spark nodes\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`Broadcast`\n","     |          :class:`Broadcast` object, a read-only variable cached on each machine\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> mapping = {1: 10001, 2: 10002}\n","     |      >>> bc = sc.broadcast(mapping)\n","     |      \n","     |      >>> rdd = sc.range(5)\n","     |      >>> rdd2 = rdd.map(lambda i: bc.value[i] if i in bc.value else -1)\n","     |      >>> rdd2.collect()\n","     |      [-1, 10001, 10002, -1, -1]\n","     |      \n","     |      >>> bc.destroy()\n","     |  \n","     |  cancelAllJobs(self) -> None\n","     |      Cancel all jobs that have been scheduled or are running.\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.cancelJobGroup`\n","     |      :meth:`SparkContext.cancelJobsWithTag`\n","     |      :meth:`SparkContext.runJob`\n","     |  \n","     |  cancelJobGroup(self, groupId: str) -> None\n","     |      Cancel active jobs for the specified group. See :meth:`SparkContext.setJobGroup`.\n","     |      for more information.\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      groupId : str\n","     |          The group ID to cancel the job.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.setJobGroup`\n","     |  \n","     |  cancelJobsWithTag(self, tag: str) -> None\n","     |      Cancel active jobs that have the specified tag. See\n","     |      :meth:`SparkContext.addJobTag`.\n","     |      \n","     |      .. versionadded:: 3.5.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      tag : str\n","     |          The tag to be cancelled. Cannot contain ',' (comma) character.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.addJobTag`\n","     |      :meth:`SparkContext.removeJobTag`\n","     |      :meth:`SparkContext.getJobTags`\n","     |      :meth:`SparkContext.clearJobTags`\n","     |      :meth:`SparkContext.setInterruptOnCancel`\n","     |  \n","     |  clearJobTags(self) -> None\n","     |      Clear the current thread's job tags.\n","     |      \n","     |      .. versionadded:: 3.5.0\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.addJobTag`\n","     |      :meth:`SparkContext.removeJobTag`\n","     |      :meth:`SparkContext.getJobTags`\n","     |      :meth:`SparkContext.cancelJobsWithTag`\n","     |      :meth:`SparkContext.setInterruptOnCancel`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.addJobTag(\"job_to_cancel\")\n","     |      >>> sc.clearJobTags()\n","     |      >>> sc.getJobTags()\n","     |      set()\n","     |  \n","     |  dump_profiles(self, path: str) -> None\n","     |      Dump the profile stats into directory `path`\n","     |      \n","     |      .. versionadded:: 1.2.0\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.show_profiles`\n","     |  \n","     |  emptyRDD(self) -> pyspark.rdd.RDD[typing.Any]\n","     |      Create an :class:`RDD` that has no partitions or elements.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          An empty RDD\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.emptyRDD()\n","     |      EmptyRDD...\n","     |      >>> sc.emptyRDD().count()\n","     |      0\n","     |  \n","     |  getCheckpointDir(self) -> Optional[str]\n","     |      Return the directory where RDDs are checkpointed. Returns None if no\n","     |      checkpoint directory has been set.\n","     |      \n","     |      .. versionadded:: 3.1.0\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.setCheckpointDir`\n","     |      :meth:`RDD.checkpoint`\n","     |      :meth:`RDD.getCheckpointFile`\n","     |  \n","     |  getConf(self) -> pyspark.conf.SparkConf\n","     |      Return a copy of this SparkContext's configuration :class:`SparkConf`.\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |  \n","     |  getJobTags(self) -> Set[str]\n","     |      Get the tags that are currently set to be assigned to all the jobs started by this thread.\n","     |      \n","     |      .. versionadded:: 3.5.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      set of str\n","     |          the tags that are currently set to be assigned to all the jobs started by this thread.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.addJobTag`\n","     |      :meth:`SparkContext.removeJobTag`\n","     |      :meth:`SparkContext.clearJobTags`\n","     |      :meth:`SparkContext.cancelJobsWithTag`\n","     |      :meth:`SparkContext.setInterruptOnCancel`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.addJobTag(\"job_to_cancel\")\n","     |      >>> sc.getJobTags()\n","     |      {'job_to_cancel'}\n","     |      >>> sc.clearJobTags()\n","     |  \n","     |  getLocalProperty(self, key: str) -> Optional[str]\n","     |      Get a local property set in this thread, or null if it is missing. See\n","     |      :meth:`setLocalProperty`.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.setLocalProperty`\n","     |  \n","     |  hadoopFile(self, path: str, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n","     |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n","     |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n","     |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n","     |      Configuration in Java.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          path to Hadoop file\n","     |      inputFormatClass : str\n","     |          fully qualified classname of Hadoop InputFormat\n","     |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n","     |      keyClass : str\n","     |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n","     |      valueClass : str\n","     |          fully qualified classname of value Writable class\n","     |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n","     |      keyConverter : str, optional\n","     |          fully qualified name of a function returning key WritableConverter\n","     |      valueConverter : str, optional\n","     |          fully qualified name of a function returning value WritableConverter\n","     |      conf : dict, optional\n","     |          Hadoop configuration, passed in as a dict\n","     |      batchSize : int, optional, default 0\n","     |          The number of Python objects represented as a single\n","     |          Java object. (default 0, choose batchSize automatically)\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          RDD of tuples of key and corresponding value\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.saveAsSequenceFile`\n","     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n","     |      :meth:`RDD.saveAsHadoopFile`\n","     |      :meth:`SparkContext.newAPIHadoopFile`\n","     |      :meth:`SparkContext.hadoopRDD`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      \n","     |      Set the related classes\n","     |      \n","     |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n","     |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n","     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n","     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"old_hadoop_file\")\n","     |      ...\n","     |      ...     # Write a temporary Hadoop file\n","     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n","     |      ...     rdd.saveAsHadoopFile(path, output_format_class, key_class, value_class)\n","     |      ...\n","     |      ...     loaded = sc.hadoopFile(path, input_format_class, key_class, value_class)\n","     |      ...     collected = sorted(loaded.collect())\n","     |      \n","     |      >>> collected\n","     |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n","     |  \n","     |  hadoopRDD(self, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n","     |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n","     |      Hadoop configuration, which is passed in as a Python dict.\n","     |      This will be converted into a Configuration in Java.\n","     |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      inputFormatClass : str\n","     |          fully qualified classname of Hadoop InputFormat\n","     |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n","     |      keyClass : str\n","     |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n","     |      valueClass : str\n","     |          fully qualified classname of value Writable class\n","     |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n","     |      keyConverter : str, optional\n","     |          fully qualified name of a function returning key WritableConverter\n","     |      valueConverter : str, optional\n","     |          fully qualified name of a function returning value WritableConverter\n","     |      conf : dict, optional\n","     |          Hadoop configuration, passed in as a dict\n","     |      batchSize : int, optional, default 0\n","     |          The number of Python objects represented as a single\n","     |          Java object. (default 0, choose batchSize automatically)\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          RDD of tuples of key and corresponding value\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n","     |      :meth:`RDD.saveAsHadoopDataset`\n","     |      :meth:`SparkContext.newAPIHadoopRDD`\n","     |      :meth:`SparkContext.hadoopFile`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      \n","     |      Set the related classes\n","     |      \n","     |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n","     |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n","     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n","     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"old_hadoop_file\")\n","     |      ...\n","     |      ...     # Create the conf for writing\n","     |      ...     write_conf = {\n","     |      ...         \"mapred.output.format.class\": output_format_class,\n","     |      ...         \"mapreduce.job.output.key.class\": key_class,\n","     |      ...         \"mapreduce.job.output.value.class\": value_class,\n","     |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n","     |      ...     }\n","     |      ...\n","     |      ...     # Write a temporary Hadoop file\n","     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n","     |      ...     rdd.saveAsHadoopDataset(conf=write_conf)\n","     |      ...\n","     |      ...     # Create the conf for reading\n","     |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n","     |      ...\n","     |      ...     loaded = sc.hadoopRDD(input_format_class, key_class, value_class, conf=read_conf)\n","     |      ...     collected = sorted(loaded.collect())\n","     |      \n","     |      >>> collected\n","     |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n","     |  \n","     |  newAPIHadoopFile(self, path: str, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n","     |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\n","     |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n","     |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n","     |      \n","     |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n","     |      Configuration in Java\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          path to Hadoop file\n","     |      inputFormatClass : str\n","     |          fully qualified classname of Hadoop InputFormat\n","     |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n","     |      keyClass : str\n","     |          fully qualified classname of key Writable class\n","     |          (e.g. \"org.apache.hadoop.io.Text\")\n","     |      valueClass : str\n","     |          fully qualified classname of value Writable class\n","     |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n","     |      keyConverter : str, optional\n","     |          fully qualified name of a function returning key WritableConverter\n","     |          None by default\n","     |      valueConverter : str, optional\n","     |          fully qualified name of a function returning value WritableConverter\n","     |          None by default\n","     |      conf : dict, optional\n","     |          Hadoop configuration, passed in as a dict\n","     |          None by default\n","     |      batchSize : int, optional, default 0\n","     |          The number of Python objects represented as a single\n","     |          Java object. (default 0, choose batchSize automatically)\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          RDD of tuples of key and corresponding value\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.saveAsSequenceFile`\n","     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n","     |      :meth:`RDD.saveAsHadoopFile`\n","     |      :meth:`SparkContext.sequenceFile`\n","     |      :meth:`SparkContext.hadoopFile`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      \n","     |      Set the related classes\n","     |      \n","     |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n","     |      >>> input_format_class = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n","     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n","     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"new_hadoop_file\")\n","     |      ...\n","     |      ...     # Write a temporary Hadoop file\n","     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n","     |      ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class, key_class, value_class)\n","     |      ...\n","     |      ...     loaded = sc.newAPIHadoopFile(path, input_format_class, key_class, value_class)\n","     |      ...     collected = sorted(loaded.collect())\n","     |      \n","     |      >>> collected\n","     |      [(1, ''), (1, 'a'), (3, 'x')]\n","     |  \n","     |  newAPIHadoopRDD(self, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n","     |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n","     |      Hadoop configuration, which is passed in as a Python dict.\n","     |      This will be converted into a Configuration in Java.\n","     |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      inputFormatClass : str\n","     |          fully qualified classname of Hadoop InputFormat\n","     |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n","     |      keyClass : str\n","     |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n","     |      valueClass : str\n","     |          fully qualified classname of value Writable class\n","     |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n","     |      keyConverter : str, optional\n","     |          fully qualified name of a function returning key WritableConverter\n","     |          (None by default)\n","     |      valueConverter : str, optional\n","     |          fully qualified name of a function returning value WritableConverter\n","     |          (None by default)\n","     |      conf : dict, optional\n","     |          Hadoop configuration, passed in as a dict (None by default)\n","     |      batchSize : int, optional, default 0\n","     |          The number of Python objects represented as a single\n","     |          Java object. (default 0, choose batchSize automatically)\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          RDD of tuples of key and corresponding value\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n","     |      :meth:`RDD.saveAsHadoopDataset`\n","     |      :meth:`SparkContext.hadoopRDD`\n","     |      :meth:`SparkContext.hadoopFile`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      \n","     |      Set the related classes\n","     |      \n","     |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n","     |      >>> input_format_class = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n","     |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n","     |      >>> value_class = \"org.apache.hadoop.io.Text\"\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"new_hadoop_file\")\n","     |      ...\n","     |      ...     # Create the conf for writing\n","     |      ...     write_conf = {\n","     |      ...         \"mapreduce.job.outputformat.class\": (output_format_class),\n","     |      ...         \"mapreduce.job.output.key.class\": key_class,\n","     |      ...         \"mapreduce.job.output.value.class\": value_class,\n","     |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n","     |      ...     }\n","     |      ...\n","     |      ...     # Write a temporary Hadoop file\n","     |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n","     |      ...     rdd.saveAsNewAPIHadoopDataset(conf=write_conf)\n","     |      ...\n","     |      ...     # Create the conf for reading\n","     |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n","     |      ...\n","     |      ...     loaded = sc.newAPIHadoopRDD(input_format_class,\n","     |      ...         key_class, value_class, conf=read_conf)\n","     |      ...     collected = sorted(loaded.collect())\n","     |      \n","     |      >>> collected\n","     |      [(1, ''), (1, 'a'), (3, 'x')]\n","     |  \n","     |  parallelize(self, c: Iterable[~T], numSlices: Optional[int] = None) -> pyspark.rdd.RDD[~T]\n","     |      Distribute a local Python collection to form an RDD. Using range\n","     |      is recommended if the input represents a range for performance.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      c : :class:`collections.abc.Iterable`\n","     |          iterable collection to distribute\n","     |      numSlices : int, optional\n","     |          the number of partitions of the new RDD\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          RDD representing distributed collection.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n","     |      [[0], [2], [3], [4], [6]]\n","     |      >>> sc.parallelize(range(0, 6, 2), 5).glom().collect()\n","     |      [[], [0], [], [2], [4]]\n","     |      \n","     |      Deal with a list of strings.\n","     |      \n","     |      >>> strings = [\"a\", \"b\", \"c\"]\n","     |      >>> sc.parallelize(strings, 2).glom().collect()\n","     |      [['a'], ['b', 'c']]\n","     |  \n","     |  pickleFile(self, name: str, minPartitions: Optional[int] = None) -> pyspark.rdd.RDD[typing.Any]\n","     |      Load an RDD previously saved using :meth:`RDD.saveAsPickleFile` method.\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      name : str\n","     |          directory to the input data files, the path can be comma separated\n","     |          paths as a list of inputs\n","     |      minPartitions : int, optional\n","     |          suggested minimum number of partitions for the resulting RDD\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          RDD representing unpickled data from the file(s).\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.saveAsPickleFile`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     # Write a temporary pickled file\n","     |      ...     path1 = os.path.join(d, \"pickled1\")\n","     |      ...     sc.parallelize(range(10)).saveAsPickleFile(path1, 3)\n","     |      ...\n","     |      ...     # Write another temporary pickled file\n","     |      ...     path2 = os.path.join(d, \"pickled2\")\n","     |      ...     sc.parallelize(range(-10, -5)).saveAsPickleFile(path2, 3)\n","     |      ...\n","     |      ...     # Load picked file\n","     |      ...     collected1 = sorted(sc.pickleFile(path1, 3).collect())\n","     |      ...     collected2 = sorted(sc.pickleFile(path2, 4).collect())\n","     |      ...\n","     |      ...     # Load two picked files together\n","     |      ...     collected3 = sorted(sc.pickleFile('{},{}'.format(path1, path2), 5).collect())\n","     |      \n","     |      >>> collected1\n","     |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","     |      >>> collected2\n","     |      [-10, -9, -8, -7, -6]\n","     |      >>> collected3\n","     |      [-10, -9, -8, -7, -6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","     |  \n","     |  range(self, start: int, end: Optional[int] = None, step: int = 1, numSlices: Optional[int] = None) -> pyspark.rdd.RDD[int]\n","     |      Create a new RDD of int containing elements from `start` to `end`\n","     |      (exclusive), increased by `step` every element. Can be called the same\n","     |      way as python's built-in range() function. If called with a single argument,\n","     |      the argument is interpreted as `end`, and `start` is set to 0.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      start : int\n","     |          the start value\n","     |      end : int, optional\n","     |          the end value (exclusive)\n","     |      step : int, optional, default 1\n","     |          the incremental step\n","     |      numSlices : int, optional\n","     |          the number of partitions of the new RDD\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          An RDD of int\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`pyspark.sql.SparkSession.range`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.range(5).collect()\n","     |      [0, 1, 2, 3, 4]\n","     |      >>> sc.range(2, 4).collect()\n","     |      [2, 3]\n","     |      >>> sc.range(1, 7, 2).collect()\n","     |      [1, 3, 5]\n","     |      \n","     |      Generate RDD with a negative step\n","     |      \n","     |      >>> sc.range(5, 0, -1).collect()\n","     |      [5, 4, 3, 2, 1]\n","     |      >>> sc.range(0, 5, -1).collect()\n","     |      []\n","     |      \n","     |      Control the number of partitions\n","     |      \n","     |      >>> sc.range(5, numSlices=1).getNumPartitions()\n","     |      1\n","     |      >>> sc.range(5, numSlices=10).getNumPartitions()\n","     |      10\n","     |  \n","     |  removeJobTag(self, tag: str) -> None\n","     |      Remove a tag previously added to be assigned to all the jobs started by this thread.\n","     |      Noop if such a tag was not added earlier.\n","     |      \n","     |      .. versionadded:: 3.5.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      tag : str\n","     |          The tag to be removed. Cannot contain ',' (comma) character.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.addJobTag`\n","     |      :meth:`SparkContext.getJobTags`\n","     |      :meth:`SparkContext.clearJobTags`\n","     |      :meth:`SparkContext.cancelJobsWithTag`\n","     |      :meth:`SparkContext.setInterruptOnCancel`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.addJobTag(\"job_to_cancel1\")\n","     |      >>> sc.addJobTag(\"job_to_cancel2\")\n","     |      >>> sc.getJobTags()\n","     |      {'job_to_cancel1', 'job_to_cancel2'}\n","     |      >>> sc.removeJobTag(\"job_to_cancel1\")\n","     |      >>> sc.getJobTags()\n","     |      {'job_to_cancel2'}\n","     |      >>> sc.clearJobTags()\n","     |  \n","     |  runJob(self, rdd: pyspark.rdd.RDD[~T], partitionFunc: Callable[[Iterable[~T]], Iterable[~U]], partitions: Optional[Sequence[int]] = None, allowLocal: bool = False) -> List[~U]\n","     |      Executes the given partitionFunc on the specified set of partitions,\n","     |      returning the result as an array of elements.\n","     |      \n","     |      If 'partitions' is not specified, this will run over all partitions.\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      rdd : :class:`RDD`\n","     |          target RDD to run tasks on\n","     |      partitionFunc : function\n","     |          a function to run on each partition of the RDD\n","     |      partitions : list, optional\n","     |          set of partitions to run on; some jobs may not want to compute on all\n","     |          partitions of the target RDD, e.g. for operations like `first`\n","     |      allowLocal : bool, default False\n","     |          this parameter takes no effect\n","     |      \n","     |      Returns\n","     |      -------\n","     |      list\n","     |          results of specified partitions\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.cancelAllJobs`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> myRDD = sc.parallelize(range(6), 3)\n","     |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n","     |      [0, 1, 4, 9, 16, 25]\n","     |      \n","     |      >>> myRDD = sc.parallelize(range(6), 3)\n","     |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n","     |      [0, 1, 16, 25]\n","     |  \n","     |  sequenceFile(self, path: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, minSplits: Optional[int] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n","     |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n","     |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n","     |      The mechanism is as follows:\n","     |      \n","     |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n","     |             and value Writable classes\n","     |          2. Serialization is attempted via Pickle pickling\n","     |          3. If this fails, the fallback is to call 'toString' on each key and value\n","     |          4. :class:`CPickleSerializer` is used to deserialize pickled objects on the Python side\n","     |      \n","     |      .. versionadded:: 1.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          path to sequencefile\n","     |      keyClass: str, optional\n","     |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n","     |      valueClass : str, optional\n","     |          fully qualified classname of value Writable class\n","     |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n","     |      keyConverter : str, optional\n","     |          fully qualified name of a function returning key WritableConverter\n","     |      valueConverter : str, optional\n","     |          fully qualifiedname of a function returning value WritableConverter\n","     |      minSplits : int, optional\n","     |          minimum splits in dataset (default min(2, sc.defaultParallelism))\n","     |      batchSize : int, optional, default 0\n","     |          The number of Python objects represented as a single\n","     |          Java object. (default 0, choose batchSize automatically)\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          RDD of tuples of key and corresponding value\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.saveAsSequenceFile`\n","     |      :meth:`RDD.saveAsNewAPIHadoopFile`\n","     |      :meth:`RDD.saveAsHadoopFile`\n","     |      :meth:`SparkContext.newAPIHadoopFile`\n","     |      :meth:`SparkContext.hadoopFile`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      \n","     |      Set the class of output format\n","     |      \n","     |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path = os.path.join(d, \"hadoop_file\")\n","     |      ...\n","     |      ...     # Write a temporary Hadoop file\n","     |      ...     rdd = sc.parallelize([(1, {3.0: \"bb\"}), (2, {1.0: \"aa\"}), (3, {2.0: \"dd\"})])\n","     |      ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class)\n","     |      ...\n","     |      ...     collected = sorted(sc.sequenceFile(path).collect())\n","     |      \n","     |      >>> collected\n","     |      [(1, {3.0: 'bb'}), (2, {1.0: 'aa'}), (3, {2.0: 'dd'})]\n","     |  \n","     |  setCheckpointDir(self, dirName: str) -> None\n","     |      Set the directory under which RDDs are going to be checkpointed. The\n","     |      directory must be an HDFS path if running on a cluster.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      dirName : str\n","     |          path to the directory where checkpoint files will be stored\n","     |          (must be HDFS path if running in cluster)\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.getCheckpointDir`\n","     |      :meth:`RDD.checkpoint`\n","     |      :meth:`RDD.getCheckpointFile`\n","     |  \n","     |  setInterruptOnCancel(self, interruptOnCancel: bool) -> None\n","     |      Set the behavior of job cancellation from jobs started in this thread.\n","     |      \n","     |      .. versionadded:: 3.5.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      interruptOnCancel : bool\n","     |          If true, then job cancellation will result in ``Thread.interrupt()``\n","     |          being called on the job's executor threads. This is useful to help ensure that\n","     |          the tasks are actually stopped in a timely manner, but is off by default due to\n","     |          HDFS-1208, where HDFS may respond to ``Thread.interrupt()`` by marking nodes as dead.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.addJobTag`\n","     |      :meth:`SparkContext.removeJobTag`\n","     |      :meth:`SparkContext.cancelAllJobs`\n","     |      :meth:`SparkContext.cancelJobGroup`\n","     |      :meth:`SparkContext.cancelJobsWithTag`\n","     |  \n","     |  setJobDescription(self, value: str) -> None\n","     |      Set a human readable description of the current job.\n","     |      \n","     |      .. versionadded:: 2.3.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      value : str\n","     |          The job description to set.\n","     |      \n","     |      Notes\n","     |      -----\n","     |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n","     |      local inheritance.\n","     |  \n","     |  setJobGroup(self, groupId: str, description: str, interruptOnCancel: bool = False) -> None\n","     |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n","     |      different value or cleared.\n","     |      \n","     |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n","     |      Application programmers can use this method to group all those jobs together and give a\n","     |      group description. Once set, the Spark web UI will associate such jobs with this group.\n","     |      \n","     |      The application can use :meth:`SparkContext.cancelJobGroup` to cancel all\n","     |      running jobs in this group.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      groupId : str\n","     |          The group ID to assign.\n","     |      description : str\n","     |          The description to set for the job group.\n","     |      interruptOnCancel : bool, optional, default False\n","     |          whether to interrupt jobs on job cancellation.\n","     |      \n","     |      Notes\n","     |      -----\n","     |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n","     |      in Thread.interrupt() being called on the job's executor threads. This is useful to help\n","     |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n","     |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n","     |      \n","     |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n","     |      local inheritance.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.cancelJobGroup`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import threading\n","     |      >>> from time import sleep\n","     |      >>> from pyspark import InheritableThread\n","     |      >>> result = \"Not Set\"\n","     |      >>> lock = threading.Lock()\n","     |      >>> def map_func(x):\n","     |      ...     sleep(100)\n","     |      ...     raise RuntimeError(\"Task should have been cancelled\")\n","     |      ...\n","     |      >>> def start_job(x):\n","     |      ...     global result\n","     |      ...     try:\n","     |      ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n","     |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n","     |      ...     except Exception as e:\n","     |      ...         result = \"Cancelled\"\n","     |      ...     lock.release()\n","     |      ...\n","     |      >>> def stop_job():\n","     |      ...     sleep(5)\n","     |      ...     sc.cancelJobGroup(\"job_to_cancel\")\n","     |      ...\n","     |      >>> suppress = lock.acquire()\n","     |      >>> suppress = InheritableThread(target=start_job, args=(10,)).start()\n","     |      >>> suppress = InheritableThread(target=stop_job).start()\n","     |      >>> suppress = lock.acquire()\n","     |      >>> print(result)\n","     |      Cancelled\n","     |  \n","     |  setLocalProperty(self, key: str, value: str) -> None\n","     |      Set a local property that affects jobs submitted from this thread, such as the\n","     |      Spark fair scheduler pool.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      key : str\n","     |          The key of the local property to set.\n","     |      value : str\n","     |          The value of the local property to set.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.getLocalProperty`\n","     |      \n","     |      Notes\n","     |      -----\n","     |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n","     |      local inheritance.\n","     |  \n","     |  setLogLevel(self, logLevel: str) -> None\n","     |      Control our logLevel. This overrides any user-defined log settings.\n","     |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      logLevel : str\n","     |          The desired log level as a string.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.setLogLevel(\"WARN\")  # doctest :+SKIP\n","     |  \n","     |  show_profiles(self) -> None\n","     |      Print the profile stats to stdout\n","     |      \n","     |      .. versionadded:: 1.2.0\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.dump_profiles`\n","     |  \n","     |  sparkUser(self) -> str\n","     |      Get SPARK_USER for user who is running SparkContext.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |  \n","     |  statusTracker(self) -> pyspark.status.StatusTracker\n","     |      Return :class:`StatusTracker` object\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |  \n","     |  stop(self) -> None\n","     |      Shut down the :class:`SparkContext`.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |  \n","     |  textFile(self, name: str, minPartitions: Optional[int] = None, use_unicode: bool = True) -> pyspark.rdd.RDD[str]\n","     |      Read a text file from HDFS, a local file system (available on all\n","     |      nodes), or any Hadoop-supported file system URI, and return it as an\n","     |      RDD of Strings. The text files must be encoded as UTF-8.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      name : str\n","     |          directory to the input data files, the path can be comma separated\n","     |          paths as a list of inputs\n","     |      minPartitions : int, optional\n","     |          suggested minimum number of partitions for the resulting RDD\n","     |      use_unicode : bool, default True\n","     |          If `use_unicode` is False, the strings will be kept as `str` (encoding\n","     |          as `utf-8`), which is faster and smaller than unicode.\n","     |      \n","     |          .. versionadded:: 1.2.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          RDD representing text data from the file(s).\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.saveAsTextFile`\n","     |      :meth:`SparkContext.wholeTextFiles`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path1 = os.path.join(d, \"text1\")\n","     |      ...     path2 = os.path.join(d, \"text2\")\n","     |      ...\n","     |      ...     # Write a temporary text file\n","     |      ...     sc.parallelize([\"x\", \"y\", \"z\"]).saveAsTextFile(path1)\n","     |      ...\n","     |      ...     # Write another temporary text file\n","     |      ...     sc.parallelize([\"aa\", \"bb\", \"cc\"]).saveAsTextFile(path2)\n","     |      ...\n","     |      ...     # Load text file\n","     |      ...     collected1 = sorted(sc.textFile(path1, 3).collect())\n","     |      ...     collected2 = sorted(sc.textFile(path2, 4).collect())\n","     |      ...\n","     |      ...     # Load two text files together\n","     |      ...     collected3 = sorted(sc.textFile('{},{}'.format(path1, path2), 5).collect())\n","     |      \n","     |      >>> collected1\n","     |      ['x', 'y', 'z']\n","     |      >>> collected2\n","     |      ['aa', 'bb', 'cc']\n","     |      >>> collected3\n","     |      ['aa', 'bb', 'cc', 'x', 'y', 'z']\n","     |  \n","     |  union(self, rdds: List[pyspark.rdd.RDD[~T]]) -> pyspark.rdd.RDD[~T]\n","     |      Build the union of a list of RDDs.\n","     |      \n","     |      This supports unions() of RDDs with different serialized formats,\n","     |      although this forces them to be reserialized using the default\n","     |      serializer:\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.union`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     # generate a text RDD\n","     |      ...     with open(os.path.join(d, \"union-text.txt\"), \"w\") as f:\n","     |      ...         _ = f.write(\"Hello\")\n","     |      ...     text_rdd = sc.textFile(d)\n","     |      ...\n","     |      ...     # generate another RDD\n","     |      ...     parallelized = sc.parallelize([\"World!\"])\n","     |      ...\n","     |      ...     unioned = sorted(sc.union([text_rdd, parallelized]).collect())\n","     |      \n","     |      >>> unioned\n","     |      ['Hello', 'World!']\n","     |  \n","     |  wholeTextFiles(self, path: str, minPartitions: Optional[int] = None, use_unicode: bool = True) -> pyspark.rdd.RDD[typing.Tuple[str, str]]\n","     |      Read a directory of text files from HDFS, a local file system\n","     |      (available on all nodes), or any  Hadoop-supported file system\n","     |      URI. Each file is read as a single record and returned in a\n","     |      key-value pair, where the key is the path of each file, the\n","     |      value is the content of each file.\n","     |      The text files must be encoded as UTF-8.\n","     |      \n","     |      .. versionadded:: 1.0.0\n","     |      \n","     |      For example, if you have the following files:\n","     |      \n","     |      .. code-block:: text\n","     |      \n","     |          hdfs://a-hdfs-path/part-00000\n","     |          hdfs://a-hdfs-path/part-00001\n","     |          ...\n","     |          hdfs://a-hdfs-path/part-nnnnn\n","     |      \n","     |      Do ``rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")``,\n","     |      then ``rdd`` contains:\n","     |      \n","     |      .. code-block:: text\n","     |      \n","     |          (a-hdfs-path/part-00000, its content)\n","     |          (a-hdfs-path/part-00001, its content)\n","     |          ...\n","     |          (a-hdfs-path/part-nnnnn, its content)\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      path : str\n","     |          directory to the input data files, the path can be comma separated\n","     |          paths as a list of inputs\n","     |      minPartitions : int, optional\n","     |          suggested minimum number of partitions for the resulting RDD\n","     |      use_unicode : bool, default True\n","     |          If `use_unicode` is False, the strings will be kept as `str` (encoding\n","     |          as `utf-8`), which is faster and smaller than unicode.\n","     |      \n","     |          .. versionadded:: 1.2.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`RDD`\n","     |          RDD representing path-content pairs from the file(s).\n","     |      \n","     |      Notes\n","     |      -----\n","     |      Small files are preferred, as each file will be loaded fully in memory.\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`RDD.saveAsTextFile`\n","     |      :meth:`SparkContext.textFile`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     # Write a temporary text file\n","     |      ...     with open(os.path.join(d, \"1.txt\"), \"w\") as f:\n","     |      ...         _ = f.write(\"123\")\n","     |      ...\n","     |      ...     # Write another temporary text file\n","     |      ...     with open(os.path.join(d, \"2.txt\"), \"w\") as f:\n","     |      ...         _ = f.write(\"xyz\")\n","     |      ...\n","     |      ...     collected = sorted(sc.wholeTextFiles(d).collect())\n","     |      >>> collected\n","     |      [('.../1.txt', '123'), ('.../2.txt', 'xyz')]\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods defined here:\n","     |  \n","     |  getOrCreate(conf: Optional[pyspark.conf.SparkConf] = None) -> 'SparkContext'\n","     |      Get or instantiate a :class:`SparkContext` and register it as a singleton object.\n","     |      \n","     |      .. versionadded:: 1.4.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      conf : :class:`SparkConf`, optional\n","     |          :class:`SparkConf` that will be used for initialization of the :class:`SparkContext`.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`SparkContext`\n","     |          current :class:`SparkContext`, or a new one if it wasn't created before the function\n","     |          call.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> SparkContext.getOrCreate()\n","     |      <SparkContext ...>\n","     |  \n","     |  setSystemProperty(key: str, value: str) -> None\n","     |      Set a Java system property, such as `spark.executor.memory`. This must\n","     |      be invoked before instantiating :class:`SparkContext`.\n","     |      \n","     |      .. versionadded:: 0.9.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      key : str\n","     |          The key of a new Java system property.\n","     |      value : str\n","     |          The value of a new Java system property.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Readonly properties defined here:\n","     |  \n","     |  applicationId\n","     |      A unique identifier for the Spark application.\n","     |      Its format depends on the scheduler implementation.\n","     |      \n","     |      * in case of local spark app something like 'local-1433865536131'\n","     |      * in case of YARN something like 'application_1433865536131_34483'\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.applicationId  # doctest: +ELLIPSIS\n","     |      'local-...'\n","     |  \n","     |  defaultMinPartitions\n","     |      Default min number of partitions for Hadoop RDDs when not given by user\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.defaultMinPartitions > 0\n","     |      True\n","     |  \n","     |  defaultParallelism\n","     |      Default level of parallelism to use when not given by user (e.g. for reduce tasks)\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.defaultParallelism > 0\n","     |      True\n","     |  \n","     |  listArchives\n","     |      Returns a list of archive paths that are added to resources.\n","     |      \n","     |      .. versionadded:: 3.4.0\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.addArchive`\n","     |  \n","     |  listFiles\n","     |      Returns a list of file paths that are added to resources.\n","     |      \n","     |      .. versionadded:: 3.4.0\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkContext.addFile`\n","     |  \n","     |  resources\n","     |      Return the resource information of this :class:`SparkContext`.\n","     |      A resource could be a GPU, FPGA, etc.\n","     |      \n","     |      .. versionadded:: 3.0.0\n","     |  \n","     |  startTime\n","     |      Return the epoch time when the :class:`SparkContext` was started.\n","     |      \n","     |      .. versionadded:: 1.5.0\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> _ = sc.startTime\n","     |  \n","     |  uiWebUrl\n","     |      Return the URL of the SparkUI instance started by this :class:`SparkContext`\n","     |      \n","     |      .. versionadded:: 2.1.0\n","     |      \n","     |      Notes\n","     |      -----\n","     |      When the web ui is disabled, e.g., by ``spark.ui.enabled`` set to ``False``,\n","     |      it returns ``None``.\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> sc.uiWebUrl\n","     |      'http://...'\n","     |  \n","     |  version\n","     |      The version of Spark on which this application is running.\n","     |      \n","     |      .. versionadded:: 1.1.0\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> _ = sc.version\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')\n","     |  \n","     |  __annotations__ = {'PACKAGE_EXTENSIONS': typing.Iterable[str], '_activ...\n","    \n","    class SparkFiles(builtins.object)\n","     |  SparkFiles() -> None\n","     |  \n","     |  Resolves paths to files added through :meth:`SparkContext.addFile`.\n","     |  \n","     |  SparkFiles contains only classmethods; users should not create SparkFiles\n","     |  instances.\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self) -> None\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods defined here:\n","     |  \n","     |  get(filename: str) -> str\n","     |      Get the absolute path of a file added through\n","     |      :meth:`SparkContext.addFile` or :meth:`SparkContext.addPyFile`.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      filename : str\n","     |          file that are added to resources\n","     |      \n","     |      Returns\n","     |      -------\n","     |      str\n","     |          the absolute path of the file\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkFiles.getRootDirectory`\n","     |      :meth:`SparkContext.addFile`\n","     |      :meth:`SparkContext.addPyFile`\n","     |      :meth:`SparkContext.listFiles`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> import os\n","     |      >>> import tempfile\n","     |      >>> from pyspark import SparkFiles\n","     |      \n","     |      >>> with tempfile.TemporaryDirectory() as d:\n","     |      ...     path1 = os.path.join(d, \"test.txt\")\n","     |      ...     with open(path1, \"w\") as f:\n","     |      ...         _ = f.write(\"100\")\n","     |      ...\n","     |      ...     sc.addFile(path1)\n","     |      ...     file_list1 = sorted(sc.listFiles)\n","     |      ...\n","     |      ...     def func1(iterator):\n","     |      ...         path = SparkFiles.get(\"test.txt\")\n","     |      ...         assert path.startswith(SparkFiles.getRootDirectory())\n","     |      ...         return [path]\n","     |      ...\n","     |      ...     path_list1 = sc.parallelize([1, 2, 3, 4]).mapPartitions(func1).collect()\n","     |      ...\n","     |      ...     path2 = os.path.join(d, \"test.py\")\n","     |      ...     with open(path2, \"w\") as f:\n","     |      ...         _ = f.write(\"import pyspark\")\n","     |      ...\n","     |      ...     # py files\n","     |      ...     sc.addPyFile(path2)\n","     |      ...     file_list2 = sorted(sc.listFiles)\n","     |      ...\n","     |      ...     def func2(iterator):\n","     |      ...         path = SparkFiles.get(\"test.py\")\n","     |      ...         assert path.startswith(SparkFiles.getRootDirectory())\n","     |      ...         return [path]\n","     |      ...\n","     |      ...     path_list2 = sc.parallelize([1, 2, 3, 4]).mapPartitions(func2).collect()\n","     |      >>> file_list1\n","     |      ['file:/.../test.txt']\n","     |      >>> set(path_list1)\n","     |      {'.../test.txt'}\n","     |      >>> file_list2\n","     |      ['file:/.../test.py', 'file:/.../test.txt']\n","     |      >>> set(path_list2)\n","     |      {'.../test.py'}\n","     |  \n","     |  getRootDirectory() -> str\n","     |      Get the root directory that contains files added through\n","     |      :meth:`SparkContext.addFile` or :meth:`SparkContext.addPyFile`.\n","     |      \n","     |      .. versionadded:: 0.7.0\n","     |      \n","     |      Returns\n","     |      -------\n","     |      str\n","     |          the root directory that contains files added to resources\n","     |      \n","     |      See Also\n","     |      --------\n","     |      :meth:`SparkFiles.get`\n","     |      :meth:`SparkContext.addFile`\n","     |      :meth:`SparkContext.addPyFile`\n","     |      \n","     |      Examples\n","     |      --------\n","     |      >>> from pyspark.files import SparkFiles\n","     |      >>> SparkFiles.getRootDirectory()  # doctest: +SKIP\n","     |      '.../spark-a904728e-08d3-400c-a872-cfd82fd6dcd2/userFiles-648cf6d6-bb2c-4f53-82bd-e658aba0c5de'\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __annotations__ = {'_is_running_on_worker': typing.ClassVar[bool], '_r...\n","    \n","    class SparkJobInfo(builtins.tuple)\n","     |  SparkJobInfo(jobId: int, stageIds: py4j.java_collections.JavaArray, status: str)\n","     |  \n","     |  Exposes information about Spark Jobs.\n","     |  \n","     |  Method resolution order:\n","     |      SparkJobInfo\n","     |      builtins.tuple\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __getnewargs__(self) from collections.SparkJobInfo\n","     |      Return self as a plain tuple.  Used by copy and pickle.\n","     |  \n","     |  __repr__(self) from collections.SparkJobInfo\n","     |      Return a nicely formatted representation string\n","     |  \n","     |  _asdict(self) from collections.SparkJobInfo\n","     |      Return a new dict which maps field names to their values.\n","     |  \n","     |  _replace(self, /, **kwds) from collections.SparkJobInfo\n","     |      Return a new SparkJobInfo object replacing specified fields with new values\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods defined here:\n","     |  \n","     |  _make(iterable) from collections.SparkJobInfo\n","     |      Make a new SparkJobInfo object from a sequence or iterable\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods defined here:\n","     |  \n","     |  __new__(_cls, jobId: int, stageIds: py4j.java_collections.JavaArray, status: str) from namedtuple_SparkJobInfo.SparkJobInfo\n","     |      Create new instance of SparkJobInfo(jobId, stageIds, status)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  jobId\n","     |      Alias for field number 0\n","     |  \n","     |  stageIds\n","     |      Alias for field number 1\n","     |  \n","     |  status\n","     |      Alias for field number 2\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __annotations__ = {'jobId': <class 'int'>, 'stageIds': <class 'py4j.ja...\n","     |  \n","     |  __match_args__ = ('jobId', 'stageIds', 'status')\n","     |  \n","     |  __orig_bases__ = (<function NamedTuple>,)\n","     |  \n","     |  _field_defaults = {}\n","     |  \n","     |  _fields = ('jobId', 'stageIds', 'status')\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from builtins.tuple:\n","     |  \n","     |  __add__(self, value, /)\n","     |      Return self+value.\n","     |  \n","     |  __contains__(self, key, /)\n","     |      Return key in self.\n","     |  \n","     |  __eq__(self, value, /)\n","     |      Return self==value.\n","     |  \n","     |  __ge__(self, value, /)\n","     |      Return self>=value.\n","     |  \n","     |  __getattribute__(self, name, /)\n","     |      Return getattr(self, name).\n","     |  \n","     |  __getitem__(self, key, /)\n","     |      Return self[key].\n","     |  \n","     |  __gt__(self, value, /)\n","     |      Return self>value.\n","     |  \n","     |  __hash__(self, /)\n","     |      Return hash(self).\n","     |  \n","     |  __iter__(self, /)\n","     |      Implement iter(self).\n","     |  \n","     |  __le__(self, value, /)\n","     |      Return self<=value.\n","     |  \n","     |  __len__(self, /)\n","     |      Return len(self).\n","     |  \n","     |  __lt__(self, value, /)\n","     |      Return self<value.\n","     |  \n","     |  __mul__(self, value, /)\n","     |      Return self*value.\n","     |  \n","     |  __ne__(self, value, /)\n","     |      Return self!=value.\n","     |  \n","     |  __rmul__(self, value, /)\n","     |      Return value*self.\n","     |  \n","     |  count(self, value, /)\n","     |      Return number of occurrences of value.\n","     |  \n","     |  index(self, value, start=0, stop=9223372036854775807, /)\n","     |      Return first index of value.\n","     |      \n","     |      Raises ValueError if the value is not present.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from builtins.tuple:\n","     |  \n","     |  __class_getitem__(...)\n","     |      See PEP 585\n","    \n","    class SparkStageInfo(builtins.tuple)\n","     |  SparkStageInfo(stageId: int, currentAttemptId: int, name: str, numTasks: int, numActiveTasks: int, numCompletedTasks: int, numFailedTasks: int)\n","     |  \n","     |  Exposes information about Spark Stages.\n","     |  \n","     |  Method resolution order:\n","     |      SparkStageInfo\n","     |      builtins.tuple\n","     |      builtins.object\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __getnewargs__(self) from collections.SparkStageInfo\n","     |      Return self as a plain tuple.  Used by copy and pickle.\n","     |  \n","     |  __repr__(self) from collections.SparkStageInfo\n","     |      Return a nicely formatted representation string\n","     |  \n","     |  _asdict(self) from collections.SparkStageInfo\n","     |      Return a new dict which maps field names to their values.\n","     |  \n","     |  _replace(self, /, **kwds) from collections.SparkStageInfo\n","     |      Return a new SparkStageInfo object replacing specified fields with new values\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods defined here:\n","     |  \n","     |  _make(iterable) from collections.SparkStageInfo\n","     |      Make a new SparkStageInfo object from a sequence or iterable\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods defined here:\n","     |  \n","     |  __new__(_cls, stageId: int, currentAttemptId: int, name: str, numTasks: int, numActiveTasks: int, numCompletedTasks: int, numFailedTasks: int) from namedtuple_SparkStageInfo.SparkStageInfo\n","     |      Create new instance of SparkStageInfo(stageId, currentAttemptId, name, numTasks, numActiveTasks, numCompletedTasks, numFailedTasks)\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  stageId\n","     |      Alias for field number 0\n","     |  \n","     |  currentAttemptId\n","     |      Alias for field number 1\n","     |  \n","     |  name\n","     |      Alias for field number 2\n","     |  \n","     |  numTasks\n","     |      Alias for field number 3\n","     |  \n","     |  numActiveTasks\n","     |      Alias for field number 4\n","     |  \n","     |  numCompletedTasks\n","     |      Alias for field number 5\n","     |  \n","     |  numFailedTasks\n","     |      Alias for field number 6\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __annotations__ = {'currentAttemptId': <class 'int'>, 'name': <class '...\n","     |  \n","     |  __match_args__ = ('stageId', 'currentAttemptId', 'name', 'numTasks', '...\n","     |  \n","     |  __orig_bases__ = (<function NamedTuple>,)\n","     |  \n","     |  _field_defaults = {}\n","     |  \n","     |  _fields = ('stageId', 'currentAttemptId', 'name', 'numTasks', 'numActi...\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Methods inherited from builtins.tuple:\n","     |  \n","     |  __add__(self, value, /)\n","     |      Return self+value.\n","     |  \n","     |  __contains__(self, key, /)\n","     |      Return key in self.\n","     |  \n","     |  __eq__(self, value, /)\n","     |      Return self==value.\n","     |  \n","     |  __ge__(self, value, /)\n","     |      Return self>=value.\n","     |  \n","     |  __getattribute__(self, name, /)\n","     |      Return getattr(self, name).\n","     |  \n","     |  __getitem__(self, key, /)\n","     |      Return self[key].\n","     |  \n","     |  __gt__(self, value, /)\n","     |      Return self>value.\n","     |  \n","     |  __hash__(self, /)\n","     |      Return hash(self).\n","     |  \n","     |  __iter__(self, /)\n","     |      Implement iter(self).\n","     |  \n","     |  __le__(self, value, /)\n","     |      Return self<=value.\n","     |  \n","     |  __len__(self, /)\n","     |      Return len(self).\n","     |  \n","     |  __lt__(self, value, /)\n","     |      Return self<value.\n","     |  \n","     |  __mul__(self, value, /)\n","     |      Return self*value.\n","     |  \n","     |  __ne__(self, value, /)\n","     |      Return self!=value.\n","     |  \n","     |  __rmul__(self, value, /)\n","     |      Return value*self.\n","     |  \n","     |  count(self, value, /)\n","     |      Return number of occurrences of value.\n","     |  \n","     |  index(self, value, start=0, stop=9223372036854775807, /)\n","     |      Return first index of value.\n","     |      \n","     |      Raises ValueError if the value is not present.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods inherited from builtins.tuple:\n","     |  \n","     |  __class_getitem__(...)\n","     |      See PEP 585\n","    \n","    class StatusTracker(builtins.object)\n","     |  StatusTracker(jtracker: py4j.java_gateway.JavaObject)\n","     |  \n","     |  Low-level status reporting APIs for monitoring job and stage progress.\n","     |  \n","     |  These APIs intentionally provide very weak consistency semantics;\n","     |  consumers of these APIs should be prepared to handle empty / missing\n","     |  information. For example, a job's stage ids may be known but the status\n","     |  API may not have any information about the details of those stages, so\n","     |  `getStageInfo` could potentially return `None` for a valid stage id.\n","     |  \n","     |  To limit memory usage, these APIs only provide information on recent\n","     |  jobs / stages.  These APIs will provide information for the last\n","     |  `spark.ui.retainedStages` stages and `spark.ui.retainedJobs` jobs.\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __init__(self, jtracker: py4j.java_gateway.JavaObject)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  getActiveJobsIds(self) -> List[int]\n","     |      Returns an array containing the ids of all active jobs.\n","     |  \n","     |  getActiveStageIds(self) -> List[int]\n","     |      Returns an array containing the ids of all active stages.\n","     |  \n","     |  getJobIdsForGroup(self, jobGroup: Optional[str] = None) -> List[int]\n","     |      Return a list of all known jobs in a particular job group.  If\n","     |      `jobGroup` is None, then returns all known jobs that are not\n","     |      associated with a job group.\n","     |      \n","     |      The returned list may contain running, failed, and completed jobs,\n","     |      and may vary across invocations of this method. This method does\n","     |      not guarantee the order of the elements in its result.\n","     |  \n","     |  getJobInfo(self, jobId: int) -> Optional[pyspark.status.SparkJobInfo]\n","     |      Returns a :class:`SparkJobInfo` object, or None if the job info\n","     |      could not be found or was garbage collected.\n","     |  \n","     |  getStageInfo(self, stageId: int) -> Optional[pyspark.status.SparkStageInfo]\n","     |      Returns a :class:`SparkStageInfo` object, or None if the stage\n","     |      info could not be found or was garbage collected.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","    \n","    class StorageLevel(builtins.object)\n","     |  StorageLevel(useDisk: bool, useMemory: bool, useOffHeap: bool, deserialized: bool, replication: int = 1)\n","     |  \n","     |  Flags for controlling the storage of an RDD. Each StorageLevel records whether to use memory,\n","     |  whether to drop the RDD to disk if it falls out of memory, whether to keep the data in memory\n","     |  in a JAVA-specific serialized format, and whether to replicate the RDD partitions on multiple\n","     |  nodes. Also contains static constants for some commonly used storage levels, MEMORY_ONLY.\n","     |  Since the data is always serialized on the Python side, all the constants use the serialized\n","     |  formats.\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  __eq__(self, other: Any) -> bool\n","     |      Return self==value.\n","     |  \n","     |  __init__(self, useDisk: bool, useMemory: bool, useOffHeap: bool, deserialized: bool, replication: int = 1)\n","     |      Initialize self.  See help(type(self)) for accurate signature.\n","     |  \n","     |  __repr__(self) -> str\n","     |      Return repr(self).\n","     |  \n","     |  __str__(self) -> str\n","     |      Return str(self).\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  DISK_ONLY = StorageLevel(True, False, False, False, 1)\n","     |  \n","     |  DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\n","     |  \n","     |  DISK_ONLY_3 = StorageLevel(True, False, False, False, 3)\n","     |  \n","     |  MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)\n","     |  \n","     |  MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)\n","     |  \n","     |  MEMORY_AND_DISK_DESER = StorageLevel(True, True, False, True, 1)\n","     |  \n","     |  MEMORY_ONLY = StorageLevel(False, True, False, False, 1)\n","     |  \n","     |  MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)\n","     |  \n","     |  NONE = StorageLevel(False, False, False, False, 1)\n","     |  \n","     |  OFF_HEAP = StorageLevel(True, True, True, False, 1)\n","     |  \n","     |  __annotations__ = {'DISK_ONLY': typing.ClassVar[ForwardRef('StorageLev...\n","     |  \n","     |  __hash__ = None\n","    \n","    class TaskContext(builtins.object)\n","     |  TaskContext() -> 'TaskContext'\n","     |  \n","     |  Contextual information about a task which can be read or mutated during\n","     |  execution. To access the TaskContext for a running task, use:\n","     |  :meth:`TaskContext.get`.\n","     |  \n","     |  .. versionadded:: 2.2.0\n","     |  \n","     |  Examples\n","     |  --------\n","     |  >>> from pyspark import TaskContext\n","     |  \n","     |  Get a task context instance from :class:`RDD`.\n","     |  \n","     |  >>> spark.sparkContext.setLocalProperty(\"key1\", \"value\")\n","     |  >>> taskcontext = spark.sparkContext.parallelize([1]).map(lambda _: TaskContext.get()).first()\n","     |  >>> isinstance(taskcontext.attemptNumber(), int)\n","     |  True\n","     |  >>> isinstance(taskcontext.partitionId(), int)\n","     |  True\n","     |  >>> isinstance(taskcontext.stageId(), int)\n","     |  True\n","     |  >>> isinstance(taskcontext.taskAttemptId(), int)\n","     |  True\n","     |  >>> taskcontext.getLocalProperty(\"key1\")\n","     |  'value'\n","     |  >>> isinstance(taskcontext.cpus(), int)\n","     |  True\n","     |  \n","     |  Get a task context instance from a dataframe via Python UDF.\n","     |  \n","     |  >>> from pyspark.sql import Row\n","     |  >>> from pyspark.sql.functions import udf\n","     |  >>> @udf(\"STRUCT<anum: INT, partid: INT, stageid: INT, taskaid: INT, prop: STRING, cpus: INT>\")\n","     |  ... def taskcontext_as_row():\n","     |  ...    taskcontext = TaskContext.get()\n","     |  ...    return Row(\n","     |  ...        anum=taskcontext.attemptNumber(),\n","     |  ...        partid=taskcontext.partitionId(),\n","     |  ...        stageid=taskcontext.stageId(),\n","     |  ...        taskaid=taskcontext.taskAttemptId(),\n","     |  ...        prop=taskcontext.getLocalProperty(\"key2\"),\n","     |  ...        cpus=taskcontext.cpus())\n","     |  ...\n","     |  >>> spark.sparkContext.setLocalProperty(\"key2\", \"value\")\n","     |  >>> [(anum, partid, stageid, taskaid, prop, cpus)] = (\n","     |  ...     spark.range(1).select(taskcontext_as_row()).first()\n","     |  ... )\n","     |  >>> isinstance(anum, int)\n","     |  True\n","     |  >>> isinstance(partid, int)\n","     |  True\n","     |  >>> isinstance(stageid, int)\n","     |  True\n","     |  >>> isinstance(taskaid, int)\n","     |  True\n","     |  >>> prop\n","     |  'value'\n","     |  >>> isinstance(cpus, int)\n","     |  True\n","     |  \n","     |  Get a task context instance from a dataframe via Pandas UDF.\n","     |  \n","     |  >>> import pandas as pd  # doctest: +SKIP\n","     |  >>> from pyspark.sql.functions import pandas_udf\n","     |  >>> @pandas_udf(\"STRUCT<\"\n","     |  ...     \"anum: INT, partid: INT, stageid: INT, taskaid: INT, prop: STRING, cpus: INT>\")\n","     |  ... def taskcontext_as_row(_):\n","     |  ...    taskcontext = TaskContext.get()\n","     |  ...    return pd.DataFrame({\n","     |  ...        \"anum\": [taskcontext.attemptNumber()],\n","     |  ...        \"partid\": [taskcontext.partitionId()],\n","     |  ...        \"stageid\": [taskcontext.stageId()],\n","     |  ...        \"taskaid\": [taskcontext.taskAttemptId()],\n","     |  ...        \"prop\": [taskcontext.getLocalProperty(\"key3\")],\n","     |  ...        \"cpus\": [taskcontext.cpus()]\n","     |  ...    })  # doctest: +SKIP\n","     |  ...\n","     |  >>> spark.sparkContext.setLocalProperty(\"key3\", \"value\")  # doctest: +SKIP\n","     |  >>> [(anum, partid, stageid, taskaid, prop, cpus)] = (\n","     |  ...     spark.range(1).select(taskcontext_as_row(\"id\")).first()\n","     |  ... )  # doctest: +SKIP\n","     |  >>> isinstance(anum, int)\n","     |  True\n","     |  >>> isinstance(partid, int)\n","     |  True\n","     |  >>> isinstance(stageid, int)\n","     |  True\n","     |  >>> isinstance(taskaid, int)\n","     |  True\n","     |  >>> prop\n","     |  'value'\n","     |  >>> isinstance(cpus, int)\n","     |  True\n","     |  \n","     |  Methods defined here:\n","     |  \n","     |  attemptNumber(self) -> int\n","     |      How many times this task has been attempted.  The first task attempt will be assigned\n","     |      attemptNumber = 0, and subsequent attempts will have increasing attempt numbers.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          current attempt number.\n","     |  \n","     |  cpus(self) -> int\n","     |      CPUs allocated to the task.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          the number of CPUs.\n","     |  \n","     |  getLocalProperty(self, key: str) -> Optional[str]\n","     |      Get a local property set upstream in the driver, or None if it is missing.\n","     |      \n","     |      Parameters\n","     |      ----------\n","     |      key : str\n","     |          the key of the local property to get.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          the value of the local property.\n","     |  \n","     |  partitionId(self) -> int\n","     |      The ID of the RDD partition that is computed by this task.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          current partition id.\n","     |  \n","     |  resources(self) -> Dict[str, pyspark.resource.information.ResourceInformation]\n","     |      Resources allocated to the task. The key is the resource name and the value is information\n","     |      about the resource.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      dict\n","     |          a dictionary of a string resource name, and :class:`ResourceInformation`.\n","     |  \n","     |  stageId(self) -> int\n","     |      The ID of the stage that this task belong to.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          current stage id.\n","     |  \n","     |  taskAttemptId(self) -> int\n","     |      An ID that is unique to this task attempt (within the same :class:`SparkContext`,\n","     |      no two task attempts will share the same attempt ID).  This is roughly equivalent\n","     |      to Hadoop's `TaskAttemptID`.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      int\n","     |          current task attempt id.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Class methods defined here:\n","     |  \n","     |  get() -> Optional[ForwardRef('TaskContext')]\n","     |      Return the currently active :class:`TaskContext`. This can be called inside of\n","     |      user functions to access contextual information about running tasks.\n","     |      \n","     |      Returns\n","     |      -------\n","     |      :class:`TaskContext`, optional\n","     |      \n","     |      Notes\n","     |      -----\n","     |      Must be called on the worker, not the driver. Returns ``None`` if not initialized.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Static methods defined here:\n","     |  \n","     |  __new__(cls: Type[ForwardRef('TaskContext')]) -> 'TaskContext'\n","     |      Even if users construct :class:`TaskContext` instead of using get, give them the singleton.\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data descriptors defined here:\n","     |  \n","     |  __dict__\n","     |      dictionary for instance variables\n","     |  \n","     |  __weakref__\n","     |      list of weak references to the object\n","     |  \n","     |  ----------------------------------------------------------------------\n","     |  Data and other attributes defined here:\n","     |  \n","     |  __annotations__ = {'_attemptNumber': typing.Optional[int], '_cpus': ty...\n","\n","FUNCTIONS\n","    inheritable_thread_target(f: Callable) -> Callable\n","        Return thread target wrapper which is recommended to be used in PySpark when the\n","        pinned thread mode is enabled. The wrapper function, before calling original\n","        thread target, it inherits the inheritable properties specific\n","        to JVM thread such as ``InheritableThreadLocal``.\n","        \n","        Also, note that pinned thread mode does not close the connection from Python\n","        to JVM when the thread is finished in the Python side. With this wrapper, Python\n","        garbage-collects the Python thread instance and also closes the connection\n","        which finishes JVM thread correctly.\n","        \n","        When the pinned thread mode is off, it return the original ``f``.\n","        \n","        .. versionadded:: 3.2.0\n","        \n","        Parameters\n","        ----------\n","        f : function\n","            the original thread target.\n","        \n","        Notes\n","        -----\n","        This API is experimental.\n","        \n","        It is important to know that it captures the local properties when you decorate it\n","        whereas :class:`InheritableThread` captures when the thread is started.\n","        Therefore, it is encouraged to decorate it when you want to capture the local\n","        properties.\n","        \n","        For example, the local properties from the current Spark context is captured\n","        when you define a function here instead of the invocation:\n","        \n","        >>> @inheritable_thread_target\n","        ... def target_func():\n","        ...     pass  # your codes.\n","        \n","        If you have any updates on local properties afterwards, it would not be reflected to\n","        the Spark context in ``target_func()``.\n","        \n","        The example below mimics the behavior of JVM threads as close as possible:\n","        \n","        >>> Thread(target=inheritable_thread_target(target_func)).start()  # doctest: +SKIP\n","\n","DATA\n","    __all__ = ['SparkConf', 'SparkContext', 'SparkFiles', 'RDD', 'StorageL...\n","\n","VERSION\n","    3.5.1\n","\n","FILE\n","    /usr/local/lib/python3.11/dist-packages/pyspark/__init__.py\n","\n","\n"]}]},{"cell_type":"code","source":["a=10\n","b=10\n","\n","hex(id(a)), hex(id(b))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7thmzgovJiE","executionInfo":{"status":"ok","timestamp":1750933346236,"user_tz":-330,"elapsed":8,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"d745b118-5a97-4e48-8b4a-6bf9dee61b4d"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('0xa42788', '0xa42788')"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["a = [1,2]\n","b = [3,4]\n","t = (a,b)\n","print(t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1N-dvfYDwpYP","executionInfo":{"status":"ok","timestamp":1750933824342,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"8f05b4e6-0d25-42d5-c492-80460ed06429"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["([1, 2], [3, 4])\n"]}]},{"cell_type":"code","source":["a.append(5)\n","print(t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yamrkcMIyeG3","executionInfo":{"status":"ok","timestamp":1750933846133,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"50dfcf40-b5fa-442d-bb0c-6139988dd65c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["([1, 2, 5], [3, 4])\n"]}]},{"cell_type":"code","source":["a = [1,'a',\"Hello\"]"],"metadata":{"id":"RtCSRsnDypgm","executionInfo":{"status":"ok","timestamp":1750933943824,"user_tz":-330,"elapsed":36,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["t = (1,'a',\"Hello\")"],"metadata":{"id":"L9W-BoN-y7RR","executionInfo":{"status":"ok","timestamp":1750933966961,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G5EsjC4QzA7M"},"execution_count":null,"outputs":[]}]}