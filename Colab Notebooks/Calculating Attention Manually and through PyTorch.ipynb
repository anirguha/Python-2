{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyPU6tTYPDQXdBysz2b/0SoS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"kHtFZyZLXB3s","executionInfo":{"status":"ok","timestamp":1765900702695,"user_tz":-330,"elapsed":4562,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import FormatStrFormatter, FuncFormatter\n","\n","from scipy.special import erf\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from time import time\n","from time import process_time\n","\n","import math"]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oeEa9KAMrovu","executionInfo":{"status":"ok","timestamp":1765900702712,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"53ed8f58-eafb-4f37-9742-f8640a41cd23"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}]},{"cell_type":"code","source":["# Set the key parametes\n","BATCH_SIZE = 4\n","embed_dim = 10\n","context_length = 8\n","vocab_size = 40"],"metadata":{"id":"F9CaoXLxrvXS","executionInfo":{"status":"ok","timestamp":1765900702714,"user_tz":-330,"elapsed":1,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Create a random dataset\n","data = torch.randint(low=0, high=vocab_size, size=(BATCH_SIZE, context_length))\n","data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCsyQEThsIDr","executionInfo":{"status":"ok","timestamp":1765900702778,"user_tz":-330,"elapsed":58,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"d3935868-cfd8-4cb7-8fe9-9ec97624a2ca"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 4, 10,  3,  0,  7, 15, 14,  3],\n","        [ 5, 29, 37, 22, 16, 30,  3, 20],\n","        [35, 18, 16, 10, 27, 33, 37,  2],\n","        [26, 18, 21, 20, 18, 33, 37, 24]])"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Create embeddings matrix\n","embeddings = nn.Embedding(vocab_size, embed_dim)"],"metadata":{"id":"1VCMVKPEs4qq","executionInfo":{"status":"ok","timestamp":1765900702780,"user_tz":-330,"elapsed":1,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Create Q, K, V matices\n","query = nn.Linear(embed_dim, embed_dim, bias=False)\n","key = nn.Linear(embed_dim, embed_dim, bias=False)\n","value = nn.Linear(embed_dim, embed_dim, bias=False)"],"metadata":{"id":"tGJLyXFytaaY","executionInfo":{"status":"ok","timestamp":1765900702782,"user_tz":-330,"elapsed":1,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Process the data\n"],"metadata":{"id":"jQEJBM7Itsbg"}},{"cell_type":"code","source":["X = embeddings(data)"],"metadata":{"id":"1-LsYkTUtm3_","executionInfo":{"status":"ok","timestamp":1765900702810,"user_tz":-330,"elapsed":23,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Create weights for the pre-tained model\n","q = query(X)\n","k = key(X)\n","v = value(X)"],"metadata":{"id":"XFwRHZONt6hH","executionInfo":{"status":"ok","timestamp":1765900702869,"user_tz":-330,"elapsed":61,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Print data sizes\n","print(f'{'Data Matrix Shape':24}: {data.shape}')\n","print(f'{'Embeddings Matrix Shape':24}: {embeddings.weight.shape}')\n","print(f'{'Token matrix shape':24}: {X.shape}')\n","\n","# sizes of matrices\n","print('')\n","print(f'{'Query Matrix Shape':24}: {query.weight.shape}')\n","print(f'{'Key Matrix Shape':24}: {key.weight.shape}')\n","print(f'{'Value Matrix Shape':24}: {value.weight.shape}')\n","\n","# Attention matrix sizes\n","print('')\n","print(f'{'Q(x)':24}: {q.shape}')\n","print(f'{'K(x)':24}: {k.shape}')\n","print(f'{'V(x)':24}: {v.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EYskqFgGuOCJ","executionInfo":{"status":"ok","timestamp":1765900702877,"user_tz":-330,"elapsed":7,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"aff9939e-50e2-49ec-8cec-00d404f0e880"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Matrix Shape       : torch.Size([4, 8])\n","Embeddings Matrix Shape : torch.Size([40, 10])\n","Token matrix shape      : torch.Size([4, 8, 10])\n","\n","Query Matrix Shape      : torch.Size([10, 10])\n","Key Matrix Shape        : torch.Size([10, 10])\n","Value Matrix Shape      : torch.Size([10, 10])\n","\n","Q(x)                    : torch.Size([4, 8, 10])\n","K(x)                    : torch.Size([4, 8, 10])\n","V(x)                    : torch.Size([4, 8, 10])\n"]}]},{"cell_type":"markdown","source":["# Create Attention activation\n","$$\n","\\operatorname{Attention}(Q, K, V)\n","=\n","\\operatorname{softmax}\\!\\left(\n","\\frac{QK^{\\top}}{\\sqrt{d_k}} + M\n","\\right) V\n","$$\n","\n","Where,  \n","A = Attention function  \n","Q = Query Matrix  \n","K = Key Matrix  \n","$\\\\d_k$ = dimension of Key Matrix  \n","M = Masking\n","V = Value Matrix\n"],"metadata":{"id":"wBDB3Ze7Y9Cu"}},{"cell_type":"markdown","source":["## *Create attention manually*"],"metadata":{"id":"TVTK62DT5jvr"}},{"cell_type":"code","source":["# Create causal mask manually - objective is to set the future values in the context window to be zero after softmax\n","mask = torch.tril(torch.ones(BATCH_SIZE, context_length, context_length))\n","\n","# Create the Q*K.T and scale it by square-root of embedding matrix\n","qk = q @ k.transpose(-2,-1)\n","qk_scaled = qk / (embed_dim ** 0.5)\n","\n","# Mnaully set the non-diagonal elements in lower triagular matrix to -inf\n","qk_scaled[mask == 0] = -torch.inf\n","\n","# Apply softmax\n","qk_softmaxed = F.softmax(qk_scaled, dim=-1)\n","\n","# Multiply with value matrix\n","attention_manual = qk_softmaxed @ v"],"metadata":{"id":"NwT0S1JbEYFs","executionInfo":{"status":"ok","timestamp":1765900702906,"user_tz":-330,"elapsed":28,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## *Create attention using PyTorch*"],"metadata":{"id":"aFLbGQkyJSJq"}},{"cell_type":"code","source":["attention_torch = F.scaled_dot_product_attention(query=q,\n","                                           key=k,\n","                                           value=v,\n","                                           is_causal=True\n",")\n","\n","torch.allclose(attention_manual, attention_torch, atol=1e-5, rtol=1e-5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-wgrZhIQHXgf","executionInfo":{"status":"ok","timestamp":1765900702913,"user_tz":-330,"elapsed":2,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"eb96e5be-7129-4f59-bef8-3cebfa5b6fec"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["## *Calculate the processing times for both manual and pytorch methods on CPU*"],"metadata":{"id":"ahlmQ8elM3-P"}},{"cell_type":"code","source":["num_runs = 50_000\n","start_time = process_time()\n","\n","for _ in range(num_runs):\n","  mask = torch.tril(torch.ones(BATCH_SIZE, context_length, context_length))\n","  qk = q @ k.transpose(-2,-1)\n","  qk_scaled = qk / (embed_dim ** 0.5)\n","  qk_scaled[mask == 0] = -torch.inf\n","  qk_softmaxed = F.softmax(qk_scaled, dim=-1)\n","  attention_manual = qk_softmaxed @ v\n","\n","elapsed_time = process_time() - start_time\n","\n","print(f'Time taken for manual calculation: {elapsed_time:.3f} seconds')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CtN1qDW2MNcM","executionInfo":{"status":"ok","timestamp":1765900710226,"user_tz":-330,"elapsed":7279,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"955b6be7-cc64-41e6-e672-09f27ac733a0"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Time taken for manual calculation: 43.629 seconds\n"]}]},{"cell_type":"code","source":["%%timeit\n","for _ in range(num_runs):\n","  attention_torch = F.scaled_dot_product_attention(query=q,\n","                                           key=k,\n","                                           value=v,\n","                                           is_causal=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wds6RqbCNung","executionInfo":{"status":"ok","timestamp":1765900763103,"user_tz":-330,"elapsed":52875,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"104149b8-35bf-49b7-eb4f-110ddae3098b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["6.57 s ± 59.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"]}]},{"cell_type":"markdown","source":["# Run the entire code on GPU"],"metadata":{"id":"8Xm9EnC_TwoJ"}},{"cell_type":"code","source":["# Set the key parametes\n","BATCH_SIZE = 64\n","embed_dim = 1000\n","context_length = 2048\n","vocab_size = 50_257"],"metadata":{"id":"RSpLihclN7th","executionInfo":{"status":"ok","timestamp":1765900763105,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Create a random dataset\n","data = torch.randint(low=0, high=vocab_size, size=(BATCH_SIZE, context_length), dtype=torch.long, device=device)\n","\n","# Create embeddings matrix\n","embeddings = nn.Embedding(vocab_size, embed_dim, device=device)\n","\n","#Create Q, K, V matices\n","query = nn.Linear(embed_dim, embed_dim, bias=False, device=device)\n","key = nn.Linear(embed_dim, embed_dim, bias=False, device=device)\n","value = nn.Linear(embed_dim, embed_dim, bias=False, device=device)\n"],"metadata":{"id":"fGKldXPBUMu-","executionInfo":{"status":"ok","timestamp":1765900763300,"user_tz":-330,"elapsed":193,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["X = embeddings(data)\n","\n","# Create weights for the pre-tained model\n","q = query(X)\n","k = key(X)\n","v = value(X)"],"metadata":{"id":"AkuXGI3VUms2","executionInfo":{"status":"ok","timestamp":1765900763520,"user_tz":-330,"elapsed":218,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["num_runs = 200\n","start_time = process_time()\n","\n","for _ in range(num_runs):\n","  mask = torch.tril(torch.ones(BATCH_SIZE, context_length, context_length))\n","  qk = q @ k.transpose(-2,-1)\n","  qk_scaled = qk / (embed_dim ** 0.5)\n","  qk_scaled[mask == 0] = -torch.inf\n","  qk_softmaxed = F.softmax(qk_scaled, dim=-1)\n","  attention_manual = qk_softmaxed @ v\n","\n","elapsed_time = process_time() - start_time\n","\n","print(f'Time taken for manual calculation on GPU: {elapsed_time:.3f} seconds')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-jKAu_SyUvJj","executionInfo":{"status":"ok","timestamp":1765900891587,"user_tz":-330,"elapsed":128065,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"eea1f664-2644-4a76-f021-5b5852722807"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Time taken for manual calculation on GPU: 634.034 seconds\n"]}]},{"cell_type":"code","source":["start_time = process_time()\n","\n","for _ in range(num_runs):\n","  attention_torch = F.scaled_dot_product_attention(query=q,\n","                                           key=k,\n","                                           value=v,\n","                                           is_causal=True\n",")\n","\n","elapsed_time = process_time() - start_time\n","\n","print(f'Time taken for PyTorch calculation on GPU: {elapsed_time:.3f} seconds')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bm2FHfy_VGpX","executionInfo":{"status":"ok","timestamp":1765900900529,"user_tz":-330,"elapsed":8943,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"908f50c6-dda0-4d1c-83a0-deffe0d382d0"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Time taken for PyTorch calculation on GPU: 8.874 seconds\n"]}]},{"cell_type":"code","source":["# Further optimizations on GPU\n","import torch._dynamo as dynamo\n","SDPA_compiled = torch.compile(F.scaled_dot_product_attention)\n","\n","# Apply recommended TF32 precision settings\n","torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n","torch.backends.cudnn.conv.fp32_precision = \"tf32\""],"metadata":{"id":"tIiiLBB9X_LT","executionInfo":{"status":"ok","timestamp":1765901352660,"user_tz":-330,"elapsed":45,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["num_runs = 200\n","\n","torch.cuda.empty_cache()\n","torch.cuda.synchronize()\n","\n","# Manual Calculations\n","start_time = process_time()\n","\n","for _ in range(num_runs):\n","  mask = torch.tril(torch.ones(BATCH_SIZE, context_length, context_length))\n","  qk = q @ k.transpose(-2,-1)\n","  qk_scaled = qk / (embed_dim ** 0.5)\n","  qk_scaled[mask == 0] = -torch.inf\n","  qk_softmaxed = F.softmax(qk_scaled, dim=-1)\n","  attention_manual = qk_softmaxed @ v\n","\n","elapsed_time = process_time() - start_time\n","\n","print(f'Time taken for manual calculation on GPU: {elapsed_time:.3f} seconds')\n","\n","# Using PyTorch\n","start_time = process_time()\n","\n","for _ in range(num_runs):\n","  attention_torch = F.scaled_dot_product_attention(query=q,\n","                                           key=k,\n","                                           value=v,\n","                                           is_causal=True\n",")\n","\n","elapsed_time = process_time() - start_time\n","\n","print(f'Time taken for PyTorch calculation on GPU: {elapsed_time:.3f} seconds')\n","\n","# Using Optimizations\n","start_time = process_time()\n","\n","for _ in range(num_runs):\n","  attention_torch = SDPA_compiled(query=q,\n","                                  key=k,\n","                                  value=v,\n","                                  is_causal=True\n",")\n","\n","elapsed_time = process_time() - start_time\n","\n","print(f'Optimized Time taken for PyTorch calculation on GPU: {elapsed_time:.3f} seconds')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IWVVhhlNYkv0","executionInfo":{"status":"ok","timestamp":1765901487966,"user_tz":-330,"elapsed":131139,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"569abf70-4497-4751-a7e7-c72e7ca31731"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Time taken for manual calculation on GPU: 633.002 seconds\n","Time taken for PyTorch calculation on GPU: 2.743 seconds\n","Optimized Time taken for PyTorch calculation on GPU: 1.263 seconds\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_VAR24DmZupo","executionInfo":{"status":"ok","timestamp":1765901043014,"user_tz":-330,"elapsed":1,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":20,"outputs":[]}]}