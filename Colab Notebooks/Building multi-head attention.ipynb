{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMbuE6UJNSrBSeJdrxXCqF1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"WXUOIoxkpwkD","executionInfo":{"status":"ok","timestamp":1766218088238,"user_tz":-330,"elapsed":8053,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"markdown","source":["# Hyperparameters"],"metadata":{"id":"L2qEQyuJqO9f"}},{"cell_type":"code","source":["# data hyperparameters\n","seq_len = 8 # aka context window\n","\n","# model hyperparameters\n","embed_dim = 128\n","n_heads = 4 # Embed dimension needs to be a multiple of number of attention heads\n","\n","# training hyperparameters\n","batch_size = 5"],"metadata":{"id":"_7Uh52zsp8ds","executionInfo":{"status":"ok","timestamp":1766218088262,"user_tz":-330,"elapsed":14,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Create Class for multihead attention"],"metadata":{"id":"SMpIl9CVqsn4"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self, num_heads, embed_dim):\n","    super().__init__()\n","\n","    # head-dimensionality is embed_dim split across the heads\n","    self.num_heads = num_heads\n","    self.head_dim = embed_dim // num_heads\n","\n","    # num_heads Q, K, and V matrices, initialized as one \"super-head\"\n","    #    note: in model 5, these three matrices are combined into one\n","    self.query = nn.Linear(embed_dim, embed_dim, bias=False)\n","    self.key   = nn.Linear(embed_dim, embed_dim, bias=False)\n","    self.value = nn.Linear(embed_dim, embed_dim, bias=False)\n","\n","    # final linear projection merges the heads' outputs\n","    self.W0 = nn.Linear(embed_dim, embed_dim, bias=False)\n","\n","  def forward(self,x,track_sizes=False):\n","\n","    # extract the dimension sizes of the inputs (token embeddings)\n","    B, T, E = x.shape # [batch, tokens (sequence length), embed_dim]\n","    if track_sizes: print(f\"1){' Input data shape:':>28} {x.shape}\")\n","\n","    # push data through Q, K, and V (actually multiple heads still in the same matrix)\n","    q = self.query(x) # [batch, seq_len, embed_dim]\n","    k = self.key(x)\n","    v = self.value(x)\n","    if track_sizes: print(f\"2){'q/k/v pre-split shape:':>28} {q.shape}\")\n","\n","    # reshape to split up the heads (note: head-splitting is done after XW_Q)\n","    q = q.view(B, T, self.num_heads, self.head_dim)\n","    k = k.view(B, T, self.num_heads, self.head_dim)\n","    v = v.view(B, T, self.num_heads, self.head_dim)\n","    if track_sizes: print(f\"3){'q/k/v post-split shape:':>28} {q.shape}\")\n","\n","    # but pytorch's SDPA function needs the shape to be [B, num_heads, T, head_dim]\n","    q = q.transpose(1,2)\n","    k = k.transpose(1,2)\n","    v = v.transpose(1,2)\n","    if track_sizes: print(f\"4){'q/k/v trnasposed shape:':>28} {q.shape}\")\n","\n","    # now we can call SDPA\n","    out = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n","    if track_sizes: print(f\"5){'Data post-attention shape:':>28} {out.shape}\")\n","\n","    # but our code still needs [B, T, num_heads, head_dim]\n","    out = out.transpose(1,2)\n","    if track_sizes: print(f\"6){'Post-attention data reshape:':>28} {out.shape}\")\n","\n","    # merge heads back into embed_dim\n","    out = out.reshape(B, T, E)\n","    if track_sizes: print(f\"7){'Data merged to size:':>28} {out.shape}\")\n","\n","    # finally, apply linear mixing matrix\n","    out = self.W0(out)\n","    if track_sizes: print(f\"8){'Post-MHA H0 linear mixing:':>28} {out.shape}\")\n","\n","    return out"],"metadata":{"id":"AzLMCC4EqnRZ","executionInfo":{"status":"ok","timestamp":1766218088343,"user_tz":-330,"elapsed":73,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["mha = MultiHeadAttention(n_heads,embed_dim)\n","mha"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZpTGhB1oq8Rf","executionInfo":{"status":"ok","timestamp":1766218088472,"user_tz":-330,"elapsed":117,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"a5141bf3-8633-4c9b-8a4a-630ba95de155"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiHeadAttention(\n","  (query): Linear(in_features=128, out_features=128, bias=False)\n","  (key): Linear(in_features=128, out_features=128, bias=False)\n","  (value): Linear(in_features=128, out_features=128, bias=False)\n","  (W0): Linear(in_features=128, out_features=128, bias=False)\n",")"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# run some fake data through\n","data = torch.randn(size=(batch_size,seq_len,embed_dim))\n","out = mha(data, track_sizes=True)\n","print(f'Input size:  {data.shape}')\n","print(f'Output size: {out.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mxx2QNelrCqT","executionInfo":{"status":"ok","timestamp":1766218088630,"user_tz":-330,"elapsed":155,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"70c07fba-bf20-4929-ac16-1e1b56cf3ed3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["1)           Input data shape: torch.Size([5, 8, 128])\n","2)      q/k/v pre-split shape: torch.Size([5, 8, 128])\n","3)     q/k/v post-split shape: torch.Size([5, 8, 4, 32])\n","4)     q/k/v trnasposed shape: torch.Size([5, 4, 8, 32])\n","5)  Data post-attention shape: torch.Size([5, 4, 8, 32])\n","6)Post-attention data reshape: torch.Size([5, 8, 4, 32])\n","7)        Data merged to size: torch.Size([5, 8, 128])\n","8)  Post-MHA H0 linear mixing: torch.Size([5, 8, 128])\n","Input size:  torch.Size([5, 8, 128])\n","Output size: torch.Size([5, 8, 128])\n"]}]},{"cell_type":"code","source":["print(f'    Sequence length: {seq_len:2d}')\n","print(f'Embedding dimension: {embed_dim}')\n","print(f'    Number of heads: {n_heads:2d}')\n","print(f'Head dimensionality: {embed_dim // n_heads}')\n","\n","print('\\nDimensions of the data as it passes through the attention sublayer of one Transformer block:')\n","out = mha(data,track_sizes=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gW6Ew0a-rW3z","executionInfo":{"status":"ok","timestamp":1766218088674,"user_tz":-330,"elapsed":46,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}},"outputId":"eefd573a-c955-4cc0-efee-63bfc7c619ad"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["    Sequence length:  8\n","Embedding dimension: 128\n","    Number of heads:  4\n","Head dimensionality: 32\n","\n","Dimensions of the data as it passes through the attention sublayer of one Transformer block:\n","1)           Input data shape: torch.Size([5, 8, 128])\n","2)      q/k/v pre-split shape: torch.Size([5, 8, 128])\n","3)     q/k/v post-split shape: torch.Size([5, 8, 4, 32])\n","4)     q/k/v trnasposed shape: torch.Size([5, 4, 8, 32])\n","5)  Data post-attention shape: torch.Size([5, 4, 8, 32])\n","6)Post-attention data reshape: torch.Size([5, 8, 4, 32])\n","7)        Data merged to size: torch.Size([5, 8, 128])\n","8)  Post-MHA H0 linear mixing: torch.Size([5, 8, 128])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Sk_mTpWnr0-i","executionInfo":{"status":"ok","timestamp":1766218088681,"user_tz":-330,"elapsed":1,"user":{"displayName":"Anirban Guha","userId":"10209519538744786819"}}},"execution_count":6,"outputs":[]}]}