# -*- coding: utf-8 -*-
"""Using Google SA in Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p8NQctpM1SUttR3Ndd21dsOiO2ZtOSuU
"""

import os, json
import shutil
import subprocess
import sys
import base64
import pathlib

# Simulate secret retrieval
sa_json_text = r"""{
  "type": "service_account",
  "project_id": "phrasal-hold-327509",
  "private_key_id": "eac5ab43ee968142cae8393a9ec74e4858c5118f",
  "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC0dTKmxQNnDwSi\nr6AwbUYjPAG8EBj5wgbqNKyO+DhF8J2HM2aiIONBtuSWLqoZpJIfTeui9ZVWLRJO\nbWPb/EU/DGxaNXeqSxg+ynuujm1L3oYMIHzO48vzaJFr/KBePBebjJoioms4D73L\n3e6Qjky21osOc2nqe3SMqd1LdYRb7O+dv8dgD+eS+vlIYG59EOrLxe3PMrJBE9QY\ngzNQXavYtlxrcSw1B3cBDnwjr/IdtX5uEZNcPGTUKWce2qnceojMvKX5IA23+YWC\n9uDbp02iO8Bmz61/FPodG15PVJTfeU+zYgOAdwS13os9DWJqonZCluaPP15+4QqI\nhOn6CXuHAgMBAAECggEAFeYXBHfqBsqixF9NsDLCjSoOB9VoQn7SOf+0ZREoOwyN\nAUgyj0Awc9wO906oY7494UCidL8apT+I9i1+Lcb6XEFUsMnGQMsmIRCPW8p65vcV\nC0mx+6VXLa2l4BXjHCxsrPquBb/yBPacEK5Y8f/ivVlhvJoy4zqiFSPT8AFN053D\n67RtSTpo4Ncm1UxZT2SKYv3yGaQAi5zmT89CRWyGes3cVnsUdzy1WU0Riu1qis9R\n/oZ4K/wV2+6uwYsCxotKuGFftdsduiRUxxTTqiy/ViSanLVpSHHjUgCkPsx5kpqk\n16z8gpC4jv9ZZ/FmeZlw9L/0V7MlvSU4tnyz9gc62QKBgQDgPsSQ2VTEApMpJgou\nTt0Mbj8zQ5v93yNDiRHN8AcK+XZaS6FSx731T+59i/rlPJ4iIfxRyMOL+EKveQWA\nkZwve1lyzxKK57fZpHNqnNiL1Kfuw76bm1mvTqgy83GpiCm8emY2etTjd5dynaxc\nLM+S70Te5UsMP41C7awhlBbegwKBgQDOAxH4ksePfHKCo/TkKP4m70STvQ9gJ/Pq\nLyWex5uMY6IFAqKX2PRAQEno5bPomEiuOTSNwMk6dK99qv2MYoZ/DdoKBOmBOlmX\nKIdt6IcOMPq0A0+1zRjfnsdut3D1OHd5p3ZOgEQysubetHgXLEiNsj++WSTZLLkl\n1O9M/mvfrQKBgGxyLoGWeQXp8N0/hkDq5r/yOSJDmhbmaJlqmm+rmtxV1ETKupPc\nASO3qjCRF57w99Ct+uZaXyc7wpQ1xNFumcfksVdyiRcgBKt+ekqe26KUDzrK800x\njefX9EPfs6I3NWfvEatbL8VpupY1ZnuVODVo/TeGUAr2WQx7AHmksoKdAoGBAKIh\nJm6DTYyt+wy3bg51+Ha5zTWcD9Kz/rEqA6hvHQqiiqr4xO9OLgO21LP2utztpB2B\nJcv9kiBwHoB/JmXqUEEDrLo/3Wj3e/SLT/H+kvNnNpafGIdSbyXmNrv6oGpM0zIp\nyeDD72gbBbj1PKG4Fy10v9fVHjvQh7GvX9kvTcRNAoGAf3k5H9tOkqNii2kbb1ww\nDJAWguEUhl8uauVF9x3vbTR3q8K+w2NeKhfoTPJxJGiLEuvbKr2i4t+BTGyIuE5j\n+sSJcpc86L8nil2WDAMbqgczucbN9OhPv9P5dc9Wi3iytfexq7GrRDUCr0dOD8Ae\n1p9SL08Xd2h2EHaoBDI9kk0=\n-----END PRIVATE KEY-----\n",
  "client_email": "colab-drive-access@phrasal-hold-327509.iam.gserviceaccount.com",
  "client_id": "117174193156951204686",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/colab-drive-access%40phrasal-hold-327509.iam.gserviceaccount.com",
  "universe_domain": "googleapis.com"
}
"""

# 2) Set env var (no UI prompts)
os.environ["GOOGLE_SA_JSON_B64"] = base64.b64encode(sa_json_text.encode("utf-8")).decode("ascii")

# 3) Materialize /content/sa.json from the env var
b = base64.b64decode(os.environ["GOOGLE_SA_JSON_B64"])
path = pathlib.Path("/content/sa.json")
path.write_bytes(b)

# 4) Validate
json.loads(path.read_text())
print("✅ Wrote and validated /content/sa.json")

import json, pathlib
try:
    json.loads(pathlib.Path("/content/sa.json").read_text())
    print("JSON OK")
except json.JSONDecodeError as e:
    print("Line", e.lineno, "col", e.colno, "|", e.msg)
    # Show context around the error
    txt = pathlib.Path("/content/sa.json").read_text().splitlines()
    start, end = max(0, e.lineno-3), e.lineno+2
    for i in range(start, min(end, len(txt))):
        print(f"{i+1:4}: {txt[i]}")

from google.oauth2 import service_account
from googleapiclient.discovery import build

creds = service_account.Credentials.from_service_account_file(
    "/content/sa.json", scopes=["https://www.googleapis.com/auth/drive"])
drive_service = build("drive", "v3", credentials=creds)

def sh(cmd, check=True):
    """Run a shell command via bash, showing the command and raising on error."""
    print("+", cmd)
    return subprocess.run(cmd, shell=True, check=check, executable="/bin/bash")

def ensure_rclone():
    # Quiet non-interactive apt on Debian/Ubuntu/Colab
    os.environ.setdefault("DEBIAN_FRONTEND", "noninteractive")

    # Try apt first (works on Colab)
    try:
        sh("apt-get -y -qq update && apt-get -y -qq install fuse rclone || true")
    except subprocess.CalledProcessError:
        pass  # not fatal; we’ll try the installer below

    # If rclone still missing, use official install script
    if not shutil.which("rclone"):
        sh("curl -fsSL https://rclone.org/install.sh | bash")

    # Final sanity check
    if not shutil.which("rclone"):
        raise RuntimeError("rclone is not installed and the installer failed.")

    # Show version
    sh("rclone version")

if __name__ == "__main__":
    ensure_rclone()

import os, textwrap, pathlib, subprocess, time

FOLDER_ID = "1UaoEBl_96l50YMA-cYU6Dmuuxr5Bl0Sb"  # e.g. "1AbC..."; must be the real folder (not a shortcut)
TEAM_DRIVE_ID = ""                   # if this folder is on a Shared drive, set its drive ID here

# Write rclone config
os.makedirs("/root/.config/rclone", exist_ok=True)
conf = textwrap.dedent(f"""
[gdrive_sa]
type = drive
scope = drive
service_account_file = /content/sa.json
root_folder_id = {FOLDER_ID}
{"team_drive = "+TEAM_DRIVE_ID if TEAM_DRIVE_ID else ""}
""").strip()+"\n"

path = "/root/.config/rclone/rclone.conf"
open(path,"w").write(conf)
print(open(path).read())

# Quick list via rclone (should show files in that folder)
# !rclone lsf gdrive_sa: --drive-root-folder-id {FOLDER_ID} --fast-list

"""# Mount the shared drive on the SA using bash"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# MNT=/content/drive_sa
# mkdir -p "$MNT"
# 
# # Start FUSE mount as a background daemon
# rclone mount gdrive_sa: "$MNT" \
#   --vfs-cache-mode writes \
#   --poll-interval 15s \
#   --daemon
# 
# # Give it a moment to attach, then list
# sleep 2
# ls -al "$MNT" | head -n 40
#

"""# Mount the shared drive using Python"""

# === EDIT THESE ===
FOLDER_ID = "1UaoEBl_96l50YMA-cYU6Dmuuxr5Bl0Sb"   # from https://drive.google.com/drive/folders/<ID>
TEAM_DRIVE_ID = "1UaoEBl_96l50YMA-cYU6Dmuuxr5Bl0Sb"                   # if folder is on a Shared Drive, paste its drive ID; else keep ""
# ==================

import os, json, textwrap, subprocess, time, pathlib, shlex

KEY = "/content/sa.json"
assert pathlib.Path(KEY).exists(), "Missing /content/sa.json"

# 0) Validate JSON (quick sanity)
json.loads(pathlib.Path(KEY).read_text())

# 1) Ensure rclone is installed
if subprocess.call(["bash","-lc","command -v rclone >/dev/null"]) != 0:
    subprocess.run(["bash","-lc","apt-get -y -qq install fuse rclone || (curl -s https://rclone.org/install.sh | bash)"], check=True)

# 2) Write rclone config
os.makedirs("/root/.config/rclone", exist_ok=True)
conf = textwrap.dedent(f"""
[gdrive_sa]
type = drive
scope = drive
service_account_file = {KEY}
root_folder_id = {FOLDER_ID}
{"team_drive = "+TEAM_DRIVE_ID if TEAM_DRIVE_ID else ""}
""").strip()+"\n"
pathlib.Path("/root/.config/rclone/rclone.conf").write_text(conf)

print("== rclone.conf ==")
print(pathlib.Path("/root/.config/rclone/rclone.conf").read_text())

# 3) Can the remote list? (if this fails, mount will fail)
print("\n== rclone lsf test ==")
ls_ret = subprocess.run(["bash","-lc","rclone lsf gdrive_sa: --fast-list | head -n 20; echo EXIT:$?"], capture_output=True, text=True)
print(ls_ret.stdout)

# 4) Start mount in foreground and capture logs
MNT = "/content/drive_sa"
os.makedirs(MNT, exist_ok=True)
log = "/content/rclone_mount.log"

# Kill any previous rclone mount to be clean
subprocess.run(["pkill","-f","rclone mount gdrive_sa:"], check=False)
time.sleep(0.5)

print("== starting mount ==")
with open(log,"w") as lf:
    proc = subprocess.Popen([
        "rclone","mount","gdrive_sa:", MNT,
        "--vfs-cache-mode","writes",
        "--poll-interval","15s",
        "--log-file", log,
        "--log-level","INFO",
    ], stdout=lf, stderr=lf)

# Wait up to ~5s for mount to appear
ok = False
for _ in range(10):
    time.sleep(0.5)
    if os.system(f"mountpoint -q {shlex.quote(MNT)}") == 0:
        ok = True
        break

print("Mounted?", ok, " PID:", proc.pid)
if not ok:
    # Show why it failed
    print("\n== mount log tail ==")
    try:
        print(pathlib.Path(log).read_text()[-4000:])
    except Exception as e:
        print("No log:", e)

"""# Unmount the shared drive from SA using bash"""

# Commented out IPython magic to ensure Python compatibility.
# ## Using bash
# %%bash
# # 0) Ensure we aren't "in" the mount
# cd /
# 
# # 1) Is it mounted?
# mountpoint -q /content/drive_sa && echo "STILL MOUNTED" || echo "NOT MOUNTED"
# grep -w "/content/drive_sa" /proc/mounts || true
# 
# # 2) Any rclone mount processes still running?
# ps -ef | grep -E '[r]clone mount|[f]use' || true
# 
# # 3) Kill any rclone mounts pointing at that path
# pkill -f "rclone mount gdrive_sa:"        || true
# pkill -f "rclone mount .* /content/drive_sa" || true
# 
# # 4) Lazy-unmount (handles lingering handles)
# fusermount -uz /content/drive_sa  || true
# umount -l /content/drive_sa       || true
# 
# # 5) Final check
# mountpoint -q /content/drive_sa && echo "STILL MOUNTED" || echo "✅ UNMOUNTED"
# 
#

## Using Python
subprocess.run(["fusermount", "-u", "/content/drive_sa"], check=False)
subprocess.run(["umount", "/content/drive_sa"], check=False)

"""# Mount shared drive using Python"""

import os, time, subprocess, signal

mnt = "/content/drive_sa"
os.makedirs(mnt, exist_ok=True)

# Start rclone in the foreground so we get a process handle
proc = subprocess.Popen([
    "rclone","mount","gdrive_sa:", mnt,
    "--vfs-cache-mode","writes",
    "--poll-interval","15s"
])

# Wait a moment and verify
time.sleep(2)
print("Mounted?", os.system(f"mountpoint -q {mnt}") == 0)

"""# Unmount shared drive using Python"""

import os, subprocess, time

# 1) Leave the mount dir so nothing holds it
os.chdir("/")

# 2) Ask rclone to stop
proc.terminate()
try:
    proc.wait(timeout=3)
except subprocess.TimeoutExpired:
    proc.kill()

# 3) As a fallback, lazy-unmount if needed
subprocess.run(["fusermount","-uz","/content/drive_sa"], check=False)
subprocess.run(["umount","-l","/content/drive_sa"], check=False)

# 4) Verify
ok = (os.system("mountpoint -q /content/drive_sa") != 0)
print("✅ Unmounted" if ok else "Still mounted")

import json, pathlib
pj = json.loads(pathlib.Path("/content/sa.json").read_text())
print(pj["project_id"])

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# gcloud auth activate-service-account --key-file=/content/sa.json
# gcloud config set project "$(jq -r .project_id /content/sa.json)"
# gsutil ls   # shows gs://<bucket>/
#

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# # EDIT: choose a unique name (add a random suffix)
# PROJECT_ID=$(jq -r .project_id /content/sa.json)
# REGION="asia-south1"   # or your preferred region
# BUCKET="anirban-secure-keys-$(date +%s)"  # example unique name
# 
# gcloud auth activate-service-account --key-file=/content/sa.json
# gcloud config set project "$PROJECT_ID"
# 
# # Create with public access blocked and uniform bucket-level access ON
# gsutil mb -p "$PROJECT_ID" -l "$REGION" -b on "gs://${BUCKET}"
# 
# echo "Created bucket: gs://${BUCKET}"
#

"""# Upload the SA key to a signed URL (valid for 10 min.)"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# # ====== EDIT THESE ======
# PROJECT_ID="phrasal-hold-327509"
# BUCKET="anirban-secure-keys-1760264572"        # e.g. "anirban-secure-keys" (must already exist or we'll create it)
# OBJ_PATH="keys/sa.json"                  # where to store it in the bucket
# SA_KEY="/content/sa.json"               # your signer key (service-account JSON) already on disk
# REGION="asia-south1"                     # pick your region
# # ========================
# 
# set -euo pipefail
# 
# # 0) Make sure gcloud/gsutil are present (they are in Colab images)
# gcloud --version >/dev/null
# 
# # 1) Activate the service account for this shell (no UI)
# gcloud auth activate-service-account --key-file="${SA_KEY}"
# 
# # 2) Set project
# gcloud config set project "${PROJECT_ID}"
# 
# # 3) Ensure the bucket exists (idempotent: create if missing)
# if ! gsutil ls -b "gs://${BUCKET}" >/dev/null 2>&1; then
#   gsutil mb -p "${PROJECT_ID}" -l "${REGION}" -b on "gs://${BUCKET}"
# fi
# 
# # 4) Upload the file (adjust source if you want to upload a different secret)
# gsutil cp "${SA_KEY}" "gs://${BUCKET}/${OBJ_PATH}"
# 
# # 5) (Recommended) make sure the SIGNER account has permission to serve GET for this object
# # Grant objectViewer at bucket-level to the signer SA (replace with your signer email if different)
# SIGNER_EMAIL="$(jq -r .client_email < "${SA_KEY}")"
# gsutil iam ch "serviceAccount:${SIGNER_EMAIL}:roles/storage.objectViewer" "gs://${BUCKET}"
# 
# # 6) Generate a short-lived (10 min) signed URL using the same SA key
# # Note: you can also use a *different* SA key to sign if you prefer.
# URL=$(gsutil signurl -d 10m "${SA_KEY}" "gs://${BUCKET}/${OBJ_PATH}" | awk 'NR==2 {print $NF}')
# 
# echo ""
# echo "Signed URL (valid ~10 minutes):"
# echo "${URL}"
#

"""# Access the secured key from the URL"""

import requests, pathlib
url = "https://console.developers.google.com/apis/api/cloudresourcemanager.googleapis.com/overview?project=1066196832845"
b = requests.get(url, timeout=20).content
pathlib.Path("/content/sa.json").write_bytes(b)
print("Downloaded /content/sa.json via signed URL.")

import pathlib, json, textwrap

p = pathlib.Path("/content/sa.json")
print("Exists:", p.exists(), " Size:", p.stat().st_size if p.exists() else 0, "bytes")

if p.exists() and p.stat().st_size:
    head = p.read_bytes()[:300]
    try:
        print("First ~300 bytes:\n", head.decode("utf-8", "replace"))
    except Exception:
        print("First ~300 bytes (binary)")
    # Try parse
    try:
        j = json.loads(p.read_text())
        print("✅ JSON OK. SA email:", j.get("client_email"))
    except json.JSONDecodeError as e:
        print(f"❌ JSON invalid at line {e.lineno}, col {e.colno}: {e.msg}")
else:
    print("❌ File missing or empty.")

