{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "868f9a8c-a7c0-4482-8571-c6ec494f2b32",
   "metadata": {},
   "source": [
    "### Set up Spark Envrionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a36bb7a-3a1b-490f-88df-fe9eb89074a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import SparkSession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55bf5a94-f758-4fe1-801c-801dd30e38e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/02 21:56:15 WARN Utils: Your hostname, Anirbans-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.149 instead (on interface en0)\n",
      "25/07/02 21:56:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/02 21:56:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Initialize a Spark Session\n",
    "my_spark = SparkSession.builder.appName('MySparkApp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd7b74b-113f-44f4-9e46-84ed8a319655",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_csv = my_spark.read.csv(\"salaries.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0239fc34-b237-4cd6-87e2-b6f3f107f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|work_year|experience_level|employment_type|           job_title|salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|\n",
      "+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|     2020|              EN|             FT| Azure Data Engineer|100000|            USD|       100000|                MU|           0|              MU|           S|\n",
      "|     2020|              EN|             CT|  Staff Data Analyst| 60000|            CAD|        44753|                CA|          50|              CA|           L|\n",
      "|     2020|              SE|             FT|Staff Data Scientist|164000|            USD|       164000|                US|          50|              US|           M|\n",
      "|     2020|              EN|             FT|        Data Analyst| 42000|            EUR|        47899|                DE|           0|              DE|           L|\n",
      "|     2020|              EX|             FT|      Data Scientist|300000|            USD|       300000|                US|         100|              US|           L|\n",
      "|     2020|              MI|             CT|  Sales Data Analyst| 60000|            USD|        60000|                NG|           0|              NG|           M|\n",
      "|     2020|              EX|             FT|  Staff Data Analyst| 15000|            USD|        15000|                NG|           0|              CA|           M|\n",
      "|     2020|              MI|             FT|Business Data Ana...| 95000|            USD|        95000|                US|           0|              US|           M|\n",
      "|     2020|              EN|             FT|        Data Analyst| 20000|            EUR|        22809|                PT|         100|              PT|           M|\n",
      "|     2020|              EN|             FT|      Data Scientist| 43200|            EUR|        49268|                DE|           0|              DE|           S|\n",
      "|     2020|              SE|             FT|Machine Learning ...|157000|            CAD|       117104|                CA|          50|              CA|           L|\n",
      "|     2020|              EN|             FT|       Data Engineer| 48000|            EUR|        54742|                PK|         100|              DE|           L|\n",
      "|     2020|              MI|             FT|Product Data Analyst| 20000|            USD|        20000|                HN|           0|              HN|           S|\n",
      "|     2020|              MI|             FT|       Data Engineer| 51999|            EUR|        59303|                DE|         100|              DE|           S|\n",
      "|     2020|              EN|             FT|   Big Data Engineer| 70000|            USD|        70000|                US|         100|              US|           L|\n",
      "|     2020|              SE|             FT|      Data Scientist| 60000|            EUR|        68428|                GR|         100|              US|           L|\n",
      "|     2020|              MI|             FT|  Research Scientist|450000|            USD|       450000|                US|           0|              US|           M|\n",
      "|     2020|              MI|             FT|        Data Analyst| 41000|            EUR|        46759|                FR|          50|              FR|           L|\n",
      "|     2020|              MI|             FT|       Data Engineer| 65000|            EUR|        74130|                AT|          50|              AT|           L|\n",
      "|     2020|              MI|             FT|      Data Scientist|103000|            USD|       103000|                US|         100|              US|           L|\n",
      "+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f874cd03-2a25-42a1-aa37-f403bc8ccc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37234"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_csv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202d4e91-d3c5-4564-affa-e1de2374de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_rdd = salary_csv.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2674b690-fca2-4ab6-9cd3-7a028e556ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(salary_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e197ae-3314-46f5-997a-d3930e17e1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[25] at javaToPython at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "print(salary_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "997b74b7-1910-4d81-b09c-b3b1410edc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: 3.10.18 (main, Jun  5 2025, 08:13:51) [Clang 14.0.6 ]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDriver:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sys\u001b[38;5;241m.\u001b[39mversion)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorker:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mparallelize([\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28m__import__\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msys\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mversion)\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Driver:\", sys.version)\n",
    "print(\"Worker:\", spark.sparkContext.parallelize([0]).map(lambda x: __import__('sys').version).collect()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3df5c1-6608-498a-8c70-2a6816adad4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark_env)",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
